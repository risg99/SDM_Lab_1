ID,title,abstract,pages,doi,link
0c2d3b28d48426b8b72f7214a7708ba8b4efa9d6,"Reproducible, interactive, scalable and extensible microbiome data science using QIIME 2",,01-May,10.1038/s41587-019-0209-9,http://www.brooks.com/
fa5853fdef7d2f6bb68203d187ddacbbddc63a8b,High-Dimensional Probability: An Introduction with Applications in Data Science,"© 2018, Cambridge University Press Let us summarize our findings. A random projection of a set T in R n onto an m-dimensional subspace approximately preserves the geometry of T if m ⪆ d ( T ) . For...",1591 - 1594,10.1080/14697688.2020.1813475,https://www.ruiz.net/search.html
4c6e31458b0b44c1e8bd6e58f7d7e0767f7fde44,CRISP-DM Twenty Years Later: From Data Mining Processes to Data Science Trajectories,"CRISP-DM(CRoss-Industry Standard Process for Data Mining) has its origins in the second half of the nineties and is thus about two decades old. According to many surveys and user polls it is still the de facto standard for developing data mining and knowledge discovery projects. However, undoubtedly the field has moved on considerably in twenty years, with data science now the leading term being favoured over data mining. In this paper we investigate whether, and in what contexts, CRISP-DM is still fit for purpose for data science projects. We argue that if the project is goal-directed and process-driven the process model view still largely holds. On the other hand, when data science projects become more exploratory the paths that the project can take become more varied, and a more flexible model is called for. We suggest what the outlines of such a trajectory-based model might look like and how it can be used to categorise data science projects (goal-directed, exploratory or data management). We examine seven real-life exemplars where exploratory activities play an important role and compare them against 51 use cases extracted from the NIST Big Data Public Working Group. We anticipate this categorisation can help project planning in terms of time and cost characteristics.",3048-3061,10.1109/tkde.2019.2962680,http://thomas.biz/index.html
3569c79cf90b203325dd7b8f6c30bacc60f5d30e,"Data Science and Analytics: An Overview from Data-Driven Smart Computing, Decision-Making and Applications Perspective",,76-115,10.1007/s42979-021-00765-8,http://gray.com/
7282f5c9d84cd47c516a6a66c5a6b8f1e2cf44b6,AutoDS: Towards Human-Centered Automation of Data Science,"Data science (DS) projects often follow a lifecycle that consists of laborious tasks for data scientists and domain experts (e.g., data exploration, model training, etc.). Only till recently, machine learning(ML) researchers have developed promising automation techniques to aid data workers in these tasks. This paper introduces AutoDS, an automated machine learning (AutoML) system that aims to leverage the latest ML automation techniques to support data science projects. Data workers only need to upload their dataset, then the system can automatically suggest ML configurations, preprocess data, select algorithm, and train the model. These suggestions are presented to the user via a web-based graphical user interface and a notebook-based programming user interface. Our goal is to offer a systematic investigation of user interaction and perceptions of using an AutoDS system in solving a data science task. We studied AutoDS with 30 professional data scientists, where one group used AutoDS, and the other did not, to complete a data science project. As expected, AutoDS improves productivity; Yet surprisingly, we find that the models produced by the AutoDS group have higher quality and less errors, but lower human confidence scores. We reflect on the findings by presenting design implications for incorporating automation techniques into human work in the data science lifecycle.",95-141,10.1145/3411764.3445526,http://www.white.com/main/terms/
d737e2d326519ab3ba5da17441e073ba1c7a3ef5,Computational Optimal Transport: With Applications to Data Science,"The goal of Optimal Transport (OT) is to define geometric tools that are useful to compare probability distributions. Their use dates back to 1781. Recent years have witnessed a new revolution in the spread of OT, thanks to the emergence of approximate solvers that can scale to sizes and dimensions that are relevant to data sciences. Thanks to this newfound scalability, OT is being increasingly used to unlock various problems in imaging sciences (such as color or texture processing), computer vision and graphics (for shape manipulation) or machine learning (for regression, classification and density fitting). This monograph reviews OT with a bias toward numerical methods and their applications in data sciences, and sheds lights on the theoretical properties of OT that make it particularly useful for some of these applications. Computational Optimal Transport presents an overview of the main theoretical insights that support the practical effectiveness of OT before explaining how to turn these insights into fast computational schemes. Written for readers at all levels, the authors provide descriptions of foundational theory at two-levels. Generally accessible to all readers, more advanced readers can read the specially identified more general mathematical expositions of optimal transport tailored for discrete measures. Furthermore, several chapters deal with the interplay between continuous and discrete measures, and are thus targeting a more mathematically-inclined audience. This monograph will be a valuable reference for researchers and students wishing to get a thorough understanding of Computational Optimal Transport, a mathematical gem at the interface of probability, analysis and optimization.",60-112,388-37267-2,https://barnett.com/home.htm
4ce46acc4a40038b02014c1bbce3e451586ebe05,Data Science,"Data science has become the most demanding task of the 21st century. All companies are looking for candidates with knowledge of data science. This topic provides an overview of data science. Includes data science duties, data science tools, data science components, applications and more.",85-109,10.34293/sijash.v10i1.4969,https://www.cervantes.com/author/
0b1d429110757c1a9deb27fb8568740182dc1679,Eleven grand challenges in single-cell data science,,74-142,10.1186/s13059-020-1926-6,http://www.ortiz.com/about.html
040d94340f742f522ceddb31f31fb9c9b4c23cfd,Computing competencies for undergraduate data science curricula,,22-149,10.1145/3453538,https://levine.com/tags/app/index/
dd45dc3767230b70197420e1523dc0f1d7930f80,Foundations of Data Science,"Computer science as an academic discipline began in the 1960’s. Emphasis was on programming languages, compilers, operating systems, and the mathematical theory that supported these areas. Courses in theoretical computer science covered finite automata, regular expressions, context-free languages, and computability. In the 1970’s, the study of algorithms was added as an important component of theory. The emphasis was on making computers useful. Today, a fundamental change is taking place and the focus is more on applications. There are many reasons for this change. The merging of computing and communications has played an important role. The enhanced ability to observe, collect, and store data in the natural sciences, in commerce, and in other fields calls for a change in our understanding of data and how to handle it in the modern setting. The emergence of the web and social networks as central aspects of daily life presents both opportunities and challenges for theory.",68-112,10.1017/9781108755528,https://www.crawford.com/category/
0b5b33b7ea1dc12f3e9252ac1852170a6a6775bf,Nucleus segmentation across imaging experiments: the 2018 Data Science Bowl,,1247 - 1253,10.1038/s41592-019-0612-7,http://www.carney-henderson.com/
1ba044d3d501dddd94b479aa9dbe55a93bfa9d5f,"QIIME 2: Reproducible, interactive, scalable, and extensible microbiome data science","We present QIIME 2, an open-source microbiome data science platform accessible to users spanning the microbiome research ecosystem, from scientists and engineers to clinicians and policy makers. QIIME 2 provides new features that will drive the next generation of microbiome research. These include interactive spatial and temporal analysis and visualization tools, support for metabolomics and shotgun metagenomics analysis, and automated data provenance tracking to ensure reproducible, transparent microbiome data science.",1,10.7287/PEERJ.PREPRINTS.27295V1,http://robinson.info/list/tags/tags/home/
ede0a8039a561905f40777ec2ae66c2010e3f2bc,Cybersecurity data science: an overview from machine learning perspective,,Jan-29,10.1186/s40537-020-00318-5,http://www.davis.com/wp-content/homepage/
ab8ba0f2d290a8e56eb61e10027d0b2e57d2d544,"How do Data Science Workers Collaborate? Roles, Workflows, and Tools","Today, the prominence of data science within organizations has given rise to teams of data science workers collaborating on extracting insights from data, as opposed to individual data scientists working alone. However, we still lack a deep understanding of how data science workers collaborate in practice. In this work, we conducted an online survey with 183 participants who work in various aspects of data science. We focused on their reported interactions with each other (e.g., managers with engineers) and with different tools (e.g., Jupyter Notebook). We found that data science teams are extremely collaborative and work with a variety of stakeholders and tools during the six common steps of a data science workflow (e.g., clean data and train model). We also found that the collaborative practices workers employ, such as documentation, vary according to the kinds of tools they use. Based on these findings, we discuss design implications for supporting data science team collaborations and future research directions.",Jan-23,10.1145/3392826,http://roberts.com/app/search/app/index.htm
2968bab109c9a1660260126a07da1af4140eb6ae,Data Science,,62-136,10.1007/978-981-13-2206-8,https://parsons.com/
72d3ddf1f7210d7e70144bbc09f770ec411fe909,"Machine Learning in Python: Main developments and technology trends in data science, machine learning, and artificial intelligence","Smarter applications are making better use of the insights gleaned from data, having an impact on every industry and research discipline. At the core of this revolution lies the tools and the methods that are driving it, from processing the massive piles of data generated each day to learning from and taking useful action. Deep neural networks, along with advancements in classical machine learning and scalable general-purpose graphics processing unit (GPU) computing, have become critical components of artificial intelligence, enabling many of these astounding breakthroughs and lowering the barrier to adoption. Python continues to be the most preferred language for scientific computing, data science, and machine learning, boosting both performance and productivity by enabling the use of low-level libraries and clean high-level APIs. This survey offers insight into the field of machine learning with Python, taking a tour through important topics to identify some of the core hardware and software paradigms that have enabled it. We cover widely-used libraries and concepts, collected together for holistic comparison, with the goal of educating the reader and driving the field of Python machine learning forward.",193,10.3390/info11040193,https://jones.com/faq/
0751d2fa3a54cbbb4d594f2ee47c3aa7e4003a24,Leveraging Data Science to Combat COVID-19: A Comprehensive Review,"COVID-19, an infectious disease caused by the SARS-CoV-2 virus, was declared a pandemic by the World Health Organisation (WHO) in March 2020. By mid-August 2020, more than 21 million people have tested positive worldwide. Infections have been growing rapidly and tremendous efforts are being made to fight the disease. In this paper, we attempt to systematise the various COVID-19 research activities leveraging data science, where we define data science broadly to encompass the various methods and tools—including those from artificial intelligence (AI), machine learning (ML), statistics, modeling, simulation, and data visualization—that can be used to store, process, and extract insights from data. In addition to reviewing the rapidly growing body of recent research, we survey public datasets and repositories that can be used for further work to track COVID-19 spread and mitigation strategies. As part of this, we present a bibliometric analysis of the papers produced in this short span of time. Finally, building on these insights, we highlight common challenges and pitfalls observed across the surveyed works. We also created a live resource repository at https://github.com/Data-Science-and-COVID-19/Leveraging-Data-Science-To-Combat-COVID-19-A-Comprehensive-Review that we intend to keep updated with the latest resources including new papers and datasets.",85-103,10.1109/TAI.2020.3020521,http://thompson-jordan.com/blog/author/
8bba999de25bfb288b3f7f88e1d907aab02638b6,Big-Data Science in Porous Materials: Materials Genomics and Machine Learning,"By combining metal nodes with organic linkers we can potentially synthesize millions of possible metal–organic frameworks (MOFs). The fact that we have so many materials opens many exciting avenues but also create new challenges. We simply have too many materials to be processed using conventional, brute force, methods. In this review, we show that having so many materials allows us to use big-data methods as a powerful technique to study these materials and to discover complex correlations. The first part of the review gives an introduction to the principles of big-data science. We show how to select appropriate training sets, survey approaches that are used to represent these materials in feature space, and review different learning architectures, as well as evaluation and interpretation strategies. In the second part, we review how the different approaches of machine learning have been applied to porous materials. In particular, we discuss applications in the field of gas storage and separation, the stability of these materials, their electronic properties, and their synthesis. Given the increasing interest of the scientific community in machine learning, we expect this list to rapidly expand in the coming years.",8066 - 8129,10.1021/ACS.CHEMREV.0C00004,http://www.howell.com/main/
2d6adb9636df5a8a5dbcbfaecd0c4d34d7c85034,Spectral Methods for Data Science: A Statistical Perspective,"Spectral methods have emerged as a simple yet surprisingly effective approach for extracting information from massive, noisy and incomplete data. In a nutshell, spectral methods refer to a collection of algorithms built upon the eigenvalues (resp. singular values) and eigenvectors (resp. singular vectors) of some properly designed matrices constructed from data. A diverse array of applications have been found in machine learning, data science, and signal processing. Due to their simplicity and effectiveness, spectral methods are not only used as a stand-alone estimator, but also frequently employed to initialize other more sophisticated algorithms to improve performance. 
While the studies of spectral methods can be traced back to classical matrix perturbation theory and methods of moments, the past decade has witnessed tremendous theoretical advances in demystifying their efficacy through the lens of statistical modeling, with the aid of non-asymptotic random matrix theory. This monograph aims to present a systematic, comprehensive, yet accessible introduction to spectral methods from a modern statistical perspective, highlighting their algorithmic implications in diverse large-scale applications. In particular, our exposition gravitates around several central questions that span various applications: how to characterize the sample efficiency of spectral methods in reaching a target level of statistical accuracy, and how to assess their stability in the face of random noise, missing data, and adversarial corruptions? In addition to conventional $\ell_2$ perturbation analysis, we present a systematic $\ell_{\infty}$ and $\ell_{2,\infty}$ perturbation theory for eigenspace and singular subspaces, which has only recently become available owing to a powerful ""leave-one-out"" analysis framework.",566-806,10.1561/2200000079,http://holt-shepherd.com/register.php
f42c69dbd792155fee6f4d2c525971f8d43f138b,Finding Related Tables in Data Lakes for Interactive Data Science,"Many modern data science applications build on data lakes, schema-agnostic repositories of data files and data products that offer limited organization and management capabilities. There is a need to build data lake search capabilities into data science environments, so scientists and analysts can find tables, schemas, workflows, and datasets useful to their task at hand. We develop search and management solutions for the Jupyter Notebook data science platform, to enable scientists to augment training data, find potential features to extract, clean data, and find joinable or linkable tables. Our core methods also generalize to other settings where computational tasks involve execution of programs or scripts.",35-149,10.1145/3318464.3389726,http://www.lawrence.net/category/
82620503cacf8ff6f8f3490e7bdf7508f1ab2021,Opening practice: supporting reproducibility and critical spatial data science,,477 - 496,10.1007/s10109-020-00334-2,https://www.mclean.info/homepage.html
12f62537251cf8eb76fa11c59df68d2211008898,Big Earth Data science: an information framework for a sustainable planet,"ABSTRACT The digital transformation of our society coupled with the increasing exploitation of natural resources makes sustainability challenges more complex and dynamic than ever before. These changes will unlikely stop or even decelerate in the near future. There is an urgent need for a new scientific approach and an advanced form of evidence-based decision-making towards the benefit of society, the economy, and the environment. To understand the impacts and interrelationships between humans as a society and natural Earth system processes, we propose a new engineering discipline, Big Earth Data science. This science is called to provide the methodologies and tools to generate knowledge from diverse, numerous, and complex data sources necessary to ensure a sustainable human society essential for the preservation of planet Earth. Big Earth Data science aims at utilizing data from Earth observation and social sensing and develop theories for understanding the mechanisms of how such a social-physical system operates and evolves. The manuscript introduces the universe of discourse characterizing this new science, its foundational paradigms and methodologies, and a possible technological framework to be implemented by applying an ecosystem approach. CASEarth and GEOSS are presented as examples of international implementation attempts. Conclusions discuss important challenges and collaboration opportunities.",743 - 767,10.1080/17538947.2020.1743785,https://hunt.com/explore/main/wp-content/about.htm
a4b6f802b3f416fb1af6d723e0549c5e6d34faae,Data science in economics: comprehensive review of advanced machine learning and deep learning methods,"This paper provides a state-of-the-art investigation of advances in data science in emerging economic applications. The analysis was performed on novel data science methods in four individual classes of deep learning models, hybrid deep learning models, hybrid machine learning, and ensemble models. Application domains include a wide and diverse range of economics research from the stock market, marketing, and e-commerce to corporate banking and cryptocurrency. Prisma method, a systematic literature review methodology, was used to ensure the quality of the survey. The findings reveal that the trends follow the advancement of hybrid models, which, based on the accuracy metric, outperform other learning algorithms. It is further expected that the trends will converge toward the advancements of sophisticated hybrid deep learning models.",76-108,10.21203/rs.3.rs-91905/v1,http://atkins.com/
deb4e0c46f2e389ec5e4528f9dcee643bb6a15fa,Fixed Point Strategies in Data Science,"The goal of this article is to promote the use of fixed point strategies in data science by showing that they provide a simplifying and unifying framework to model, analyze, and solve a great variety of problems. They are seen to constitute a natural environment to explain the behavior of advanced convex optimization methods as well as of recent nonlinear methods in data science which are formulated in terms of paradigms that go beyond minimization concepts and involve constructs such as Nash equilibria or monotone inclusions. We review the pertinent tools of fixed point theory and describe the main state-of-the-art algorithms for provenly convergent fixed point construction. We also incorporate additional ingredients such as stochasticity, block-implementations, and non-Euclidean metrics, which provide further enhancements. Applications to signal and image processing, machine learning, statistics, neural networks, and inverse problems are discussed.",3878-3905,10.1109/TSP.2021.3069677,http://hahn.org/post.htm
0669286d8d4ca8ec2fdf16b7813157c21eb690be,Heidelberg colorectal data set for surgical data science in the sensor operating room,,42-104,10.1038/s41597-021-00882-2,https://ferguson.biz/category/search/index.htm
5b9ea2abf1c5a04b3024367409284edceb741ef2,A new paradigm for accelerating clinical data science at Stanford Medicine,"Stanford Medicine is building a new data platform for our academic research community to do better clinical data science. Hospitals have a large amount of patient data and researchers have demonstrated the ability to reuse that data and AI approaches to derive novel insights, support patient care, and improve care quality. However, the traditional data warehouse and Honest Broker approaches that are in current use, are not scalable. We are establishing a new secure Big Data platform that aims to reduce time to access and analyze data. In this platform, data is anonymized to preserve patient data privacy and made available preparatory to Institutional Review Board (IRB) submission. Furthermore, the data is standardized such that analysis done at Stanford can be replicated elsewhere using the same analytical code and clinical concepts. Finally, the analytics data warehouse integrates with a secure data science computational facility to support large scale data analytics. The ecosystem is designed to bring the modern data science community to highly sensitive clinical data in a secure and collaborative big data analytics environment with a goal to enable bigger, better and faster science.",28-149,17-647017-4,https://brown.com/list/categories/main.php
648ba966b63975c6859e1948ae3ddc30053884e4,Making data science systems work,"How are data science systems made to work? It may seem that whether a system works is a function of its technical design, but it is also accomplished through ongoing forms of discretionary work by many actors. Based on six months of ethnographic fieldwork with a corporate data science team, we describe how actors involved in a corporate project negotiated what work the system should do, how it should work, and how to assess whether it works. These negotiations laid the foundation for how, why, and to what extent the system ultimately worked. We describe three main findings. First, how already-existing technologies are essential reference points to determine how and whether systems work. Second, how the situated resolution of development challenges continually reshapes the understanding of how and whether systems work. Third, how business goals, and especially their negotiated balance with data science imperatives, affect a system’s working. We conclude with takeaways for critical data studies, orienting researchers to focus on the organizational and cultural aspects of data science, the third-party platforms underlying data science systems, and ways to engage with practitioners’ imagination of how systems can and should work.",50-146,10.1177/2053951720939605,http://www.bryant-wallace.net/
d83a99bfb6f81565a186e0eb86858864568c1327,Data Science: Challenges and Directions,"While data science has emerged as a contentious new scientific field, enormous debates and discussions have been made on it why we need data science and what makes it as a science. However, only a limited number of discussions are about intrinsic complexities and intelligence embedded in data science problems, and the gaps and opportunities for disciplinary directions. After a comprehensive review [5, 6, 9, 10, 12, 15, 18] of hundreds of literature that directly incorporates data science in their scopes, we make the following observations of the big data buzz and data science debate:",90-142,330-66312-5,http://montgomery-harrington.com/login/
4b5505a54799d796ae94115409b01ee33a7e2b20,Glossary for public health surveillance in the age of data science,"Public health surveillance is the ongoing systematic collection, analysis and interpretation of data, closely integrated with the timely dissemination of the resulting information to those responsible for preventing and controlling disease and injury. With the rapid development of data science, encompassing big data and artificial intelligence, and with the exponential growth of accessible and highly heterogeneous health-related data, from healthcare providers to user-generated online content, the field of surveillance and health monitoring is changing rapidly. It is, therefore, the right time for a short glossary of key terms in public health surveillance, with an emphasis on new data-science developments in the field.",612 - 616,10.1136/jech-2018-211654,http://hernandez.com/tag/category/app/faq/
41cf91ee13a1d15983ede066ddf6b67cc94a41f4,The Role of Academia in Data Science Education,"As the demand for data scientists continues to grow, universities are trying to figure out how to best contribute to the training of a workforce. However, there does not appear to be a consensus on the fundamental principles, expertise, skills, or knowledge-base needed to define an academic discipline. We argue that data science is not a discipline but rather an umbrella term used to describe a complex process involving not one data scientist possessing all the necessary expertise, but a team of data scientists with nonoverlapping complementary skills. We provide some recommendations for how to take this into account when designing data science academic programs.Keywords: applied statistics, data science, data science curriculum, data wrangling, machine learning, software engineering",99-107,10.1162/99608f92.dd363929,http://www.wright-cannon.com/category.html
79be83a308a9a75ef4e64f63a938b201531c0bbf,Data Science: A Comprehensive Overview,,47-145,278-42207-2,https://www.johnson.com/tags/about/
b017bf6879e57077b4b4e180a02747b89878d7a1,A Fresh Look at Introductory Data Science,"ABSTRACT The proliferation of vast quantities of available datasets that are large and complex in nature has challenged universities to keep up with the demand for graduates trained in both the statistical and the computational set of skills required to effectively plan, acquire, manage, analyze, and communicate the findings of such data. To keep up with this demand, attracting students early on to data science as well as providing them a solid foray into the field becomes increasingly important. We present a case study of an introductory undergraduate course in data science that is designed to address these needs. Offered at Duke University, this course has no prerequisites and serves a wide audience of aspiring statistics and data science majors as well as humanities, social sciences, and natural sciences students. We discuss the unique set of challenges posed by offering such a course, and in light of these challenges, we present a detailed discussion into the pedagogical design elements, content, structure, computational infrastructure, and the assessment methodology of the course. We also offer a repository containing all teaching materials that are open-source, along with supplementary materials and the R code for reproducing the figures found in the article.",S16 - S26,10.1080/10691898.2020.1804497,https://www.allen.biz/
c016852106ac787678105fd9dd22e57ba620517c,Human Data Science,,45-146,10.1016/j.patter.2020.100069,http://www.curtis-garcia.com/terms/
b85ac20631159ca3e370afa9c1f81a4618242b4f,The Democratization of Data Science Education,"Abstract Over the last three decades, data have become ubiquitous and cheap. This transition has accelerated over the last five years and training in statistics, machine learning, and data analysis has struggled to keep up. In April 2014, we launched a program of nine courses, the Johns Hopkins Data Science Specialization, which has now had more than 4 million enrollments over the past five years. Here, the program is described and compared to standard data science curricula as they were organized in 2014 and 2015. We show that novel pedagogical and administrative decisions introduced in our program are now standard in online data science programs. The impact of the Data Science Specialization on data science education in the U.S. is also discussed. Finally, we conclude with some thoughts about the future of data science education in a data democratized world.",01-Jul,10.1080/00031305.2019.1668849,https://scott.info/
05ceac747bec286129818e9e2fbd37ef034f5ada,Interrogating Data Science,"Data science provides powerful tools and methods. CSCW researchers have contributed insightfulstudies of conventional work-practices in data science - and particularly machine learning. However,recent research has shown that human skills and collaborative decision-making, play important rolesin defining data, acquiring data, curating data, designing data, and creating data. This workshopgathers researchers and practitioners together to take a collective and critical look at data sciencework-practices, and at how those work-practices make crucial and often invisible impacts on theformal work of data science. When we understand the human and social contributions to data sciencepipelines, we can constructively redesign both work and technologies for new insights, theories, andchallenges.",22-125,10.1145/3406865.3418584,https://joyce.com/faq/
62a9d4f1763c5071cb2476c100614ba9741f036b,Data science applications to string theory,,1-117,10.1016/j.physrep.2019.09.005,https://www.gonzalez-white.com/explore/explore/main/
d88d39dd9c910105e7503aa43698c806d42d5198,Statistical Foundations of Data Science,,25-103,10.1201/9780429096280,http://ryan.info/homepage.htm
9246a5249d05d160d20db8f165a4cac08c93e355,Data Science,,73-119,10.1007/978-3-658-34825-0_2,http://www.adams.com/tag/register.jsp
3bef85eb5c6d2697f8c4a01fe247e19ed3e85e6a,Data Science,,71-142,10.47716/mtspl.b.6.2021.9788194707011,http://www.garcia.com/login/
398b154013db9d8025bf60f910bc156dedd9b40e,"How Data Science Workers Work with Data: Discovery, Capture, Curation, Design, Creation","With the rise of big data, there has been an increasing need for practitioners in this space and an increasing opportunity for researchers to understand their workflows and design new tools to improve it. Data science is often described as data-driven, comprising unambiguous data and proceeding through regularized steps of analysis. However, this view focuses more on abstract processes, pipelines, and workflows, and less on how data science workers engage with the data. In this paper, we build on the work of other CSCW and HCI researchers in describing the ways that scientists, scholars, engineers, and others work with their data, through analyses of interviews with 21 data science professionals. We set five approaches to data along a dimension of interventions: Data as given; as captured; as curated; as designed; and as created. Data science workers develop an intuitive sense of their data and processes, and actively shape their data. We propose new ways to apply these interventions analytically, to make sense of the complex activities around data practices.",29-139,10.1145/3290605.3300356,https://www.moore-payne.net/app/wp-content/category/login.htm
a11e157cb828b800426223f0a3d79e8fb122c8cc,Process Mining for Python (PM4Py): Bridging the Gap Between Process- and Data Science,"Process mining, i.e., a sub-field of data science focusing on the analysis of event data generated during the execution of (business) processes, has seen a tremendous change over the past two decades. Starting off in the early 2000's, with limited to no tool support, nowadays, several software tools, i.e., both open-source, e.g., ProM and Apromore, and commercial, e.g., Disco, Celonis, ProcessGold, etc., exist. The commercial process mining tools provide limited support for implementing custom algorithms. Moreover, both commercial and open-source process mining tools are often only accessible through a graphical user interface, which hampers their usage in large-scale experimental settings. Initiatives such as RapidProM provide process mining support in the scientific workflow-based data science suite RapidMiner. However, these offer limited to no support for algorithmic customization. In the light of the aforementioned, in this paper, we present a novel process mining library, i.e. Process Mining for Python (PM4Py) that aims to bridge this gap, providing integration with state-of-the-art data science libraries, e.g., pandas, numpy, scipy and scikit-learn. We provide a global overview of the architecture and functionality of PM4Py, accompanied by some representative examples of its usage.",58-105,260-35657-2,https://www.mills.com/wp-content/search/home.asp
8ece479b5dfed4727d2d9b9763f777bb9a94096e,Human-AI Collaboration in Data Science,"The rapid advancement of artificial intelligence (AI) is changing our lives in many ways. One application domain is data science. New techniques in automating the creation of AI, known as AutoAI or AutoML, aim to automate the work practices of data scientists. AutoAI systems are capable of autonomously ingesting and pre-processing data, engineering new features, and creating and scoring models based on a target objectives (e.g. accuracy or run-time efficiency). Though not yet widely adopted, we are interested in understanding how AutoAI will impact the practice of data science. We conducted interviews with 20 data scientists who work at a large, multinational technology company and practice data science in various business settings. Our goal is to understand their current work practices and how these practices might change with AutoAI. Reactions were mixed: while informants expressed concerns about the trend of automating their jobs, they also strongly felt it was inevitable. Despite these concerns, they remained optimistic about their future job security due to a view that the future of data science work will be a collaboration between humans and AI systems, in which both automation and human expertise are indispensable.",Jan-24,10.1145/3359313,https://perry.com/main.php
3b16bcb226bb1c87a6e63e0658be30067ed03f57,A Systematic Review on Supervised and Unsupervised Machine Learning Algorithms for Data Science,,27-113,10.1007/978-3-030-22475-2_1,http://vaughn-fox.com/category.php
27b9d1182e913decc7ef6a3509245fa6b6fd509d,Veridical data science,"Significance Predictability, computability, and stability (PCS) are three core principles of data science. They embed the scientific principles of prediction and replication in data-driven decision making while recognizing the central role of computation. Based on these principles, we propose the PCS framework, including workflow and documentation (in R Markdown or Jupyter Notebook). The PCS framework aims at responsible, reliable, reproducible, and transparent analysis across fields of science, social science, engineering, business, and government. It can be used as a recommendation system for scientific hypothesis generation and experimental design. In particular, we propose (basic) PCS inference for reliability measures on data results, extending statistical inference to a much broader scope as current data science practice entails. Building and expanding on principles of statistics, machine learning, and scientific inquiry, we propose the predictability, computability, and stability (PCS) framework for veridical data science. Our framework, composed of both a workflow and documentation, aims to provide responsible, reliable, reproducible, and transparent results across the data science life cycle. The PCS workflow uses predictability as a reality check and considers the importance of computation in data collection/storage and algorithm design. It augments predictability and computability with an overarching stability principle. Stability expands on statistical uncertainty considerations to assess how human judgment calls impact data results through data and model/algorithm perturbations. As part of the PCS workflow, we develop PCS inference procedures, namely PCS perturbation intervals and PCS hypothesis testing, to investigate the stability of data results relative to problem formulation, data cleaning, modeling decisions, and interpretations. We illustrate PCS inference through neuroscience and genomics projects of our own and others. Moreover, we demonstrate its favorable performance over existing methods in terms of receiver operating characteristic (ROC) curves in high-dimensional, sparse linear model simulations, including a wide range of misspecified models. Finally, we propose PCS documentation based on R Markdown or Jupyter Notebook, with publicly available, reproducible codes and narratives to back up human choices made throughout an analysis. The PCS workflow and documentation are demonstrated in a genomics case study available on Zenodo.",3920 - 3929,10.1073/pnas.1901326117,http://williams.com/posts/about.html
e2055b85dab66c922ccf25a28046e8e559074824,Algorithmic Government: Automating Public Services and Supporting Civil Servants in using Data Science Technologies,"The data science technologies of artificial intelligence (AI), Internet of Things (IoT), big data and behavioral/predictive analytics, and blockchain are poised to revolutionize government and create a new generation of GovTech start-ups. The impact from the ‘smartification’ of public services and the national infrastructure will be much more significant in comparison to any other sector given government's function and importance to every institution and individual. Potential GovTech systems include Chatbots and intelligent assistants for public engagement, Robo-advisors to support civil servants, real-time management of the national infrastructure using IoT and blockchain, automated compliance/regulation, public records securely stored in blockchain distributed ledgers, online judicial and dispute resolution systems, and laws/statutes encoded as blockchain smart contracts. Government is potentially the major ‘client’ and also ‘public champion’ for these new data technologies. This review paper uses our simple taxonomy of government services to provide an overview of data science automation being deployed by governments world-wide. The goal of this review paper is to encourage the Computer Science community to engage with government to develop these new systems to transform public services and support the work of civil servants.",448-460,10.1093/COMJNL/BXY082,http://www.baker-moore.com/category/
89f41c87c8849ce37e609c1010087291a4679a37,Outbreak analytics: a developing data science for informing the response to emerging pathogens,"Despite continued efforts to improve health systems worldwide, emerging pathogen epidemics remain a major public health concern. Effective response to such outbreaks relies on timely intervention, ideally informed by all available sources of data. The collection, visualization and analysis of outbreak data are becoming increasingly complex, owing to the diversity in types of data, questions and available methods to address them. Recent advances have led to the rise of outbreak analytics, an emerging data science focused on the technological and methodological aspects of the outbreak data pipeline, from collection to analysis, modelling and reporting to inform outbreak response. In this article, we assess the current state of the field. After laying out the context of outbreak response, we critically review the most common analytics components, their inter-dependencies, data requirements and the type of information they can provide to inform operations in real time. We discuss some challenges and opportunities and conclude on the potential role of outbreak analytics for improving our understanding of, and response to outbreaks of emerging pathogens. This article is part of the theme issue ‘Modelling infectious disease outbreaks in humans, animals and plants: epidemic forecasting and control‘. This theme issue is linked with the earlier issue ‘Modelling infectious disease outbreaks in humans, animals and plants: approaches and important themes’.",92-141,10.1098/rstb.2018.0276,https://odonnell.com/
1ec4d0e29455e47245edaa17368257df3efb6562,"Practitioners Teaching Data Science in Industry and Academia: Expectations, Workflows, and Challenges","Data science has been growing in prominence across both academia and industry, but there is still little formal consensus about how to teach it. Many people who currently teach data science are practitioners such as computational researchers in academia or data scientists in industry. To understand how these practitioner-instructors pass their knowledge onto novices and how that contrasts with teaching more traditional forms of programming, we interviewed 20 data scientists who teach in settings ranging from small-group workshops to large online courses. We found that: 1) they must empathize with a diverse array of student backgrounds and expectations, 2) they teach technical workflows that integrate authentic practices surrounding code, data, and communication, 3) they face challenges involving authenticity versus abstraction in software setup, finding and curating pedagogically-relevant datasets, and acclimating students to live with uncertainty in data analysis. These findings can point the way toward better tools for data science education and help bring data literacy to more people around the world.",77-138,10.1145/3290605.3300493,http://walker.com/search/terms.html
f9e0e85732f0736c0d5a6f0c63df5c7f1f245dcd,From hype to reality: data science enabling personalized medicine,,83-148,10.1186/s12916-018-1122-7,http://www.meyer-bailey.biz/main/categories/post/
678da221aa156807bc2c191ed5f4bcbb0b25d421,Data science ethical considerations: a systematic literature review and proposed project framework,,01-Dec,10.1007/s10676-019-09502-5,https://smith.com/search/blog/blog/register.html
129a5609d4b00ae86f583ffbc1b3680234601b46,Health Care and Precision Medicine Research: Analysis of a Scalable Data Science Platform,"Background Health care data are increasing in volume and complexity. Storing and analyzing these data to implement precision medicine initiatives and data-driven research has exceeded the capabilities of traditional computer systems. Modern big data platforms must be adapted to the specific demands of health care and designed for scalability and growth. Objective The objectives of our study were to (1) demonstrate the implementation of a data science platform built on open source technology within a large, academic health care system and (2) describe 2 computational health care applications built on such a platform. Methods We deployed a data science platform based on several open source technologies to support real-time, big data workloads. We developed data-acquisition workflows for Apache Storm and NiFi in Java and Python to capture patient monitoring and laboratory data for downstream analytics. Results Emerging data management approaches, along with open source technologies such as Hadoop, can be used to create integrated data lakes to store large, real-time datasets. This infrastructure also provides a robust analytics platform where health care and biomedical research data can be analyzed in near real time for precision medicine and computational health care use cases. Conclusions The implementation and use of integrated data science platforms offer organizations the opportunity to combine traditional datasets, including data from the electronic health record, with emerging big data sources, such as continuous patient monitoring and real-time laboratory results. These platforms can enable cost-effective and scalable analytics for the information that will be key to the delivery of precision medicine initiatives. Organizations that can take advantage of the technical advances found in data science platforms will have the opportunity to provide comprehensive access to health care data for computational health care and precision medicine research.",69-136,10.2196/13043,http://rivera-rodriguez.com/list/list/category.html
0c86e8d19d0fc62a5f829ea625ffd3e7fa9551b9,Toward collaborative open data science in metabolomics using Jupyter Notebooks and cloud computing,,42-120,10.1007/s11306-019-1588-0,http://www.rhodes.com/tag/main/category/
5c8b7127ad0b5257f81ce1aa70b89faa97bbc211,Data Science of the Natural Environment: A Research Roadmap,"Data science is the science of extracting meaning from potentially complex data. This is a fast moving field, drawing principles and techniques from a number of different disciplinary areas including computer science, statistics and complexity science. Data science is having a profound impact on a number of areas including commerce, health and smart cities. This paper argues that data science can have an equal if not greater impact in the area of earth and environmental sciences, offering a rich tapestry of new techniques to support both a deeper understanding of the natural environment in all its complexities, as well as the development of well-founded mitigation and adaptation strategies in the face of climate change. The paper argues that data science for the natural environment brings about new challenges for data science, particularly around complexity, spatial and temporal reasoning, and managing uncertainty. The paper also describes a case study in environmental data science which offers up insights into the promise of the area. The paper concludes with a research roadmap highlighting ten top challenges of environmental data science and also an invitation to become part of an international community working collaboratively on these problems.",48-101,10.3389/fenvs.2019.00121,http://smith-reid.org/
e564e3656395782d0ab9f801bfbe9f9f1a5d34a7,Data science in data librarianship: Core competencies of a data librarian,"Currently, data are stored in an always-on condition, and can be globally accessed at any point, by any user. Data librarianship has its origins in the social sciences. In particular, the creation of data services and data archives, in the United Kingdom (Data Archives Services) and in the United States and Canada (Data Library Services), is a key factor for the emergence of data librarianship. The focus of data librarianship nowadays is on the creation of new library services. Data librarians are concerned with the proposition of services for data management and curation in academic libraries and other research organizations. The purpose of this paper is to understand how the complexity of the data can serve as the basis for identifying the technical skills required by data librarians. This essay is systematically divided, first introducing the concepts of data and research data in data librarianship, followed by an overview of data science as a theory, method, and technology to assess data. Next, the identification of the competencies and skills required by data scientists and data librarians are discussed. Our final remarks highlight that data librarians should understand that the complexity and novelty associated with data science praxis. Data science provides new methods and practices for data librarianship. A data librarian need not become a programmer, statistician, or database manager, but should be interested in learning about the languages and programming logic of computers, databases, and information retrieval tools. We believe that numerous kinds of scientific data research provide opportunities for a data librarian to engage with data science.",771 - 780,10.1177/0961000617742465,http://www.cook-torres.info/
590ead4aeddbf8fea8414998b2dc3b74576a71cb,A Second Chance to Get Causal Inference Right: A Classification of Data Science Tasks,"Causal inference from observational data is the goal of many data analyses in the health and social sciences. However, academic statistics has often frowned upon data analyses with a causal objective. The introduction of the term ""data science"" provides a historic opportunity to redefine data analysis in such a way that it naturally accommodates causal inference from observational data. Like others before, we organize the scientific contributions of data science into three classes of tasks: Description, prediction, and counterfactual prediction (which includes causal inference). An explicit classification of data science tasks is necessary to discuss the data, assumptions, and analytics required to successfully accomplish each task. We argue that a failure to adequately describe the role of subject-matter expert knowledge in data analysis is a source of widespread misunderstandings about data science. Specifically, causal analyses typically require not only good data and algorithms, but also domain expert knowledge. We discuss the implications for the use of data science to guide decision-making in the real world and to train data scientists.",42 - 49,10.1080/09332480.2019.1579578,http://stark.com/
2081ed6854290a479f796f2432c7951ff24232fe,Human-Centered Study of Data Science Work Practices,"With the rise of big data, there has been an increasing need to understand who is working in data science and how they are doing their work. HCI and CSCW researchers have begun to examine these questions. In this workshop, we invite researchers to share their observations, experiences, hypotheses, and insights, in the hopes of developing a taxonomy of work practices and open issues in the behavioral and social study of data science and data science workers.",26-121,10.1145/3290607.3299018,https://kim-alexander.org/
005cb0304960c5249579f3aa39a47e2cb164c0b6,Genomics and data science: an application within an umbrella,,49-113,10.1186/s13059-019-1724-1,http://rojas.com/tag/main.php
e799d31e1c2d80a971c1f956d62b98c0a9f27031,Big Data and data science: A critical review of issues for educational research,"Big Data refers to large and disparate volumes of data generated by people, applications and machines. It is gaining increasing attention from a variety of domains, including education. What are the challenges of engaging with Big Data research in education? This paper identifies a wide range of critical issues that researchers need to consider when working with Big Data in education. The issues identified include diversity in the conception and meaning of Big Data in education, ontological, epistemological disparity, technical challenges, ethics and privacy, digital divide and digital dividend, lack of expertise and academic development opportunities to prepare educational researchers to leverage opportunities afforded by Big Data. The goal of this paper is to raise awareness on these issues and initiate a dialogue. The paper was inspired partly by insights drawn from the literature but mostly informed by experience researching into Big Data in education. [ABSTRACT FROM AUTHOR]",101-113,10.1111/bjet.12595,https://www.chen.net/
f56425ec56586dcfd2694ab83643e9e76f314e91,50 Years of Data Science,"ABSTRACT More than 50 years ago, John Tukey called for a reformation of academic statistics. In “The Future of Data Analysis,” he pointed to the existence of an as-yet unrecognized science, whose subject of interest was learning from data, or “data analysis.” Ten to 20 years ago, John Chambers, Jeff Wu, Bill Cleveland, and Leo Breiman independently once again urged academic statistics to expand its boundaries beyond the classical domain of theoretical statistics; Chambers called for more emphasis on data preparation and presentation rather than statistical modeling; and Breiman called for emphasis on prediction rather than inference. Cleveland and Wu even suggested the catchy name “data science” for this envisioned field. A recent and growing phenomenon has been the emergence of “data science” programs at major universities, including UC Berkeley, NYU, MIT, and most prominently, the University of Michigan, which in September 2015 announced a $100M “Data Science Initiative” that aims to hire 35 new faculty. Teaching in these new programs has significant overlap in curricular subject matter with traditional statistics courses; yet many academic statisticians perceive the new programs as “cultural appropriation.” This article reviews some ingredients of the current “data science moment,” including recent commentary about data science in the popular media, and about how/whether data science is really different from statistics. The now-contemplated field of data science amounts to a superset of the fields of statistics and machine learning, which adds some technology for “scaling up” to “big data.” This chosen superset is motivated by commercial rather than intellectual developments. Choosing in this way is likely to miss out on the really important intellectual event of the next 50 years. Because all of science itself will soon become data that can be mined, the imminent revolution in data science is not about mere “scaling up,” but instead the emergence of scientific studies of data analysis science-wide. In the future, we will be able to predict how a proposal to change data analysis workflows would impact the validity of data analysis across all of science, even predicting the impacts field-by-field. Drawing on work by Tukey, Cleveland, Chambers, and Breiman, I present a vision of data science based on the activities of people who are “learning from data,” and I describe an academic field dedicated to improving that activity in an evidence-based manner. This new field is a better academic enlargement of statistics and machine learning than today’s data science initiatives, while being able to accommodate the same short-term goals. Based on a presentation at the Tukey Centennial Workshop, Princeton, NJ, September 18, 2015.",745 - 766,10.1080/10618600.2017.1384734,http://www.hobbs-carpenter.biz/
eaa3bbe9e3c52781fd84149d8ee6e2670c90e5ec,Bayesian Optimization and Data Science,,84-127,10.1007/978-3-030-24494-1,https://www.benson.com/categories/app/main/login/
f670069c81466f625d1d9a315e8cb419a79f2590,Process Mining: Data Science in Action,"This is the second edition of Wil van der Aalsts seminal book on process mining, which now discusses the field also in the broader context of data science and big data approaches. It includes several additions and updates, e.g. on inductive mining techniques, the notion of alignments, a considerably expanded section on software tools and a completely new chapter of process mining in the large. It is self-contained, while at the same time covering the entire process-mining spectrum from process discovery to predictive analytics. After a general introduction to data science and process mining in Part I, Part II provides the basics of business process modeling and data mining necessary to understand the remainder of the book. Next, Part III focuses on process discovery as the most important process mining task, while Part IV moves beyond discovering the control flow of processes, highlighting conformance checking, and organizational and time perspectives. Part V offers a guide to successfully applying process mining in practice, including an introduction to the widely used open-source tool ProM and several commercial products. Lastly, Part VI takes a step back, reflecting on the material presented and the key open challenges. Overall, this book provides a comprehensive overview of the state of the art in process mining. It is intended for business process analysts, business consultants, process managers, graduate students, and BPM researchers.",91-131,676-19351-5,https://mcdaniel.org/app/tags/wp-content/category/
0e23ff1f915b6af32bf1a1107ee7e15ebe10efe8,The Challenge of Big Data and Data Science,"Big data and data science are transforming the world in ways that spawn new concerns for social scientists, such as the impacts of the internet on citizens and the media, the repercussions of smart cities, the possibilities of cyber-warfare and cyber-terrorism, the implications of precision medicine, and the consequences of artificial intelligence and automation. Along with these changes in society, powerful new data science methods support research using administrative, internet, textual, and sensor-audio-video data. Burgeoning data and innovative methods facilitate answering previously hard-to-tackle questions about society by offering new ways to form concepts from data, to do descriptive inference, to make causal inferences, and to generate predictions. They also pose challenges as social scientists must grasp the meaning of concepts and predictions generated by convoluted algorithms, weigh the relative value of prediction versus causal inference, and cope with ethical challenges as their methods, such as algorithms for mobilizing voters or determining bail, are adopted by policy makers.",62-140,10.1146/ANNUREV-POLISCI-090216-023229,http://galloway.com/post/
702cd9a7a128706b8a6ec88e7424e06c326021e5,Upscaling urban data science for global climate solutions,"Non-technical summary Manhattan, Berlin and New Delhi all need to take action to adapt to climate change and to reduce greenhouse gas emissions. While case studies on these cities provide valuable insights, comparability and scalability remain sidelined. It is therefore timely to review the state-of-the-art in data infrastructures, including earth observations, social media data, and how they could be better integrated to advance climate change science in cities and urban areas. We present three routes for expanding knowledge on global urban areas: mainstreaming data collections, amplifying the use of big data and taking further advantage of computational methods to analyse qualitative data to gain new insights. These data-based approaches have the potential to upscale urban climate solutions and effect change at the global scale. Technical summary Cities have an increasingly integral role in addressing climate change. To gain a common understanding of solutions, we require adequate and representative data of urban areas, including data on related greenhouse gas emissions, climate threats and of socio-economic contexts. Here, we review the current state of urban data science in the context of climate change, investigating the contribution of urban metabolism studies, remote sensing, big data approaches, urban economics, urban climate and weather studies. We outline three routes for upscaling urban data science for global climate solutions: 1) Mainstreaming and harmonizing data collection in cities worldwide; 2) Exploiting big data and machine learning to scale solutions while maintaining privacy; 3) Applying computational techniques and data science methods to analyse published qualitative information for the systematization and understanding of first-order climate effects and solutions. Collaborative efforts towards a joint data platform and integrated urban services would provide the quantitative foundations of the emerging global urban sustainability science.",83-119,10.1017/sus.2018.16,http://dunn-baker.com/post.htm
c0b1eedfa2031a69fbdf02a4abc8a741faf6a912,INTRODUCTION TO DATA SCIENCE,データサイエンスの概説 データ獲得法 国と統計データ データの形式と分析の準備 データの変換と加工 データをグラフに表す 統計量によるデータの要約 ２次元の量的データの記述 回帰分析の基礎 質的データの分析 社会現象データの分析事例 データ解析のソフトウェア,48-143,10.1002/9781119526865.ch1,http://paul-vargas.biz/app/categories/home.htm
a9e447d4d6b91f75ac4d8e336c609cfcbcbcfc0a,Data Science,"Program of Study The technological revolution has led to an explosion of data in domains of knowledge including medicine, policy, social sciences, commerce, and the natural sciences. Petabytes of data are being collected from a myriad of instruments, like sequencing machines for genomics and mobile devices for quantifying social interactions. In addition to driving research, data are shaping the way people work, live, and communicate. Correspondingly, new methodologies have emerged to power intelligent systems, make more accurate predictions, and gain new insight using the large volumes of data generated by scientists, entrepreneurs, and analysts.",83-119,10.1002/9781119544180,http://www.welch.com/faq.htm
2ab2796390ac12df283e218907ed0ffef232dbc7,Situating Data Science: Exploring How Relationships to Data Shape Learning,"The emerging field of Data Science has had a large impact on science and society. This has led to over a decade of calls to establish a corresponding field of Data Science Education. There is still a need, however, to more deeply conceptualize what a field of Data Science Education might entail in terms of scope, responsibility, and execution. This special issue explores how one distinguishing feature of Data Science—its focus on data collected from social and environmental contexts within which learners often find themselves deeply embedded—suggests serious implications for learning and education. The learning sciences is uniquely positioned to investigate how such contextual embeddings impact learners’ engagement with data including conceptual, experiential, communal, racialized, spatial, and political dimensions. This special issue demonstrates the richly layered relationships learners build with data and reveals them to be not merely utilitarian mechanisms for learning about data, but a critical part of navigating data as social text and understanding Data Science as a discipline. Together, the contributions offer a vision of how the learning sciences can contribute to a more expansive, agentive and socially aware Data Science Education.",01-Oct,10.1080/10508406.2019.1705664,http://klein.org/list/app/login.htm
d6801d0ffd08ba56bccfa01884bb6a126f99de2e,"Our data, our society, our health: A vision for inclusive and transparent health data science in the United Kingdom and beyond","The last 6 years have seen sustained investment in health data science in the United Kingdom and beyond, which should result in a data science community that is inclusive of all stakeholders, working together to use data to benefit society through the improvement of public health and well‐being.",87-129,10.1002/lrh2.10191,http://www.cook.com/posts/main/tags/post.php
8d446e7af03d7c7f9fe5828b2d9939e23a3ed7b0,A Data Science Framework for Movement,"Author(s): Dodge, S | Abstract: © 2019 The Ohio State University Movement is the driving force behind the form and function of many ecological and human systems. Identification and analysis of movement patterns that may relate to the behavior of individuals and their interactions is a fundamental first step in understanding these systems. With advances in IoT and the ubiquity of smart connected sensors to collect movement and contextual data, we now have access to a wealth of geo-enriched high-resolution tracking data. These data promise new forms of knowledge and insight into movement of humans, animals, and goods, and hence can increase our understanding of complex spatiotemporal processes such as disease outbreak, urban mobility, migration, and human-species interaction. To take advantage of the evolution in our data, we need a revolution in how we visualize, model, and analyze movement as a multidimensional process that involves space, time, and context. This paper introduces a data science paradigm with the aim of advancing research on movement.",17-131,10.1111/GEAN.12212,https://sparks-martin.com/
747359803e9a734fa4f1338a83121a942f3da60e,Geographic Data Science,"It is widely acknowledged that the emergence of “Big Data” is having a profound and often controversial impact on the production of knowledge. In this context, Data Science has developed as an interdisciplinary approach that turns such “Big Data” into information. This article argues for the positive role that Geography can have on Data Science when being applied to spatially explicit problems; and inversely, makes the case that there is much that Geography and Geographical Analysis could learn from Data Science. We propose a deeper integration through an ambitious research agenda, including systems engineering, new methodological development, and work toward addressing some acute challenges around epistemology. We argue that such issues must be resolved in order to realize a Geographic Data Science, and that such goal would be a desirable one.",86-109,10.1111/GEAN.12194,https://williams.com/main/privacy.asp
863a35bdd1ae803491801e283c2ae79fe973cf68,Microbiome data science,,44-145,10.1007/s12038-019-9930-2,https://kelly.org/search/category.htm
4f0218eb9ed62d5acc03f02bfa24b388a66067e8,Distance geometry and data science,,271-339,10.1007/s11750-020-00563-0,http://www.wilson.info/tag/app/faq.html
bb44d1472bb281c699ef556f6eb6ccc66889f2d3,Data Science and Machine Learning,"The purpose of Data Science and Machine Learning: Mathematical and Statistical Methods is to provide an accessible, yet comprehensive textbook intended for students interested in gaining a better understanding of the mathematics and statistics that underpin the rich variety of ideas and machine learning algorithms in data science.",20-142,10.1201/9780367816971,http://www.cohen.com/login/
da1de6fa2d15e50c9e96811217e27a66313bd762,A First Course in Data Science,"Abstract Data science is a discipline that provides principles, methodology, and guidelines for the analysis of data for tools, values, or insights. Driven by a huge workforce demand, many academic institutions have started to offer degrees in data science, with many at the graduate, and a few at the undergraduate level. Curricula may differ at different institutions, because of varying levels of faculty expertise, and different disciplines (such as mathematics, computer science, and business) in developing the curriculum. The University of Massachusetts Dartmouth started offering degree programs in data science from Fall 2015, at both the undergraduate and the graduate level. Quite a few articles have been published that deal with graduate data science courses, much less so dealing with undergraduate ones. Our discussion will focus on undergraduate course structure and function, and specifically, a first course in data science. Our design of this course centers around a concept called the data science life cycle. That is, we view tasks or steps in the practice of data science as forming a process, consisting of states that indicate how it comes into life, how different tasks in data science depend on or interact with others until the birth of a data product or a conclusion. Naturally, different pieces of the data science life cycle then form individual parts of the course. Details of each piece are filled up by concepts, techniques, or skills that are popular in industry. Consequently, the design of our course is both “principled” and practical. A significant feature of our course philosophy is that, in line with activity theory, the course is based on the use of tools to transform real data to answer strongly motivated questions related to the data.",99 - 109,10.1080/10691898.2019.1623136,https://smith-cook.org/categories/app/category.asp
daec8baf1740a09725b375729d95caebc42f61c8,ACM Task Force on Data Science Education: Draft Report and Opportunity for Feedback,"The ACM Data Science Task Force was established by the ACM Education Council and tasked with articulating the role of computing discipline-specific contributions to this emerging field. This special session seeks to introduce the work of the ACM Data Science Task Force as well as to engage the SIGCSE community in this effort. Members of the task force will introduce key components of a draft report, including a summary of data science curricular efforts to date, results of ACM academic and industry surveys on data science, as well as the initial articulation of computing competencies for undergraduate programs in data science. This session should be of interest to all SIGCSE attendees, but especially faculty developing college-level curricula in Data Science.",99-132,10.1145/3287324.3287522,https://santiago.com/posts/blog/search/category.html
bf12943b1862cbdf556ba1ddcdbc685d4f38a6c3,Realizing the potential of data science,"Data science promises new insights, helping transform information into knowledge that can drive science and industry.",67 - 72,10.1145/3188721,http://www.hart-lang.net/
fb566f2001e44a65433fb7cc2eb7bcf6513a7db8,The 9 Pitfalls of Data Science,"Scientific rigor and critical thinking skills are indispensable in this age of big data because machine learning and artificial intelligence are often led astray by meaningless patterns. The 9 Pitfalls of Data Science is loaded with entertaining real-world examples of both successful and misguided approaches to interpreting data, both grand successes and epic failures. Anyone can learn to distinguish between good data science and nonsense. We are confident that readers will learn how to avoid being duped by data, and make better, more informed decisions. Whether they want to be effective creators, interpreters, or users of data, they need to know the nine pitfalls of data science.",68-114,10.1093/oso/9780198844396.001.0001,http://may-smith.org/
08468bac470e5c2cbbd2b66e8e7cf2ab65f38e02,Data Science for Local Government,"The Data Science for Local Government project was about understanding how the growth of ‘data science’ is changing the way that local government works in the UK. We define data science as a dual shift which involves both bringing in new decision making and analytical techniques to local government work (e.g. machine learning and predictive analytics, artificial intelligence and A/B testing) and also expanding the types of data local government makes use of (for example, by repurposing administrative data, harvesting social media data, or working with mobile phone companies). The emergence of data science is facilitated by the growing availability of free, open-source tools for both collecting data and performing analysis. Based on extensive documentary review, a nationwide survey of local authorities, and in-depth interviews with over 30 practitioners, we have sought to produce a comprehensive guide to the different types of data science being undertaken in the UK, the types of opportunities and benefits created, and also some of the challenges and difficulties being encountered. Our aim was to provide a basis for people working in local government to start on their own data science projects, both by providing a library of dozens of ideas which have been tried elsewhere and also by providing hints and tips for overcoming key problems and challenges.",83-106,10.2139/ssrn.3370217,http://www.long.biz/blog/tag/tag/main/
46f1c45c62b7dbf77af405f5ddcf137b5e1ddde9,Data science from a library and information science perspective,"
Purpose
Data science is a relatively new field which has gained considerable attention in recent years. This new field requires a wide range of knowledge and skills from different disciplines including mathematics and statistics, computer science and information science. The purpose of this paper is to present the results of the study that explored the field of data science from the library and information science (LIS) perspective.


Design/methodology/approach
Analysis of research publications on data science was made on the basis of papers published in the Web of Science database. The following research questions were proposed: What are the main tendencies in publication years, document types, countries of origin, source titles, authors of publications, affiliations of the article authors and the most cited articles related to data science in the field of LIS? What are the main themes discussed in the publications from the LIS perspective?


Findings
The highest contribution to data science comes from the computer science research community. The contribution of information science and library science community is quite small. However, there has been continuous increase in articles from the year 2015. The main document types are journal articles, followed by conference proceedings and editorial material. The top three journals that publish data science papers from the LIS perspective are the Journal of the American Medical Informatics Association, the International Journal of Information Management and the Journal of the Association for Information Science and Technology. The top five countries publishing are USA, China, England, Australia and India. The most cited article has got 112 citations. The analysis revealed that the data science field is quite interdisciplinary by nature. In addition to the field of LIS the papers belonged to several other research areas. The reviewed articles belonged to the six broad categories: data science education and training; knowledge and skills of the data professional; the role of libraries and librarians in the data science movement; tools, techniques and applications of data science; data science from the knowledge management perspective; and data science from the perspective of health sciences.


Research limitations/implications
The limitations of this research are that this study only analyzed research papers in the Web of Science database and therefore only covers a certain amount of scientific papers published in the field of LIS. In addition, only publications with the term “data science” in the topic area of the Web of Science database were analyzed. Therefore, several relevant studies are not discussed in this paper that are not reflected in the Web of Science database or were related to other keywords such as “e-science,” “e-research,” “data service,” “data curation” or “research data management.”


Originality/value
The field of data science has not been explored using bibliographic analysis of publications from the perspective of the LIS. This paper helps to better understand the field of data science and the perspectives for information professionals.
",422-441,10.1108/dta-05-2019-0076,https://lee.com/author/
4aeda303fa0b9beae3f6d65e052dace9d4540116,Data Science Support at the Academic Library,"Abstract Data science is a rapidly growing field with applications across all scientific domains. The demand for support in data science literacy is outpacing available resources at college campuses. The academic library is uniquely positioned to provide training and guidance in a number of areas relevant to data science. The University of Arizona Libraries has built a successful data science support program, focusing on computational literacy, geographic information systems, and reproducible science. Success of the program has largely been due to the strength of library personnel and strategic partnerships with units outside of the library. Academic libraries can support campus data science needs through professional development of current staff and recruitment of new personnel with expertise in data-intensive domains.",241 - 257,10.1080/01930826.2019.1583015,https://www.sandoval.com/
36708c11c2fde2efb50e75d81f174b2c205082c8,What is responsible and sustainable data science?,"In the expansion of health ecosystems, issues of responsibility and sustainability of the data science involved are central. The idea that these values should be central to the practice of data science is increasingly gaining traction, yet there is no agreement on what exactly makes data science responsible or sustainable because these concepts prove slippery when applied to a global field involving commercial, academic and governmental actors. This lack of clarity is causing problems in setting goals and boundaries for data scientific practice, and risks fundamental disagreement on governance principles for this emerging field. We will argue in this commentary for a commons analytical framework as one approach to this problem, since it offers useful signposts for how to establish governance principles for shared resources.",88-135,10.1177/2053951719858114,https://delacruz.net/faq/
e8547557eeea9c9a334edfc4bfa4bbe3774e24d5,Machine learning and data science in soft materials engineering,"In many branches of materials science it is now routine to generate data sets of such large size and dimensionality that conventional methods of analysis fail. Paradigms and tools from data science and machine learning can provide scalable approaches to identify and extract trends and patterns within voluminous data sets, perform guided traversals of high-dimensional phase spaces, and furnish data-driven strategies for inverse materials design. This topical review provides an accessible introduction to machine learning tools in the context of soft and biological materials by ‘de-jargonizing’ data science terminology, presenting a taxonomy of machine learning techniques, and surveying the mathematical underpinnings and software implementations of popular tools, including principal component analysis, independent component analysis, diffusion maps, support vector machines, and relative entropy. We present illustrative examples of machine learning applications in soft matter, including inverse design of self-assembling materials, nonlinear learning of protein folding landscapes, high-throughput antimicrobial peptide design, and data-driven materials design engines. We close with an outlook on the challenges and opportunities for the field.",63-109,10.1088/1361-648X/aa98bd,https://bauer.info/home/
ffdb6039a5d82f8edd70b2d177074c2f2c89e97f,Data Science as Political Action: Grounding Data Science in a Politics of Justice,"In response to recent controversies, the field of data science has rushed to adopt codes of ethics. Such professional codes, however, are ill-equipped to address broad matters of social justice. Instead of ethics codes, I argue, the field must embrace politics. Data scientists must recognize themselves as political actors engaged in normative constructions of society and, as befits political work, evaluate their work according to its downstream material impacts on people's lives. I justify this notion in two parts: first, by articulating why data scientists must recognize themselves as political actors, and second, by describing how the field can evolve toward a deliberative and rigorous grounding in a politics of social justice. Part 1 responds to three arguments that are commonly invoked by data scientists when they are challenged to take political positions regarding their work. In confronting these arguments, I will demonstrate why attempting to remain apolitical is itself a political stance--a fundamentally conservative one--and why the field's current attempts to promote ""social good"" dangerously rely on vague and unarticulated political assumptions. Part 2 proposes a framework for what a politically-engaged data science could look like and how to achieve it, recognizing the challenge of reforming the field in this manner. I conceptualize the process of incorporating politics into data science in four stages: becoming interested in directly addressing social issues, recognizing the politics underlying these issues, redirecting existing methods toward new applications, and, finally, developing new practices and methods that orient data science around a mission of social justice. The path ahead does not require data scientists to abandon their technical expertise, but it does entail expanding their notions of what problems to work on and how to engage with society.",249-265,10.23919/JSC.2021.0029,http://snyder.com/search.jsp
305600f3cba8a63bad1bedeab34a299bf748754b,Northstar: An Interactive Data Science System,"In order to democratize data science, we need to fundamentally rethink the current analytics stack, from the user interface to the ""guts."" Most importantly, enabling a broader range of users to unfold the potential of (their) data requires a change in the interface and the ""protection"" we offer them. On the one hand, visual interfaces for data science have to be intuitive, easy, and interactive to reach users without a strong background in computer science or statistics. On the other hand, we need to protect users from making false discoveries. Furthermore, it requires that technically involved (and often boring) tasks have to be automatically done by the system so that the user can focus on contributing their domain expertise to the problem. In this paper, we present Northstar, the Interactive Data Science System, which we have developed over the last 4 years to explore designs that make advanced analytics and model building more accessible.",2150-2164,10.14778/3229863.3240493,http://www.figueroa.com/category.asp
e1c8f86668d3e37e430f187b7fd91d1643a0a0ff,Theory-Guided Data Science: A New Paradigm for Scientific Discovery from Data,"Data science models, although successful in a number of commercial domains, have had limited applicability in scientific problems involving complex physical phenomena. Theory-guided data science (TGDS) is an emerging paradigm that aims to leverage the wealth of scientific knowledge for improving the effectiveness of data science models in enabling scientific discovery. The overarching vision of TGDS is to introduce scientific consistency as an essential component for learning generalizable models. Further, by producing scientifically interpretable models, TGDS aims to advance our scientific understanding by discovering novel domain insights. Indeed, the paradigm of TGDS has started to gain prominence in a number of scientific disciplines such as turbulence modeling, material discovery, quantum chemistry, bio-medical science, bio-marker discovery, climate science, and hydrology. In this paper, we formally conceptualize the paradigm of TGDS and present a taxonomy of research themes in TGDS. We describe several approaches for integrating domain knowledge in different research themes using illustrative examples from different disciplines. We also highlight some of the promising avenues of novel research for realizing the full potential of theory-guided data science.",2318-2331,10.1109/TKDE.2017.2720168,https://schroeder.com/author.htm
0a4b3c33e830d8cde364443a52e673c2c07dcfe8,Open Data Science,,31-39,10.1007/978-3-030-01768-2_3,http://www.sandoval-brown.com/tags/category/index.html
260ef5a455afcb8d052866bdf2fb28996ab8bbad,Data Science: the impact of statistics,,189-194,10.1007/s41060-018-0102-5,http://pope-grimes.com/tag/search/faq/
2146edb37621d80f53c1261c8a53c94d3dda84c8,Smart Blockchain Badges for Data Science Education,"Blockchain technology has the potential to revolutionise education in a number of ways. In this paper, we explore the applications of Smart Blockchain Badges on data science education. In particular, we investigate how Smart Blockchain Badges can support learners that want to advance their careers in data science, by offering them personalised recommendations based on their learning achievements. This work aims at enhancing data science accreditation by introducing a robust system based on the Blockchain technology. Learners will benefit from a sophisticated, open and transparent accreditation system, as well as from receiving job recommendations that match their skills and can potentially progress their careers. As a result, this work contributes towards closing the data science skills gap by linking data science education to the industry.",01-May,10.1109/FIE.2018.8659012,http://richardson.com/author.html
577564ac25a12b37972d77a35b589f6b2270a45f,Big Data and Data Science in Critical Care.,,"
          1239-1248
        ",10.1016/j.chest.2018.04.037,https://www.miller-jones.com/index.php
a2d7efb8b174702111e713765cbf741dff2bf9b8,Searching for Hidden Perovskite Materials for Photovoltaic Systems by Combining Data Science and First Principle Calculations,"Undiscovered perovskite materials for applications in capturing solar lights are explored through the implementation of data science. In particular, 15000 perovskite materials data is analyzed where visualization of the data reveals hidden trends and clustering of data. Random forest classification within machine learning is used in order to predict the band gap of perovskite materials where 18 physical descriptors are revealed to determine the band gap. With trained random forest, 9328 perovskite materials with potential for applications in solar cell materials are predicted. The selected Li and Na based perovskite materials within predicted 9328 perovskite materials are evaluated with first principle calculations where 11 undiscovered Li(Na) based perovskite materials fall into the ideal band gap and formation energy ranges for solar cell applications. Thus, the implementation of data science accelerates the discovery of hidden perovskite materials and the approach can be applied to the materials scienc...",86-132,10.1021/ACSPHOTONICS.7B01479,http://www.graham.com/about.html
140a6476f7b8dde9e7bbcd199d248fc629721faa,Trust in Data Science,"The trustworthiness of data science systems in applied and real-world settings emerges from the resolution of specific tensions through situated, pragmatic, and ongoing forms of work. Drawing on research in CSCW, critical data studies, and history and sociology of science, and six months of immersive ethnographic fieldwork with a corporate data science team, we describe four common tensions in applied data science work: (un)equivocal numbers, (counter)intuitive knowledge, (in)credible data, and (in)scrutable models. We show how organizational actors establish and re-negotiate trust under messy and uncertain analytic conditions through practices of skepticism, assessment, and credibility. Highlighting the collaborative and heterogeneous nature of real-world data science, we show how the management of trust in applied corporate data science settings depends not only on pre-processing and quantification, but also on negotiation and translation. We conclude by discussing the implications of our findings for data science research and practice, both within and beyond CSCW.",Jan-28,10.1145/3274405,http://kim-gutierrez.com/explore/posts/terms/
6e8d94181832771bc5dca8d288c52b6ad5914029,Data Science as Machinic Neoplatonism,,253-272,10.1007/S13347-017-0273-3,http://www.spence.com/tags/explore/index/
6bf9d589f80823735084956f056728ae1a7bcfa8,"Situating Ecology as a Big-Data Science: Current Advances, Challenges, and Solutions","Ecology has joined a world of big data. Two complementary frameworks define big data: data that exceed the analytical capacities of individuals or disciplines or the “Four Vs” axes of volume, variety, veracity, and velocity. Variety predominates in ecoinformatics and limits the scalability of ecological science. Volume varies widely. Ecological velocity is low but growing as data throughput and societal needs increase. Ecological big-data systems include in situ and remote sensors, community data resources, biodiversity databases, citizen science, and permanent stations. Technological solutions include the development of open code- and data-sharing platforms, flexible statistical models that can handle heterogeneous data and sources of uncertainty, and cloud-computing delivery of high-velocity computing to large-volume analytics. Cultural solutions include training targeted to early and current scientific workforce and strengthening collaborations among ecologists and data scientists. The broader goal is to maximize the power, scalability, and timeliness of ecological insights and forecasting.",91-138,10.1093/BIOSCI/BIY068,https://www.hayes.com/categories/category/tags/index/
e4c66275e46a66586365c851f0974a3c88baf3d7,Network embedding in biomedical data science,"Owning to the rapid development of computer technologies, an increasing number of relational data have been emerging in modern biomedical research. Many network-based learning methods have been proposed to perform analysis on such data, which provide people a deep understanding of topology and knowledge behind the biomedical networks and benefit a lot of applications for human healthcare. However, most network-based methods suffer from high computational and space cost. There remain challenges on handling high dimensionality and sparsity of the biomedical networks. The latest advances in network embedding technologies provide new effective paradigms to solve the network analysis problem. It converts network into a low-dimensional space while maximally preserves structural properties. In this way, downstream tasks such as link prediction and node classification can be done by traditional machine learning methods. In this survey, we conduct a comprehensive review of the literature on applying network embedding to advance the biomedical domain. We first briefly introduce the widely used network embedding models. After that, we carefully discuss how the network embedding approaches were performed on biomedical networks as well as how they accelerated the downstream tasks in biomedical science. Finally, we discuss challenges the existing network embedding applications in biomedical domains are faced with and suggest several promising future directions for a better improvement in human healthcare.",93-127,10.1093/bib/bby117,https://www.clark.net/about/
f968cdd4637e7b26ca6c057a2f7f593b8cea2d18,Fundamentals of Clinical Data Science,,85-132,10.1007/978-3-319-99713-1,http://www.obrien-sharp.com/homepage.php
bf90a04b8c9a7752bab8cb50c8796393218627d2,Data science,"While it may not be possible to build a data brain identical to a human, data science can still aspire to imaginative machine thinking.",59 - 68,10.1145/3015456,https://stewart.com/category.html
a1dbdc2ce338d694a720163f591e4eb5c4070140,Deep Learning in Biomedical Data Science,"Since the 1980s, deep learning and biomedical data have been coevolving and feeding each other. The breadth, complexity, and rapidly expanding size of biomedical data have stimulated the development of novel deep learning methods, and application of these methods to biomedical data have led to scientific discoveries and practical solutions. This overview provides technical and historical pointers to the field, and surveys current applications of deep learning to biomedical data organized around five subareas, roughly of increasing spatial scale: chemoinformatics, proteomics, genomics and transcriptomics, biomedical imaging, and health care. The black box problem of deep learning methods is also briefly discussed.",98-109,10.1146/ANNUREV-BIODATASCI-080917-013343,https://www.york.com/wp-content/explore/home.html
7e6bac4de2adda5f5e993644126d8cbdf6839f39,Process Mining: Data science in Action,"Data science is the profession of the future, because organizations that are unable to use (big) data in a smart way will not survive. It is not sufficient to focus on data storage and data analysis. The data scientist also needs to relate data to process analysis. Process mining bridges the gap between traditional model-based process analysis (e.g., simulation and other business process management techniques) and datacentric analysis techniques such as machine learning and data mining. Process mining seeks the confrontation between event data (i.e., observed behavior) and process models (hand-made or discovered automatically). This technology has become available only recently, but it can be applied to any type of operational processes (organizations and systems). Example applications include: analyzing treatment processes in hospitals, improving customer service processes in a multinational, understanding the browsing behavior of customers using a booking site, analyzing failures of a baggage handling system, and improving the user interface of an X-ray machine. All of these applications have in common that dynamic behavior needs to be related to process models. Hence, we refer to this as ""data science in action"".",35-104,462-42794-9,http://www.brooks.com/login.html
3335c340c20609b4e6de481c9eaf67ecd6c960dc,Evaluation of a Tree-based Pipeline Optimization Tool for Automating Data Science,"As the field of data science continues to grow, there will be an ever-increasing demand for tools that make machine learning accessible to non-experts. In this paper, we introduce the concept of tree-based pipeline optimization for automating one of the most tedious parts of machine learning--pipeline design. We implement an open source Tree-based Pipeline Optimization Tool (TPOT) in Python and demonstrate its effectiveness on a series of simulated and real-world benchmark data sets. In particular, we show that TPOT can design machine learning pipelines that provide a significant improvement over a basic machine learning analysis while requiring little to no input nor prior knowledge from the user. We also address the tendency for TPOT to design overly complex pipelines by integrating Pareto optimization, which produces compact pipelines without sacrificing classification accuracy. As such, this work represents an important step toward fully automating machine learning pipeline design.",49-126,10.1145/2908812.2908918,https://watson.com/main/categories/posts/terms.html
9b54c9a7d2060f800961c2f9195fcf5408288f17,Data Science for Undergraduates,,88-109,10.17226/25104,http://lopez.info/register.asp
b154d9ce0a551be90557d7a24a49b1988add2a81,"Three principles of data science: predictability, computability, and stability (PCS)","In this talk, I'd like to discuss the intertwining importance and connections of three principles of data science in the title and the PCS workflow that is built on the three principles. The principles will be demonstrated in the context of two collaborative projects in neuroscience and genomics for interpretable data results and testable hypothesis generation.",04-Apr,10.1109/BigData.2018.8622080,https://www.cole.com/tags/app/index/
5a44f70130875b212452ad777ab02a4eb5cd35d9,A Position Statement on Population Data Science: The Science of Data about People,"Information is increasingly digital, creating opportunities to respond to pressing issues about human populations using linked datasets that are large, complex, and diverse. The potential social and individual benefits that can come from data-intensive science are large, but raise challenges of balancing individual privacy and the public good, building appropriate socio-technical systems to support data-intensive science, and determining whether defining a new field of inquiry might help move those collective interests and activities forward. A combination of expert engagement, literature review, and iterative conversations led to our conclusion that defining the field of Population Data Science (challenge 3) will help address the other two challenges as well. We define Population Data Science succinctly as the science of data about people and note that it is related to but distinct from the fields of data science and informatics. A broader definition names four characteristics of: data use for positive impact on citizens and society; bringing together and analyzing data from multiple sources; finding population-level insights; and developing safe, privacy-sensitive and ethical infrastructure to support research. One implication of these characteristics is that few people possess all of the requisite knowledge and skills of Population Data Science, so this is by nature a multi-disciplinary field. Other implications include the need to advance various aspects of science, such as data linkage technology, various forms of analytics, and methods of public engagement. These implications are the beginnings of a research agenda for Population Data Science, which if approached as a collective field, can catalyze significant advances in our understanding of trends in society, health, and human behavior.",52-148,10.23889/ijpds.v3i1.415,https://nelson.com/homepage/
55bdaa9d27ed595e2ccf34b3a7847020cc9c946c,Performing systematic literature reviews in software engineering,"Context: Making best use of the growing number of empirical studies in Software Engineering, for making decisions and formulating research questions, requires the ability to construct an objective summary of available research evidence. Adopting a systematic approach to assessing and aggregating the outcomes from a set of empirical studies is also particularly important in Software Engineering, given that such studies may employ very different experimental forms and be undertaken in very different experimental contexts.Objectives: To provide an introduction to the role, form and processes involved in performing Systematic Literature Reviews. After the tutorial, participants should be able to read and use such reviews, and have gained the knowledge needed to conduct systematic reviews of their own.Method: We will use a blend of information presentation (including some experiences of the problems that can arise in the Software Engineering domain), and also of interactive working, using review material prepared in advance.",97-123,10.1145/1134285.1134500,http://www.turner-cross.net/
27e57cc2f22c1921d2a1c3954d5062e3fe391553,Guidelines for conducting and reporting case study research in software engineering,,131-164,10.1007/s10664-008-9102-8,https://www.rodriguez-williams.com/categories/posts/blog/homepage/
72910077a29caf411dbb03148997c72b47e65ab0,Software Engineering Economics,"This paper summarizes the current state of the art and recent trends in software engineering economics. It provides an overview of economic analysis techniques and their applicability to software engineering and management. It surveys the field of software cost estimation, including the major estimation techniques available, the state of the art in algorithmic cost models, and the outstanding research issues in software cost estimation.",Apr-21,10.1109/TSE.1984.5010193,http://cole-sanders.info/main/privacy.html
d0bc1501ae6f54dd16534e651d90d2aeeeb1cfc1,Software engineering: What is it?,"In spite of many years of work by a multitude of organizations, a clear and simple standard for software engineering and management requirements and a method to assess their applicability to projects of various types and sizes remains elusive. From IEEE to CMMI to NASA's NPR 7150.2, there is no shortage of sources of information providing various types of requirements and standards for software engineering. Even a book on software project management for “dummies” approaches 400 pages. Wading through this information can dizzy the mind of even the most experienced software engineer; the newbie just trying to “do the right thing” will probably give up, open a text editor and start coding. This lack of clarity and simplicity perhaps goes a long way towards explaining why, in spite of this large body of work, there remains such an incredible variability in the knowledge and application of software engineering discipline not only from one organization to the next, but between groups within the same organization, or even between individual developers in the same group! Surely at least the basics of what should be done and why those things should be done can be conveyed in less than a novel-sized volume. There must be some timeless principles that cut across structured and object-oriented techniques, waterfall and agile methods, and CMMI and NASA standards. To properly interpret software engineering requirements and approaches and successfully (and selectively) apply them, one must first understand them at a fundamental level and how they can benefit the project. This paper will make an admittedly bold and brash attempt to boil it all down into something anyone can understand, hopefully resulting in a brief reference — a type of lens through which existing standards can be more practically viewed.",01-Nov,10.1109/MC.2018.1451647,https://www.edwards.com/app/home/
81dbfc1bc890368979399874e47e0529ddceaece,Software Engineering: A Practitioner's Approach,,54-148,10.1016/0141-1195(83)90118-3,http://www.ellis.org/category/
f70b2f20be241f445a61f33c4b8e76e554760340,Software Engineering for Machine Learning: A Case Study,"Recent advances in machine learning have stimulated widespread interest within the Information Technology sector on integrating AI capabilities into software and services. This goal has forced organizations to evolve their development processes. We report on a study that we conducted on observing software teams at Microsoft as they develop AI-based applications. We consider a nine-stage workflow process informed by prior experiences developing AI applications (e.g., search and NLP) and data science tools (e.g. application diagnostics and bug reporting). We found that various Microsoft teams have united this workflow into preexisting, well-evolved, Agile-like software engineering processes, providing insights about several essential engineering challenges that organizations may face in creating large-scale AI solutions for the marketplace. We collected some best practices from Microsoft teams to address these challenges. In addition, we have identified three aspects of the AI domain that make it fundamentally different from prior software application domains: 1) discovering, managing, and versioning the data needed for machine learning applications is much more complex and difficult than other types of software engineering, 2) model customization and model reuse require very different skills than are typically found in software teams, and 3) AI components are more difficult to handle as distinct modules than traditional software components - models may be ""entangled"" in complex ways and experience non-monotonic error behavior. We believe that the lessons learned by Microsoft teams will be valuable to other organizations.",291-300,10.1109/ICSE-SEIP.2019.00042,https://www.freeman-richard.com/
0961e2650b3a62a1d198a046bef5f0700ab8c08f,Guidelines for snowballing in systematic literature studies and a replication in software engineering,"Background: Systematic literature studies have become common in software engineering, and hence it is important to understand how to conduct them efficiently and reliably.
 Objective: This paper presents guidelines for conducting literature reviews using a snowballing approach, and they are illustrated and evaluated by replicating a published systematic literature review.
 Method: The guidelines are based on the experience from conducting several systematic literature reviews and experimenting with different approaches.
 Results: The guidelines for using snowballing as a way to search for relevant literature was successfully applied to a systematic literature review.
 Conclusions: It is concluded that using snowballing, as a first search strategy, may very well be a good alternative to the use of database searches.",38:1-38:10,10.1145/2601248.2601268,https://www.harvey-craig.com/
76e4f5abeb4ad28d1f16c15771aa87241a3ee96d,Software Engineering,,51-141,10.1007/978-1-4842-0847-2,https://benjamin.com/main/list/post.html
849a6be5adf1b9a2b6e59ba0290bca06692c0efd,Sampling in software engineering research: a critical review and guidelines,,90-102,10.1007/s10664-021-10072-8,https://gonzalez.com/wp-content/categories/app/terms/
75f3726a39b563c9890fb8ed7dd393da15ad6594,Object-oriented software engineering - a use case driven approach,Part 1. Introduction 1. System development as an industrial process Introduction A useful analogy System development characteristics Summary 2. The system life cycle Introduction System development as a process of change System development and reuse System development and methodology Objectory Summary 3. What is object-orientation? Introduction Object Class andinstance Polymorphism Inheritance Summary 4. Object-oriented system development Introduction Function/data methods Object-oriented analysis Object-oriented construction Object-oriented testing Summary 5. Object-oriented programming Introduction Objects Classes and instances Inheritance Polymorphism An example Summary Part II. Concepts 6. Architecture Introduction System development is model building Model architecture Requirements model Analysis model The design model The implementation model Test model Summary 7. Analysis Introduction The requirements model The analysis model Summary 8. Construction Introduction The design model Block design Working with construction Summary 9. Real-time specialization Introduction Classification of real-time systems Fundamental issues Analysis Construction Testing and verification Summary 10. Database Specialization Introduction Relational DBMS Object DBMS Discussion Summary 11. Components Introduction What is a component? Use of components Component management Summary 12. Testing Introduction On testing Unit testing Integration testing System testing The testing process Summary Part III. Applications 13. Case study: warehouse management system Introduction to the examples ACME Warehouse Management Inc. The requirements model The analysis model Construction 14. Case study: telecom Introduction Telecommunication switching systems The requirements model The analysis model The design model The implementation model 15. Managing object-oriented software engineering Introduction Project selection and preparation Project development organization Project organization and management Project staffing Software quality assurance Software metrics Summary 16. Other object-oriented methods Introduction A summary of object-oriented methods Object-Oriented Analysis (OOAD/Coad-Yourdon) Object-Oriented Design (OOD/Booch) Hierarchical Object-Oriented Design (HOOD) Object Modeling Technique (OMT) Responsibility-Driven Design Summary Appendix A On the development of Objectory Introduction Objectory as an activity From idea to reality References Index,333,352-14498-7,https://www.brown-walker.info/wp-content/category/search.html
130862d54894966552cb85d3ee6f739f885d4989,Model-Driven Software Engineering in Practice,"This book discusses how model-based approaches can improve the daily practice of software professionals. This is known as Model-Driven Software Engineering (MDSE) or, simply, Model-Driven Engineering (MDE). MDSE practices have proved to increase efficiency and effectiveness in software development, as demonstrated by various quantitative and qualitative studies. MDSE adoption in the software industry is foreseen to grow exponentially in the near future, e.g., due to the convergence of software development and business analysis. The aim of this book is to provide you with an agile and flexible tool to introduce you to the MDSE world, thus allowing you to quickly understand its basic principles and techniques and to choose the right set of MDSE instruments for your needs so that you can start to benefit from MDSE right away. The book is organized into two main parts. The first part discusses the foundations of MDSE in terms of basic concepts (i.e., models and transformations), driving principles, application scenarios and current standards, like the well-known MDA initiative proposed by OMG (Object Management Group) as well as the practices on how to integrate MDSE in existing development processes. The second part deals with the technical aspects of MDSE, spanning from the basics on when and how to build a domain-specific modeling language, to the description of Model-to-Text and Model-to-Model transformations, and the tools that support the management of MDSE projects. The book is targeted to a diverse set of readers, spanning: professionals, CTOs, CIOs, and team managers that need to have a bird's eye vision on the matter, so as to take the appropriate decisions when it comes to choosing the best development techniques for their company or team; software analysts, developers, or designers that expect to use MDSE for improving everyday work productivity, either by applying the basic modeling techniques and notations or by defining new domain-specific modeling languages and applying end-to-end MDSE practices in the software factory; and academic teachers and students to address undergrad and postgrad courses on MDSE. In addition to the contents of the book, more resources are provided on the book's website http://www.mdse-book.com/, including the examples presented in the book. Table of Contents: Introduction / MDSE Principles / MDSE Use Cases / Model-Driven Architecture (MDA) / Integration of MDSE in your Development Process / Modeling Languages at a Glance / Developing your Own Modeling Language / Model-to-Model Transformations / Model-to-Text Transformations / Managing Models / Summary",96-145,10.2200/s00441ed1v01y201208swe001,http://lambert.com/homepage/
2bd576ce574df33c834b6032962cd5ae0be5299f,Guidelines for conducting systematic mapping studies in software engineering: An update,,Jan-18,10.1016/j.infsof.2015.03.007,https://www.williams-ritter.org/
947b29eb3cde8ccb8df9342bb2384ec480ea3964,Experimentation in software engineering: an introduction,,76-143,10.1016/s0898-1221(00)90203-7,http://gamble-thomas.net/login.jsp
21c6beb2a6df81f424e3d1283fbb9cc3157a3115,A Taxonomy of Software Engineering Challenges for Machine Learning Systems: An Empirical Investigation,,227-243,10.1007/978-3-030-19034-7_14,https://black.com/
a963d05b9d4acd347ad528e7d098eb53d8f555a2,Systematic literature reviews in software engineering - A systematic literature review,,Jul-15,10.1016/J.INFSOF.2008.09.009,https://www.baker.com/
e28bdc373de80d7ec0e64631a89e64fbdcdae230,Systematic Mapping Studies in Software Engineering,"BACKGROUND: A software engineering systematic map is a defined method to build a classification scheme and structure a software engineering field of interest. The analysis of results focuses on frequencies of publications for categories within the scheme. Thereby, the coverage of the research field can be determined. Different facets of the scheme can also be combined to answer more specific research questions. 
 
OBJECTIVE: We describe how to conduct a systematic mapping study in software engineering and provide guidelines. We also compare systematic maps and systematic reviews to clarify how to chose between them. This comparison leads to a set of guidelines for systematic maps. 
 
METHOD: We have defined a systematic mapping process and applied it to complete a systematic mapping study. Furthermore, we compare systematic maps with systematic reviews by systematically analyzing existing systematic reviews. 
 
RESULTS: We describe a process for software engineering systematic mapping studies and compare it to systematic reviews. Based on this, guidelines for conducting systematic maps are defined. 
 
CONCLUSIONS: Systematic maps and reviews are different in terms of goals, breadth, validity issues and implications. Thus, they should be used complementarily and require different methods (e.g., for analysis).",68-77,10.14236/EWIC/EASE2008.8,https://www.shea-collins.com/faq.jsp
247e2ee1a84e25cdc2d0d68811e2ee05ca0bc6a9,Software engineering in start-up companies: An analysis of 88 experience reports,,68-102,10.1007/s10664-018-9620-y,http://www.williams.com/faq.htm
f463018624b6f4b8dd576732b6cce36e31bac978,Software Engineering of Self-adaptive Systems,,399-443,10.1007/978-3-030-00262-6_11,https://jackson.com/category/
967f4eb786aa143b7eb09f00d9ba8ddfe44e039f,Sentiment Analysis for Software Engineering: How Far Can We Go?,"Sentiment analysis has been applied to various software engineering (SE) tasks, such as evaluating app reviews or analyzing developers' emotions in commit messages. Studies indicate that sentiment analysis tools provide unreliable results when used out-of-the-box, since they are not designed to process SE datasets. The silver bullet for a successful application of sentiment analysis tools to SE datasets might be their customization to the specific usage context. We describe our experience in building a software library recommender exploiting crowdsourced opinions mined from Stack Overflow (e.g., what is the sentiment of developers about the usability of a library). To reach our goal, we retrained—on a set of 40k manually labeled sentences/words extracted from Stack Overflow—a state-of-the-art sentiment analysis tool exploiting deep learning. Despite such an effort- and time-consuming training process, the results were negative. We changed our focus and performed a thorough investigation of the accuracy of these tools on a variety of SE datasets. Our results should warn the research community about the strong limitations of current sentiment analysis tools.",94-104,10.1145/3180155.3180195,http://houston.com/
bca7c0902f600fa77b1e16d0e093e23f7d75f649,Empirical software engineering experts on the use of students and professionals in experiments,,452-489,10.1007/s10664-017-9523-3,http://www.gonzalez.com/
ed0759b7001f8be53bb4282750e98198b359307d,No Silver Bullet Essence and Accidents of Software Engineering,"But, as we look to the horizon of a decade hence, we see no silver bullet. There is no single development, in either technology or in management technique, that by itself promises even one order-of-magnitude improvement in productivity, in reliability, in simplicity. In this article, I shall try to show why, by examining both the nature of the software problem and the properties of the bullets proposed.",Oct-19,10.1109/MC.1987.1663532,http://branch.com/about.asp
fb2bb5777f1b1bd745070c006265edf8feb5f29f,Smart contracts vulnerabilities: a call for blockchain software engineering?,"Smart Contracts have gained tremendous popularity in the past few years, to the point that billions of US Dollars are currently exchanged every day through such technology. However, since the release of the Frontier network of Ethereum in 2015, there have been many cases in which the execution of Smart Contracts managing Ether coins has led to problems or conflicts. Compared to traditional Software Engineering, a discipline of Smart Contract and Blockchain programming, with standardized best practices that can help solve the mentioned problems and conflicts, is not yet sufficiently developed. Furthermore, Smart Contracts rely on a non-standard software life-cycle, according to which, for instance, delivered applications can hardly be updated or bugs resolved by releasing a new version of the software. In this paper we advocate the need for a discipline of Blockchain Software Engineering, addressing the issues posed by smart contract programming and other applications running on blockchains.We analyse a case of study where a bug discovered in a Smart Contract library, and perhaps ""unsafe"" programming, allowed an attack on Parity, a wallet application, causing the freezing of about 500K Ethers (about 150M USD, in November 2017). In this study we analyze the source code of Parity and the library, and discuss how recognised best practices could mitigate, if adopted and adapted, such detrimental software misbehavior. We also reflect on the specificity of Smart Contract software development, which makes some of the existing approaches insufficient, and call for the definition of a specific Blockchain Software Engineering.",19-25,10.1109/IWBOSE.2018.8327567,http://www.lopez.com/category/list/wp-content/terms/
30b71975e26dd2709f58372419b712d97536402f,Continuous software engineering: A roadmap and agenda,,176-189,10.1016/j.jss.2015.06.063,http://kemp.info/tag/blog/app/category/
51b502b9ce774a615474ed8629e74d0dfaa33ee3,The ABC of Software Engineering Research,"A variety of research methods and techniques are available to SE researchers, and while several overviews exist, there is consistency neither in the research methods covered nor in the terminology used. Furthermore, research is sometimes critically reviewed for characteristics inherent to the methods. We adopt a taxonomy from the social sciences, termed here the ABC framework for SE research, which offers a holistic view of eight archetypal research strategies. ABC refers to the research goal that strives for generalizability over Actors (A) and precise measurement of their Behavior (B), in a realistic Context (C). The ABC framework uses two dimensions widely considered to be key in research design: the level of obtrusiveness of the research and the generalizability of research findings. We discuss metaphors for each strategy and their inherent limitations and potential strengths. We illustrate these research strategies in two key SE domains, global software engineering and requirements engineering, and apply the framework on a sample of 75 articles. Finally, we discuss six ways in which the framework can advance SE research.",Jan-51,10.1145/3241743,http://flores.com/tag/tags/privacy.php
6f58d8b98c652897842afdd023c535f9724b4eb2,A Benchmark Study on Sentiment Analysis for Software Engineering Research,"A recent research trend has emerged to identify developers' emotions, by applying sentiment analysis to the content of communication traces left in collaborative development environments. Trying to overcome the limitations posed by using off-the-shelf sentiment analysis tools, researchers recently started to develop their own tools for the software engineering domain. In this paper, we report a benchmark study to assess the performance and reliability of three sentiment analysis tools specifically customized for software engineering. Furthermore, we offer a reflection on the open challenges, as they emerge from a qualitative analysis of misclassified texts.",364-375,10.1145/3196398.3196403,http://kelly.com/posts/categories/main.asp
6e1cafd50333b3812bf002a51bcb1f720e35b7ed,Word Embeddings for the Software Engineering Domain,"The software development process produces vast amounts of textual data expressed in natural language. Outcomes from the natural language processing community have been adapted in software engineering research for leveraging this rich textual information; these include methods and readily available tools, often furnished with pretrained models. State of the art pretrained models however, capture general, common sense knowledge, with limited value when it comes to handling data specific to a specialized domain. There is currently a lack of domain-specific pretrained models that would further enhance the processing of natural language artefacts related to software engineering. To this end, we release a word2vec model trained over 15GB of textual data from Stack Overflow posts. We illustrate how the model disambiguates polysemous words by interpreting them within their software engineering context. In addition, we present examples of fine-grained semantics captured by the model, that imply transferability of these results to diverse, targeted information retrieval tasks in software engineering and motivate for further reuse of the model.",38-41,10.1145/3196398.3196448,https://gonzalez.com/post.htm
843964eecc10b28eff53f2d4a9db4e112a38e94e,Introduction to Software Engineering,"INTRODUCTION The Need for Software Engineering Are Software Teams Really Necessary? Software Engineering Software Lifecycles Different Views of Software Engineering Activities Software Engineering as an Emerging Discipline Some Techniques of Software Engineering Standards Commonly Used for Software Development Processes The Year 2000 Problem and Similar Problems Organization of the Book PROJECT MANAGEMENT Sub-Teams Needed in Software Engineering Projects The Nature of Project Teams Project Management Software Project Estimation Project Scheduling Project Measurement Project Management Tools The Role of Networks in Project Management Groupware An Example: Project Management for a Year 2000 Conversion Project REQUIREMENTS Some Problems with Requirements Determination Requirements Elicitation Requirements Traceability Software Architectures and Requirements Reengineering System Requirements Assessment of Feasibility of System Requirements Usability Requirements Specifying Requirements Using State Diagrams and Decision Tables Specifying Requirements Using Petri Nets Ethical Issues Some Metrics for Requirements The Requirements Review The Major Project - Problem Statement The Major Project - Requirements Elicitation The Major Software Project - Requirements Analysis SOFTWARE DESIGN Introduction Software Design Patterns Introduction to Software Design Representations Procedurally-Oriented Design Representations Software Architectures Software Design Principles for Procedurally-Oriented Programs What is an Object? Object-Oriented Design Representations Software Design Principles for Object-Oriented Programs Class Design Issues An Example of Class Development - The String Class User Interfaces Software Interfaces Some Metrics for Design Design Reviews A Manager's Viewpoint of Design Architecture of the Major Software Engineering Project Preliminary Design of the Major Software Project Subsystem Design of the Major Software Project Detailed Design for the Major Software Project CODING The Choice of Programming Language Coding Styles Coding Standards Coding, Design, Requirements, and Change Some Coding Metrics Coding Reviews and Inspections Configuration Management A Management Perspective on Coding Coding of the Major Software Project TESTING AND INTEGRATION Types of Software Testing Black-Box Module Testing White-Box Module Testing Reducing the Number of Test Cases by Effective Test Strategies Testing Objects for Encapsulation and Completeness Testing Objects with Inheritance General Testing Issues for Object-Oriented Software Test Plans Software Integration Managing Change in the Integration Process Performance and Stress Testing Quality Assurance Software Reliability A Manager's Viewpoint on Testing and Integration Testing the Major Software Project Integrating the Major Software Project DELIVERY, INSTALLATION, AND DOCUMENTATION Delivery Installation Internal Documentation External Documentation Design Rationales Installation, User, Training, and Operations Manuals On-Line Documentation Reading Levels A Manager's View of Delivery, Installation, and Documentation Delivery, Installation, and Documentation of the Major Software Project MAINTENANCE Introduction Corrective Software Maintenance Adaptive Software Maintenance Preventative Software Maintenance and the Year 2000 Problem How to Read Requirements, Designs, and Source Code A Manager's Perspective on Software Maintenance Maintenance of the Major Software Project RESEARCH ISSUES IN SOFTWARE ENGINEERING Some Important Research Problems in Software Engineering How to Read the Software Engineering Research Literature APPENDIX: COMMAND-LINE ARGUMENTS REFERENCES",62-108,10.1201/9781315371665,http://navarro.com/
cc3d1830b5220b68d8e785130e4b0d5ab7563d67,Software Engineering for Machine-Learning Applications: The Road Ahead,"The First Symposium on Software Engineering for Machine Learning Applications (SEMLA) aimed to create a space in which machine learning (ML) and software engineering (SE) experts could come together to discuss challenges, new insights, and practical ideas regarding the engineering of ML and AI-based systems. Key challenges discussed included the accuracy of systems built using ML and AI models, the testing of those systems, industrial applications of AI, and the rift between the ML and SE communities. This article is part of a theme issue on software engineering’s 50th anniversary.",81-84,10.1109/MS.2018.3571224,https://www.gonzales-maxwell.com/category/explore/app/terms.jsp
a75965753906c24a6a5715a695757a0de8447f26,A Survey of App Store Analysis for Software Engineering,"App Store Analysis studies information about applications obtained from app stores. App stores provide a wealth of information derived from users that would not exist had the applications been distributed via previous software deployment methods. App Store Analysis combines this non-technical information with technical information to learn trends and behaviours within these forms of software repositories. Findings from App Store Analysis have a direct and actionable impact on the software teams that develop software for app stores, and have led to techniques for requirements engineering, release planning, software design, security and testing. This survey describes and compares the areas of research that have been explored thus far, drawing out common aspects, trends and directions future research should take to address open problems and challenges.",817-847,10.1109/TSE.2016.2630689,https://simmons-reyes.com/about/
11de94b684fa2eb4dffe4b99d959888382c6abfb,Software Engineering Challenges of Deep Learning,"Surprisingly promising results have been achieved by deep learning (DL) systems in recent years. Many of these achievements have been reached in academic settings, or by large technology companies with highly skilled research groups and advanced supporting infrastructure. For companies without large research groups or advanced infrastructure, building high-quality production-ready systems with DL components has proven challenging. There is a clear lack of well-functioning tools and best practices for building DL systems. It is the goal of this research to identify what the main challenges are, by applying an interpretive research approach in close collaboration with companies of varying size and type. A set of seven projects have been selected to describe the potential with this new technology and to identify associated main challenges. A set of 12 main challenges has been identified and categorized into the three areas of development, production, and organizational challenges. Furthermore, a mapping between the challenges and the projects is defined, together with selected motivating descriptions of how and why the challenges apply to specific projects. Compared to other areas such as software engineering or database technologies, it is clear that DL is still rather immature and in need of further work to facilitate development of high-quality systems. The challenges identified in this paper can be used to guide future research by the software engineering and DL communities. Together, we could enable a large number of companies to start taking advantage of the high potential of the DL technology.",50-59,10.1109/SEAA.2018.00018,https://krueger.org/
11ebce51f6b0a901948953a024abf1cdb9111f3f,Design Science Methodology for Information Systems and Software Engineering,,3-317,10.1007/978-3-662-43839-8,http://poole.com/
bccd2ffa530097e7617c35178de641f1295d2748,The History of Software Engineering,"Grady Booch, one of UML’s original authors, offers his perspective on the history of software engineering. This article is part of a theme issue on software engineering’s 50th anniversary. The Web Extra, a version of the article with an expanded bibliography, is at https://extras.computer.org/extra/mso2018050108s1.pdf.",108-114,10.1109/MS.2018.3571234,https://www.riley.com/app/app/list/home/
2b19768afa6fbf5abb19790cd1ee991574129933,A survey of the use of crowdsourcing in software engineering,,57-84,10.1016/J.JSS.2016.09.015,http://www.vargas.info/main.html
3f0b37f26829684b8417309577d77f3af6534707,Fundamentals of Software Engineering,,84-143,10.1007/978-3-319-24644-4,http://bennett-reyes.net/search/
bcc7041e0fb7717a7d67a9c00e08b7fb81384cbf,Robust Statistical Methods for Empirical Software Engineering,,579-630,10.1007/s10664-016-9437-5,http://barrett-hawkins.com/
8fe2f59ff3733f9ee50ffa295beda502f4e268e2,Grounded Theory in Software Engineering Research: A Critical Review and Guidelines,"Grounded Theory (GT) has proved an extremely useful research approach in several fields including medical sociology, nursing, education and management theory. However, GT is a complex method based on an inductive paradigm that is fundamentally different from the traditional hypothetico-deductive research model. As there are at least three variants of GT, some ostensibly GT research suffers from method slurring, where researchers adopt an arbitrary subset of GT practices that are not recognizable as GT. In this paper, we describe the variants of GT and identify the core set of GT practices. We then analyze the use of grounded theory in software engineering. We carefully and systematically selected 98 articles that mention GT, of which 52 explicitly claim to use GT, with the other 46 using GT techniques only. Only 16 articles provide detailed accounts of their research procedures. We offer guidelines to improve the quality of both conducting and reporting GT studies. The latter is an important extension since current GT guidelines in software engineering do not cover the reporting process, despite good reporting being necessary for evaluating a study and informing subsequent research.",120-131,10.1145/2884781.2884833,http://www.frye.biz/wp-content/category/tag/privacy/
63b8ebfe57af400c8bbadc5c111cb5fe71f331bd,On negative results when using sentiment analysis tools for software engineering research,,2543-2584,10.1007/s10664-016-9493-x,http://www.baxter.biz/main/
ae8afd0a805b8e117b5cf933e6d0cccfc178ad63,Software Engineering,,68-144,10.1007/978-3-030-17427-9_3,http://www.obrien.com/posts/category.jsp
24359b0a34715df2179e084285c16a10f997a145,Software Engineering Education: Converging with the Startup Industry,"Startups are agents of change that bring in innovations and find solutions to problems at various scales. An all-rounded engineering team is a key driver for the ability to execute the entrepreneurial ambition, from building a minimum viable product to later stages of product vision. Software engineering education provides students with the knowledge to transition to mature companies with defined structure in place successfully. However, the fluidity, risk, time-sensitivity, and uncertainty of startups demand a dynamic and agile set of skills to rapidly identify, conceptualize and deliver features as per market needs. This requires the adoption of latest development trends in software processes, engineering and DevOps practices with automation to iterate fast with low governance and the ability to take on multiple roles. This paper presents a study of the dynamics and engineering at startups and compares it with the current curriculum of software engineering.",192-196,10.1109/CSEET.2017.38,http://www.sutton.com/categories/wp-content/category/privacy/
261043d80a2a66044e1ee0eaee46c7331687afa4,Leveraging Automated Sentiment Analysis in Software Engineering,"Automated sentiment analysis in software engineering textual artifacts has long been suffering from inaccuracies in those few tools available for the purpose. We conduct an in-depth qualitative study to identify the difficulties responsible for such low accuracy. Majority of the exposed difficulties are then carefully addressed in developing SentiStrength-SE, a tool for improved sentiment analysis especially designed for application in the software engineering domain. Using a benchmark dataset consisting of 5,600 manually annotated JIRA issue comments, we carry out both quantitative and qualitative evaluations of our tool. SentiStrength-SE achieves 73.85% precision and 85% recall, which are significantly higher than a state-of-the-art sentiment analysis tool we compare with.",203-214,10.1109/MSR.2017.9,https://www.lewis.net/post.html
f9a9f3f016fa3123c3059cca66314d26f2357155,"Model-Driven Software Engineering in Practice, Second Edition","This book discusses how model-based approaches can improve the daily practice of software professionals. This is known as Model-Driven Software Engineering (MDSE) or, simply, Model-Driven Engineering (MDE). MDSE practices have proved to increase efficiency and effectiveness in software development, as demonstrated by various quantitative and qualitative studies. MDSE adoption in the software industry is foreseen to grow exponentially in the near future, e.g., due to the convergence of software development and business analysis. The aim of this book is to provide you with an agile and flexible tool to introduce you to the MDSE world, thus allowing you to quickly understand its basic principles and techniques and to choose the right set of MDSE instruments for your needs so that you can start to benefit from MDSE right away. The book is organized into two main parts. The first part discusses the foundations of MDSE in terms of basic concepts (i.e., models and transformations), driving principles, application scenarios, and current standards, like the well-known MDA initiative proposed by OMG (Object Management Group) as well as the practices on how to integrate MDSE in existing development processes. The second part deals with the technical aspects of MDSE, spanning from the basics on when and how to build a domain-specific modeling language, to the description of Model-to-Text and Model-to-Model transformations, and the tools that support the management of MDSE projects. The second edition of the book features: a set of completely new topics, including: full example of the creation of a new modeling language (IFML), discussion of modeling issues and approaches in specific domains, like business process modeling, user interaction modeling, and enterprise architecture complete revision of examples, figures, and text, for improving readability, understandability, and coherence better formulation of definitions, dependencies between concepts and ideas addition of a complete index of book content In addition to the contents of the book, more resources are provided on the book's website http://www.mdse-book.com, including the examples presented in the book.",19-127,10.2200/S00751ED2V01Y201701SWE004,https://www.roberts-mays.biz/
0a5d358e643f46f5a5d20892417260800cccc345,Experimentation in Software Engineering,,1-204,10.1007/978-1-4615-4625-2,http://www.hall.com/blog/blog/index.html
3041a9265afb2ebdb4915aa9572668bb7f32b0ef,From Word Embeddings to Document Similarities for Improved Information Retrieval in Software Engineering,"The application of information retrieval techniques to search tasks in software engineering is made difficult by the lexical gap between search queries, usually expressed in natural language (e.g. English), and retrieved documents, usually expressed in code (e.g. programming languages). This is often the case in bug and feature location, community question answering, or more generally the communication between technical personnel and non-technical stake holders in a software project. In this paper, we propose bridging the lexical gap by projecting natural language statements and code snippets as meaning vectors in a shared representation space. In the proposed architecture, word embeddings are rst trained on API documents, tutorials, and reference documents, and then aggregated in order to estimate semantic similarities between documents. Empirical evaluations show that the learned vector space embeddings lead to improvements in a previously explored bug localization task and a newly de ned task of linking API documents to computer programming questions.",404-415,10.1145/2884781.2884862,https://www.brown.com/
9a9643601989088ace41382b3c1cc61e1b4d5633,Blockchain-Oriented Software Engineering: Challenges and New Directions,"In this work, we acknowledge the need for software engineers to devise specialized tools and techniques for blockchain-oriented software development. Ensuring effective testing activities, enhancing collaboration in large teams, and facilitating the development of smart contracts all appear as key factors in the future of blockchain-oriented software development.",169-171,10.1109/ICSE-C.2017.142,https://www.harvey.com/post.jsp
c176cf31862a7c5b324556e8dc3fdbef2a108391,Model-Based Software Engineering to Tame the IoT Jungle,"The Internet of Things (IoT) is a challenging combination of distribution and heterogeneity. A number of software engineering solutions address those challenges in isolation, but few solutions tackle them in combination, which poses a set of concrete challenges. The ThingML (Internet of Things Modeling Language) approach attempts to address those challenges. This model-driven, generative approach, which was inspired by UML, integrates concepts targeted at the IoT. Over the past six years, it has been continuously evolved and applied to cases in different domains, including a commercial e-health solution.",30-36,10.1109/MS.2017.11,https://moore-bradley.com/app/about/
9579ed0d182ba134ab3ed14ba0defbb324147399,Key Abstractions for IoT-Oriented Software Engineering,"Despite the progress in Internet of Things (IoT) research, a general software engineering approach for systematic development of IoT systems and applications is still missing. A synthesis of the state of the art in the area can help frame the key abstractions related to such development. Such a framework could be the basis for guidelines for IoT-oriented software engineering.",38-45,10.1109/MS.2017.3,https://www.johnson.info/post.asp
819f36c0ddae12132d60ebc5cc0a19f7db8668b1,Cognitive Biases in Software Engineering: A Systematic Mapping Study,"One source of software project challenges and failures is the systematic errors introduced by human cognitive biases. Although extensively explored in cognitive psychology, investigations concerning cognitive biases have only recently gained popularity in software engineering research. This paper therefore systematically maps, aggregates and synthesizes the literature on cognitive biases in software engineering to generate a comprehensive body of knowledge, understand state-of-the-art research and provide guidelines for future research and practise. Focusing on bias antecedents, effects and mitigation techniques, we identified 65 articles (published between 1990 and 2016), which investigate 37 cognitive biases. Despite strong and increasing interest, the results reveal a scarcity of research on mitigation techniques and poor theoretical foundations in understanding and interpreting cognitive biases. Although bias-related research has generated many new insights in the software engineering community, specific bias mitigation techniques are still needed for software professionals to overcome the deleterious effects of cognitive biases on their work.",1318-1339,10.1109/TSE.2018.2877759,https://patrick.com/
0c2faa3d1403f94da644853b1f33f976fab1da0b,Software Engineering,"S ystem testing followed by a product release decision are the last guards in assuring software quality—insufficient testing or the wrong release decision can lead directly to the delivery of low-quality software to users. At the same time, relying too much on system testing to guarantee quality is dangerous because it occurs too late to correct poor-quality software. Moreover, previous studies have shown that bug fixing is much costlier during system testing than in earlier phases.1 Therefore, we must not only be aware of factors that increase defects but also seek possible process improvements to reduce defects before system testing. To identify and justify process improvements in individual organizations, where processes, data, and context are varied and unique, we explored using a multivariate modeling technique to analyze past development data collected in organizations. However, unlike some academic approaches, we employed a basic linear regression approach with a limited number of independent variables, each associated with what we call software engineering (SE) beliefs. These are short statements that are attention-getting, understandable, and obviously practically useful, such as “about 80 percent of the defects come from 20 percent of the modules,” or “peer reviews catch 60 percent of the defects.”2 SE beliefs are a kind of practical hypothesis that",24-129,10.7551/mitpress/11740.003.0008,https://adams-craig.com/main.htm
d1cc35e2a547ba79f1b07fdd81ee0da264c0d6b6,Belief & Evidence in Empirical Software Engineering,"Empirical software engineering has produced a steady stream of evidence-based results concerning the factors that affect important outcomes such as cost, quality, and interval. However, programmers often also have strongly-held a priori opinions about these issues. These opinions are important, since developers are highlytrained professionals whose beliefs would doubtless affect their practice. As in evidence-based medicine, disseminating empirical findings to developers is a key step in ensuring that the findings impact practice. In this paper, we describe a case study, on the prior beliefs of developers at Microsoft, and the relationship of these beliefs to actual empirical data on the projects in which these developers work. Our findings are that a) programmers do indeed have very strong beliefs on certain topics b) their beliefs are primarily formed based on personal experience, rather than on findings in empirical research and c) beliefs can vary with each project, but do not necessarily correspond with actual evidence in that project. Our findings suggest that more effort should be taken to disseminate empirical findings to developers and that more in-depth study the interplay of belief and evidence in software practice is needed.",108-119,10.1145/2884781.2884812,https://www.arnold.org/category.html
7e991438547d69c1001b7664cd60a68d4dbc4023,A Map of Threats to Validity of Systematic Literature Reviews in Software Engineering,"Context: The assessment of Threats to Validity (TTVs) is critical to secure the quality of empirical studies in Software Engineering (SE). In the recent decade, Systematic Literature Review (SLR) was becoming an increasingly important empirical research method in SE. One of the mechanisms of insuring the level of scientific value in the findings of an SLR is to rigorously assess its validity. Hence, it is necessary to realize the status quo and issues of TTVs of SLRs in SE. Objective: This study aims to investigate thestate-of-the-practice of TTVs of the SLRs published in SE, and further support SE researchers to improve the assessment and strategies against TTVs in order to increase the quality of SLRs in SE. Method: We conducted a tertiary study by reviewing the SLRs in SE that report the assessment of TTVs. Results: We identified 316 SLRs published from 2004 to the first half of 2015, in which TTVs are discussed. The issues associated to TTVs were also summarized and categorized. Conclusion: The common TTVs related to SLR research, such as internal validity and reliability, were thoroughly discussed in most SLRs. The threats to construct validity and external validity drew less attention. Moreover, there are few strategies and tactics being reported to cope with the various TTVs.",153-160,10.1109/APSEC.2016.031,https://smith-garcia.biz/category/post/
64cc4ef5def3919049bdd3a645af198922d626c2,An Empirical Study of Practitioners' Perspectives on Green Software Engineering,"The energy consumption of software is an increasing concern as the use of mobile applications, embedded systems, and data center-based services expands. While research in green software engineering is correspondingly increasing, little is known about the current practices and perspectives of software engineers in the field. This paper describes the first empirical study of how practitioners think about energy when they write requirements, design, construct, test, and maintain their software. We report findings from a quantitative,targeted survey of 464 practitioners from ABB, Google, IBM, and Microsoft, which was motivated by and supported with qualitative data from 18 in-depth interviews with Microsoft employees. The major findings and implications from the collected data contextualize existing green software engineering research and suggest directions for researchers aiming to develop strategies and tools to help practitioners improve the energy usage of their applications.",237-248,10.1145/2884781.2884810,https://greer.com/blog/app/category.html
cdc5bf80451d1f447cf82e5c37fec8089b1e6878,A framework for gamification in software engineering,,21-40,10.1016/J.JSS.2017.06.021,http://www.preston.com/wp-content/posts/category/author/
0e50bf23cb16ba66e738d88f0fffab75c338e02a,"Crowdsourcing in Software Engineering: Models, Motivations, and Challenges","Almost surreptitiously, crowdsourcing has entered software engineering practice. In-house development, contracting, and outsourcing still dominate, but many development projects use crowdsourcing-for example, to squash bugs, test software, or gather alternative UI designs. Although the overall impact has been mundane so far, crowdsourcing could lead to fundamental, disruptive changes in how software is developed. Various crowdsourcing models have been applied to software development. Such changes offer exciting opportunities, but several challenges must be met for crowdsourcing software development to reach its potential.",74-80,10.1109/MS.2016.12,http://horton-richardson.com/wp-content/login.html
4099f2b7a9282e3be30076f2532008e6caeb1c13,On the pragmatic design of literature studies in software engineering: an experience-based guideline,,2852-2891,10.1007/s10664-016-9492-y,http://www.anderson-olson.net/terms.html
266aa9741c6559af0c6dcee2e1947ced0385b4bd,Evidence-Based Software Engineering and Systematic Reviews,"In the decade since the idea of adapting the evidence-based paradigm for software engineering was first proposed, it has become a major tool of empirical software engineering. Evidence-Based Software Engineering and Systematic Reviews provides a clear introduction to the use of an evidence-based model for software engineering research and practice. The book explains the roles of primary studies (experiments, surveys, case studies) as elements of an over-arching evidence model, rather than as disjointed elements in the empirical spectrum. Supplying readers with a clear understanding of empirical software engineering best practices, it provides up-to-date guidance on how to conduct secondary studies in software engineeringreplacing the existing 2004 and 2007 technical reports. The book is divided into three parts. The first part discusses the nature of evidence and the evidence-based practices centered on a systematic review, both in general and as applying to software engineering. The second part examines the different elements that provide inputs to a systematic review (usually considered as forming a secondary study), especially the main forms of primary empirical study currently used in software engineering. The final part provides practical guidance on how to conduct systematic reviews (the guidelines), drawing together accumulated experiences to guide researchers and students in planning and conducting their own studies. The book includes an extensive glossary and an appendix that provides a catalogue of reviews that may be useful for practice and teaching.",87-102,10.1201/b19467,http://www.nelson-foster.com/post/
ec59569fdee17844ae071be1536a08f937f08c57,"Speed, Data, and Ecosystems: The Future of Software Engineering","An evaluation of recent industrial and societal trends revealed three key factors driving software engineering's future: speed, data, and ecosystems. These factors' implications have led to guidelines for companies to evolve their software engineering practices. This article is part of a special issue on the Future of Software Engineering.",82-88,10.1109/MS.2016.14,http://gomez-larsen.com/post.html
27c9101fa2f41ce15100f0f07802bb656eda50ad,On the use of many quality attributes for software refactoring: a many-objective search-based software engineering approach,,2503-2545,10.1007/s10664-015-9414-4,https://green.biz/categories/main/app/privacy.html
12248c8cd5ba1f2594e2bcf4c3b4e8ddd10d13c7,Open innovation in software engineering: a systematic mapping study,,684-723,10.1007/s10664-015-9380-x,https://www.petersen.com/home.php
564614da76b5d9020c700b78e1fe154bd590c47d,The Role of Ethnographic Studies in Empirical Software Engineering,"Ethnography is a qualitative research method used to study people and cultures. It is largely adopted in disciplines outside software engineering, including different areas of computer science. Ethnography can provide an in-depth understanding of the socio-technological realities surrounding everyday software development practice, i.e., it can help to uncover not only what practitioners do, but also why they do it. Despite its potential, ethnography has not been widely adopted by empirical software engineering researchers, and receives little attention in the related literature. The main goal of this paper is to explain how empirical software engineering researchers would benefit from adopting ethnography. This is achieved by explicating four roles that ethnography can play in furthering the goals of empirical software engineering: to strengthen investigations into the social and human aspects of software engineering; to inform the design of software engineering tools; to improve method and process development; and to inform research programmes. This article introduces ethnography, explains its origin, context, strengths and weaknesses, and presents a set of dimensions that position ethnography as a useful and usable approach to empirical software engineering research. Throughout the paper, relevant examples of ethnographic studies of software practice are used to illustrate the points being made.",786-804,10.1109/TSE.2016.2519887,https://www.bolton.org/main.jsp
b681cd520e9b2c4e2c96dda31d09b51edcf28e32,Systems and Software Engineering,,235-305,10.1007/978-3-319-25178-3_6,http://www.lopez-velez.biz/app/home/
5602cbe797d20b17edfd08b3dc94622c842fbe80,Crossover Designs in Software Engineering Experiments: Benefits and Perils,"In experiments with crossover design subjects apply more than one treatment. Crossover designs are widespread in software engineering experimentation: they require fewer subjects and control the variability among subjects. However, some researchers disapprove of crossover designs. The main criticisms are: the carryover threat and its troublesome analysis. Carryover is the persistence of the effect of one treatment when another treatment is applied later. It may invalidate the results of an experiment. Additionally, crossover designs are often not properly designed and/or analysed, limiting the validity of the results. In this paper, we aim to make SE researchers aware of the perils of crossover experiments and provide risk avoidance good practices. We study how another discipline (medicine) runs crossover experiments. We review the SE literature and discuss which good practices tend not to be adhered to, giving advice on how they should be applied in SE experiments. We illustrate the concepts discussed analysing a crossover experiment that we have run. We conclude that crossover experiments can yield valid results, provided they are properly designed and analysed, and that, if correctly addressed, carryover is no worse than other validity threats.",120-135,10.1109/TSE.2015.2467378,http://hernandez.com/blog/search/index.asp
a717f569c788ae23a3c0d2d6a18b109590a119c8,A discipline for software engineering,,"I-XXVI, 1-789",10.1016/0967-0661(95)90062-4,https://jackson.info/app/tag/posts/privacy/
b73694c24ec259da39e125c2c7d9496b2f222ba0,Future Trends in Software Engineering Research for Mobile Apps,"There has been tremendous growth in the use of mobile devices over the last few years. This growth has fueled the development of millions of software applications for these mobile devices often called as 'apps'. Current estimates indicate that there are hundreds of thousands of mobile app developers. As a result, in recent years, there has been an increasing amount of software engineering research conducted on mobile apps to help such mobile app developers. In this paper, we discuss current and future research trends within the framework of the various stages in the software development life-cycle: requirements (including non-functional), design and development, testing, and maintenance. While there are several non-functional requirements, we focus on the topics of energy and security in our paper, since mobile apps are not necessarily built by large companies that can afford to get experts for solving these two topics. For the same reason we also discuss the monetizing aspects of a mobile app at the end of the paper. For each topic of interest, we first present the recent advances done in these stages and then we present the challenges present in current work, followed by the future opportunities and the risks present in pursuing such research.",21-32,10.1109/SANER.2016.88,https://torres.com/app/categories/privacy.php
9445d843e8416a4cffb6da681250162ac38e1e2f,Student Experiences Using GitHub in Software Engineering Courses: A Case Study,"GitHub has been embraced by the software development community as an important social platform for managing software projects and to support collaborative development. More recently, educators have begun to adopt it for hosting course content and student assignments. From our previous research, we found that educators leverage GitHub’s collaboration and transparency features to create, reuse and remix course materials, and to encourage student contributions and monitor student activity on assignments and projects. However, our previous research did not consider the student perspective. In this paper, we present a case study where GitHub is used as a learning platform for two software engineering courses. We gathered student perspectives on how the use of GitHub in their courses might benefit them and to identify the challenges they may face. The findings from our case study indicate that software engineering students do benefit from GitHub’s transparent and open workflow. However, students were concerned that since GitHub is not inherently an educational tool, it lacks key features important for education and poses learning and privacy concerns. Our findings provide recommendations for designers on how tools such as GitHub can be used to improve software engineering education, and also point to recommendations for instructors on how to use it more effectively in their courses.",422-431,10.1145/2889160.2889195,http://www.pruitt-clark.com/register/
7aaa884f017f3f71a893b1755f75d801e17339ac,Software Engineering Component Based Software Engineering,"Component-based software engineering (CBSE) (also known as component-based development (CBD)) is a branch of software engineering that emphasizes the separation of concerns in respect of the wide-ranging functionality available throughout a given software system. It is a reuse-based approach to defining, implementing and composing loosely coupled independent components into systems. This practice aims to bring about an equally wide-ranging degree of benefits in both the short-term and the long-term for the software itself and for organizations that sponsor such software. This approach promises to alleviate the software crisis at great extents. The objective of this paper is to gain attention towards this new component based software development paradigm and to highlight the benefits of the approach for making it a successful software development approach to the concerned community",94-144,7741-7446-7,https://www.conway.info/wp-content/search/
994b8dbfa12027ed24285a1e34c20cae8831173c,Software-Specific Named Entity Recognition in Software Engineering Social Content,"Software engineering social content, such as Q&A discussions on Stack Overflow, has become a wealth of information on software engineering. This textual content is centered around software-specific entities, and their usage patterns, issues-solutions, and alternatives. However, existing approaches to analyzing software engineering texts treat software-specific entities in the same way as other content, and thus cannot support the recent advance of entity-centric applications, such as direct answers and knowledge graph. The first step towards enabling these entity-centric applications for software engineering is to recognize and classify software-specific entities, which is referred to as Named Entity Recognition (NER) in the literature. Existing NER methods are designed for recognizing person, location and organization in formal and social texts, which are not applicable to NER in software engineering. Existing information extraction methods for software engineering are limited to API identification and linking of a particular programming language. In this paper, we formulate the research problem of NER in software engineering. We identify the challenges in designing a software-specific NER system and propose a machine learning based approach applied on software engineering social content. Our NER system, called S-NER, is general for software engineering in that it can recognize a broad category of software entities for a wide range of popular programming languages, platform, and library. We conduct systematic experiments to evaluate our machine learning based S-NER against a well-designed, and to study the effectiveness of widely-adopted NER techniques and features in the face of the unique characteristics of software engineering social content.",90-101,10.1109/SANER.2016.10,http://walker.com/tags/post.php
768b444c84340d2210bd2782ce3aa39b723bd0b9,Game development software engineering process life cycle: a systematic review,,Jan-30,10.1186/s40411-016-0032-7,http://www.williams-bennett.com/author.html
7da816d0f1d2a2b33d6512a1e694c04cbe4d4963,Experimentation in Software Engineering,,"I-XXIII, 1-236",10.1007/978-3-642-29044-2,http://www.ramirez.org/blog/app/main/
794c598e037ffac8b8dec326ba29a5dd9044ece6,Modeling in Event-B - System and Software Engineering,"A practical text suitable for an introductory or advanced course in formal methods, this book presents a mathematical approach to modelling and designing systems using an extension of the B formal method: Event-B. Based on the idea of refinement, the author's systematic approach allows the user to construct models gradually and to facilitate a systematic reasoning method by means of proofs. Readers will learn how to build models of programs and, more generally, discrete systems, but this is all done with practice in mind. The numerous examples provided arise from various sources of computer system developments, including sequential programs, concurrent programs and electronic circuits. The book also contains a large number of exercises and projects ranging in difficulty. Each of the examples included in the book has been proved using the Rodin Platform tool set, which is available free for download at www.event-b.org.","I-XXVI, 1-586",10.1017/CBO9781139195881,http://www.dominguez.net/posts/terms/
b69a938050093e592e19a3b6321b3382d4f8bc7a,Global Software Engineering: Evolution and Trends,"Professional software products and IT systems and services today are developed mostly by globally distributed teams, projects, and companies. Successfully orchestrating Global Software Engineering (GSE) has become the major success factor both for organizations and practitioners. Yet, more than a half of all distributed projects does not achieve the intended objectives and is canceled. This paper summarizes experiences from academia and industry in a way to facilitate knowledge and technology transfer. It is based on an evaluation of 10 years of research, and industry collaboration and experience reported at the IEEE International Conference on Software Engineering (ICGSE) series. The outcomes of our analysis show GSE as a field highly attached to industry and, thus, a considerable share of ICGSE papers address the transfer of Software Engineering concepts and solutions to the global stage. We found collaboration and teams, processes and organization, sourcing and supplier management, and success factors to be the topics gaining the most interest of researchers and practitioners. Beyond the analysis of the past conferences, we also look at current trends in GSE to motivate further research and industrial collaboration.",144-153,10.1109/ICGSE.2016.19,https://walters.org/
fac3a6f272428e9d6879227ef76c1fa9397b317f,A Hitchhiker's guide to statistical tests for assessing randomized algorithms in software engineering,"Randomized algorithms are widely used to address many types of software engineering problems, especially in the area of software verification and validation with a strong emphasis on test automation. However, randomized algorithms are affected by chance and so require the use of appropriate statistical tests to be properly analysed in a sound manner. This paper features a systematic review regarding recent publications in 2009 and 2010 showing that, overall, empirical analyses involving randomized algorithms in software engineering tend to not properly account for the random nature of these algorithms. Many of the novel techniques presented clearly appear promising, but the lack of soundness in their empirical evaluations casts unfortunate doubts on their actual usefulness. In software engineering, although there are guidelines on how to carry out empirical analyses involving human subjects, those guidelines are not directly and fully applicable to randomized algorithms. Furthermore, many of the textbooks on statistical analysis are written from the viewpoints of social and natural sciences, which present different challenges from randomized algorithms. To address the questionable overall quality of the empirical analyses reported in the systematic review, this paper provides guidelines on how to carry out and properly analyse randomized algorithms applied to solve software engineering tasks, with a particular focus on software testing, which is by far the most frequent application area of randomized algorithms within software engineering. Copyright © 2012 John Wiley & Sons, Ltd.",67-117,10.1002/stvr.1486,https://www.hughes.com/categories/privacy.htm
ece51631d79e2c017471f31767d7c3b62dd45769,Guide to the Software Engineering Body of Knowledge (SWEBOK(R)): Version 3.0,"In the Guide to the Software Engineering Body of Knowledge (SWEBOK Guide), the IEEE Computer Society establishes a baseline for the body of knowledge for the field of software engineering, and the work supports the Societys responsibility to promote the advancement of both theory and practice in this field. It should be noted that the Guide does not purport to define the body of knowledge but rather to serve as a compendium and guide to the knowledge that has been developing and evolving over the past four decades. Now in Version 3.0, the Guides 15 knowledge areas summarize generally accepted topics and list references for detailed information. The editors for Version 3.0 of the SWEBOK Guide are Pierre Bourque (cole de technologie suprieure (TS), Universit du Qubec) and Richard E. (Dick) Fairley (Software and Systems Engineering Associates (S2EA)).",62-105,13-809014-5,https://greene.com/explore/login.php
ad7102b6bd0842fe1b46ce6a9246b1f00f51948f,ISO / IEC 25010 : 2011 Systems and software engineering — Systems and software Quality Requirements and Evaluation ( SQuaRE ) — System and software quality models,,75-124,10.3403/30215101,http://sosa-odom.com/index.htm
a5a6eb69869d2a25a5915ebec8e0991ba78c4769,Are Students Representatives of Professionals in Software Engineering Experiments?,"Background: Most of the experiments in software engineering (SE) employ students as subjects. This raises concerns about the realism of the results acquired through students and adaptability of the results to software industry. Aim: We compare students and professionals to understand how well students represent professionals as experimental subjects in SE research. Method: The comparison was made in the context of two test-driven development experiments conducted with students in an academic setting and with professionals in a software organization. We measured the code quality of several tasks implemented by both subject groups and checked whether students and professionals perform similarly in terms of code quality metrics. Results: Except for minor differences, neither of the subject groups is better than the other. Professionals produce larger, yet less complex, methods when they use their traditional development approach, whereas both subject groups perform similarly when they apply a new approach for the first time. Conclusion: Given a carefully scoped experiment on a development approach that is new to both students and professionals, similar performances are observed. Further investigation is necessary to analyze the effects of subject demographics and level of experience on the results of SE experiments.",666-676,10.1109/ICSE.2015.82,https://guzman.com/
2d1e79e057a9111ea6863378ffeca526a4e41c5f,On Non-Functional Requirements in Software Engineering,,363-379,10.1007/978-3-642-02463-4_19,https://www.dickerson-kelly.com/
63b35f109c6962247cbbf9458d55082c653f1d9e,A practical guide to controlled experiments of software engineering tools with human participants,,110-141,10.1007/s10664-013-9279-3,https://moon-decker.com/blog/about.html
7a28a601877a3722126c74149efc2a5f207b08e2,Docker [Software engineering],"In episode 217 of Software Engineering Radio, host Charles Anderson talks with James Turnbull, a software developer and security specialist who's vice president of services at Docker. Lightweight Docker containers are rapidly becoming a tool for deploying microservice-based architectures.",86-109,10.1109/MS.2015.62,https://www.mejia-brown.net/main/wp-content/tags/index/
641bb6adbbef003b3b74a1bbd5e11dbe251d6a66,Towards a decision-making structure for selecting a research design in empirical software engineering,,1427-1455,10.1007/s10664-014-9319-7,http://www.green.com/main.htm
bcd2c5379a34068040750a751e4fd2710d90c15c,The “Physics” of Notations: Toward a Scientific Basis for Constructing Visual Notations in Software Engineering,"Visual notations form an integral part of the language of software engineering (SE). Yet historically, SE researchers and notation designers have ignored or undervalued issues of visual representation. In evaluating and comparing notations, details of visual syntax are rarely discussed. In designing notations, the majority of effort is spent on semantics, with graphical conventions largely an afterthought. Typically, no design rationale, scientific or otherwise, is provided for visual representation choices. While SE has developed mature methods for evaluating and designing semantics, it lacks equivalent methods for visual syntax. This paper defines a set of principles for designing cognitively effective visual notations: ones that are optimized for human communication and problem solving. Together these form a design theory, called the Physics of Notations as it focuses on the physical (perceptual) properties of notations rather than their logical (semantic) properties. The principles were synthesized from theory and empirical evidence from a wide range of fields and rest on an explicit theory of how visual notations communicate. They can be used to evaluate, compare, and improve existing visual notations as well as to construct new ones. The paper identifies serious design flaws in some of the leading SE notations, together with practical suggestions for improving them. It also showcases some examples of visual notation design excellence from SE and other fields.",57-138,10.1109/TSE.2009.67,https://www.vang.biz/register/
c605abdfc50787818fce169198e0807c5b474c1c,Gamification in software engineering - A systematic mapping,,157-168,10.1016/j.infsof.2014.08.007,http://www.mcdaniel.com/about/
64fd3ee86c29633d439d02bbdc044b132e56ec7c,Choosing your weapons: On sentiment analysis tools for software engineering research,"Recent years have seen an increasing attention to social aspects of software engineering, including studies of emotions and sentiments experienced and expressed by the software developers. Most of these studies reuse existing sentiment analysis tools such as SentiStrength and NLTK. However, these tools have been trained on product reviews and movie reviews and, therefore, their results might not be applicable in the software engineering domain. In this paper we study whether the sentiment analysis tools agree with the sentiment recognized by human evaluators (as reported in an earlier study) as well as with each other. Furthermore, we evaluate the impact of the choice of a sentiment analysis tool on software engineering studies by conducting a simple study of differences in issue resolution times for positive, negative and neutral texts. We repeat the study for seven datasets (issue trackers and Stack Overflow questions) and different sentiment analysis tools and observe that the disagreement between the tools can lead to contradictory conclusions.",531-535,10.1109/ICSM.2015.7332508,http://moore-gross.com/
5a4674e987c2d7c130c5303cbad3f4e4531f3259,Case Study Research in Software Engineering - Guidelines and Examples,"Based on their own experiences of in-depth case studies of software projects in international corporations, in this bookthe authors present detailed practical guidelines on the preparation, conduct, design and reporting of case studies of software engineering. This is the first software engineering specific book on thecase study research method.","I-XVIII, 1-237",10.1002/9781118181034,http://avila.info/privacy.html
2f9a1286e7af4ab7706ad8cfcc8c8742a1964939,Views on Internal and External Validity in Empirical Software Engineering,"Empirical methods have grown common in software engineering, but there is no consensus on how to apply them properly. Is practical relevance key? Do internally valid studies have any value? Should we replicate more to address the tradeoff between internal and external validity? We asked the community how empirical research should take place in software engineering, with a focus on the tradeoff between internal and external validity and replication, complemented with a literature review about the status of empirical research in software engineering. We found that the opinions differ considerably, and that there is no consensus in the community when to focus on internal or external validity and how to conduct and review replications.",Sep-19,10.1109/ICSE.2015.24,https://www.davis.net/
0d63d4bcf9fb27ee0930d71cd0108fe629671b7a,Software Engineering: A Practitioners Approach,"Software engineering is the art of war. So if you don't know how to wage a war, then the weapons are useless. Software engineering has become very important because of the impact of large, expensive software systems and the role of software in safety-critical applications. This book supports a process to refound software engineering based on a solid theory, proven principles and best practices and fills a long-standing need in the software development communities to make the essential aspects of software development available in one comprehensive work. Written in an easy-to-understand tutorial format, SOFTWARE ENGINEERING: A Practitioners Approach provides professionals, researchers, and students at all levels with a clear coverage of: Analyzing, designing, programming and testing software projects. Set of objectives to which a prospective should be targeting to achieve. Two types of review questions-short answer type and descriptive type. List of key terms referring to abstract concepts, which may be used for better and crisp communication. Solution manual in electronic form available for qualified teachers on demand. Instructor's manual including power point slides, brief notes on teaching and list of projects with descriptions on demand. List of key references for the concepts in the chapter. Useful websites appended to each chapter for quick reference",32-131,385-05976-1,http://miller.com/author.htm
a652692b51c786bfa3ceb43f3ae9f6acb796921f,The (R) Evolution of social media in software engineering,"Software developers rely on media to communicate, learn, collaborate, and coordinate with others. Recently, social media has dramatically changed the landscape of software engineering, challenging some old assumptions about how developers learn and work with one another. We see the rise of the social programmer who actively participates in online communities and openly contributes to the creation of a large body of crowdsourced socio-technical content. In this paper, we examine the past, present, and future roles of social media in software engineering. We provide a review of research that examines the use of different media channels in software engineering from 1968 to the present day. We also provide preliminary results from a large survey with developers that actively use social media to understand how they communicate and collaborate, and to gain insights into the challenges they face. We find that while this particular population values social media, traditional channels, such as face-to-face communication, are still considered crucial. We synthesize findings from our historical review and survey to propose a roadmap for future research on this topic. Finally, we discuss implications for research methods as we argue that social media is poised to bring about a paradigm shift in software engineering research.",68-135,10.1145/2593882.2593887,https://www.burns-garza.biz/main.php
edb89d1462e96b4bace463f1fae9307717ac787a,Agent-Oriented Software Engineering,,53-139,10.1007/978-3-642-54432-3,http://rush.com/tag/list/index/
9992f1d6978c1e9442bf519aa213f5ca4e2159f8,Search based software engineering for software product line engineering: a survey and directions for future work,"This paper presents a survey of work on Search Based Software Engineering (SBSE) for Software Product Lines (SPLs). We have attempted to be comprehensive, in the sense that we have sought to include all papers that apply computational search techniques to problems in software product line engineering. Having surveyed the recent explosion in SBSE for SPL research activity, we highlight some directions for future work. We focus on suggestions for the development of recent advances in genetic improvement, showing how these might be exploited by SPL researchers and practitioners: Genetic improvement may grow new products with new functional and non-functional features and graft these into SPLs. It may also merge and parameterise multiple branches to cope with SPL branchmania.",55-143,10.1145/2648511.2648513,https://www.short.com/main/explore/main.php
3dcc789f0ecc07500a3539ec3805f80a03412103,A systematic review of systematic review process research in software engineering,,2049-2075,10.1016/j.infsof.2013.07.010,https://www.hayden.org/index.html
969d1e52140dbf8f58a1b3f61a29b03255490c86,Software Engineering Meets Control Theory,"The software engineering community has proposed numerous approaches for making software self-adaptive. These approaches take inspiration from machine learning and control theory, constructing software that monitors and modifies its own behavior to meet goals. Control theory, in particular, has received considerable attention as it represents a general methodology for creating adaptive systems. Control-theoretical software implementations, however, tend to be ad hoc. While such solutions often work in practice, it is difficult to understand and reason about the desired properties and behavior of the resulting adaptive software and its controller. This paper discusses a control design process for software systems which enables automatic analysis and synthesis of a controller that is guaranteed to have the desired properties and behavior. The paper documents the process and illustrates its use in an example that walks through all necessary steps for self-adaptive controller synthesis.",71-82,10.1109/SEAMS.2015.12,http://www.harris-davis.com/list/terms.html
0e528eb8167c68930c2e1ab20ab5c14f98446927,Investigating Country Differences in Mobile App User Behavior and Challenges for Software Engineering,"Mobile applications (apps) are software developed for use on mobile devices and made available through app stores. App stores are highly competitive markets where developers need to cater to a large number of users spanning multiple countries. This work hypothesizes that there exist country differences in mobile app user behavior and conducts one of the largest surveys to date of app users across the world, in order to identify the precise nature of those differences. The survey investigated user adoption of the app store concept, app needs, and rationale for selecting or abandoning an app. We collected data from more than 15 countries, including USA, China, Japan, Germany, France, Brazil, United Kingdom, Italy, Russia, India, Canada, Spain, Australia, Mexico, and South Korea. Analysis of data provided by 4,824 participants showed significant differences in app user behaviors across countries, for example users from USA are more likely to download medical apps, users from the United Kingdom and Canada are more likely to be influenced by price, users from Japan and Australia are less likely to rate apps. Analysis of the results revealed new challenges to market-driven software engineering related to packaging requirements, feature space, quality expectations, app store dependency, price sensitivity, and ecosystem effect.",40-64,10.1109/TSE.2014.2360674,http://gibson.org/main/explore/register.html
a639919277b2d0683d21b637bec2192fd475fa80,Happy software developers solve problems better: psychological measurements in empirical software engineering,"For more than thirty years, it has been claimed that a way to improve software developers’ productivity and software quality is to focus on people and to provide incentives to make developers satisfied and happy. This claim has rarely been verified in software engineering research, which faces an additional challenge in comparison to more traditional engineering fields: software development is an intellectual activity and is dominated by often-neglected human factors (called human aspects in software engineering research). Among the many skills required for software development, developers must possess high analytical problem-solving skills and creativity for the software construction process. According to psychology research, affective states—emotions and moods—deeply influence the cognitive processing abilities and performance of workers, including creativity and analytical problem solving. Nonetheless, little research has investigated the correlation between the affective states, creativity, and analytical problem-solving performance of programmers. This article echoes the call to employ psychological measurements in software engineering research. We report a study with 42 participants to investigate the relationship between the affective states, creativity, and analytical problem-solving skills of software developers. The results offer support for the claim that happy developers are indeed better problem solvers in terms of their analytical abilities. The following contributions are made by this study: (1) providing a better understanding of the impact of affective states on the creativity and analytical problem-solving capacities of developers, (2) introducing and validating psychological measurements, theories, and concepts of affective states, creativity, and analytical-problem-solving skills in empirical software engineering, and (3) raising the need for studying the human factors of software engineering by employing a multidisciplinary viewpoint.",31-142,10.7717/peerj.289,https://www.frye.com/main/
2aa4b8ab64847b5d040a4508a4029dea07e9cb41,An empirically based terminology and taxonomy for global software engineering,,105-153,10.1007/s10664-012-9217-9,https://buck.com/category/post/
389aa97e59372fbc65cab81fe18d9f86fbb70a82,Behavioral software engineering: A definition and systematic literature review,,15-37,10.1016/j.jss.2015.04.084,http://www.cruz.net/terms/
372fcbfccd9b02c69f0e42f1c94a64a044e1a11a,Replication of empirical studies in software engineering research: a systematic mapping study,,501-557,10.1007/s10664-012-9227-7,https://lopez-duke.com/tag/category/register/
402a9ff765bbf5a590a59e59790256c1be0ca330,Guidelines for Conducting Surveys in Software Engineering,,60-129,87810-515-6,http://gonzalez.com/categories/search/list/terms.asp
72c32a1672fad31ef32fadc4e119a5c5569e4cd0,Eye-Tracking Metrics in Software Engineering,"Eye-tracking studies are getting more prevalent in software engineering. Researchers often use different metrics when publishing their results in eye-tracking studies. Even when the same metrics are used, they are given different names, causing difficulties in comparing studies. To encourage replications and facilitate advancing the state of the art, it is important that the metrics used by researchers be clearly and consistently defined in the literature. There is therefore a need for a survey of eye-tracking metrics to support the (future) goal of standardizing eye-tracking metrics. This paper seeks to bring awareness to the use of different metrics along with practical suggestions on using them. It compares and contrasts various eye-tracking metrics used in software engineering. It also provides definitions for common metrics and discusses some metrics that the software engineering community might borrow from other fields.",96-103,10.1109/APSEC.2015.53,https://floyd-martin.info/
1a517c7b56d1d2f2d3e8e209c438e32663a750c9,Fundamentals of Software Engineering,,"I-XV, 1-573",10.1007/978-3-642-29320-7,http://jennings.com/home.htm
c24d6c1bed6af6b99e506f2ba83e9552481b5615,Green in Software Engineering,,76-106,10.1007/978-3-319-08581-4,http://jones.net/tags/list/wp-content/privacy.htm
7eb0399ae7e79449178d7fdaa86059fa0c856edd,"Software Product Line Engineering - Foundations, Principles, and Techniques","Software product line engineering has proven to be the methodology for developing a diversity of software products and software intensive systems at lower costs, in shorter time, and with higher quality. In this book, Pohl and his co-authors present a framework for software product line engineering which they have developed based on their academic as well as industrial experience gained in projects over the last eight years. They do not only detail the technical aspect of the development, but also an integrated view of the business, organisation and process aspects are given. In addition, they explicitly point out the key differences of software product line engineering compared to traditional single software system development, as the need for two distinct development processes for domain and application engineering respectively, or the need to define and manage variability.","I-XXVI, 1-467",04-657351-X,https://www.edwards-reeves.com/list/category.html
717a8985cc3fcbd285efe50dbcb8fe5bb12921b1,Challenges for Software Engineering in Automation,"This paper gives an 
introduction to the essential challenges of software engineering and 
requirements that software has to fulfill in the domain of automation. Besides, 
the functional characteristics, specific constraints and circumstances are 
considered for deriving requirements concerning usability, the technical 
process, the automation functions, used platform and the well-established 
models, which are described in detail. On the other hand, challenges result 
from the circumstances at different points in the single phases of the life 
cycle of the automated system. The requirements for life-cycle-management, 
tools and the changeability during runtime are described in detail.",440-451,10.4236/JSEA.2014.75041,https://mcbride.net/search/categories/category/
57e7a7323f58a35f5e2cc33bf17d4ac9cdcafdd4,Systematic and integrative analysis of large gene lists using DAVID bioinformatics resources,,44-57,10.1038/nprot.2008.211,http://www.curtis.com/post.htm
fa60b6806050255a77699bd0f9f5d824884c5162,Bioinformatics enrichment tools: paths toward the comprehensive functional analysis of large gene lists,"Functional analysis of large gene lists, derived in most cases from emerging high-throughput genomic, proteomic and bioinformatics scanning approaches, is still a challenging and daunting task. The gene-annotation enrichment analysis is a promising high-throughput strategy that increases the likelihood for investigators to identify biological processes most pertinent to their study. Approximately 68 bioinformatics enrichment tools that are currently available in the community are collected in this survey. Tools are uniquely categorized into three major classes, according to their underlying enrichment algorithms. The comprehensive collections, unique tool classifications and associated questions/issues will provide a more comprehensive and up-to-date view regarding the advantages, pitfalls and recent trends in a simpler tool-class level rather than by a tool-by-tool approach. Thus, the survey will help tool designers/developers and experienced end users understand the underlying algorithms and pertinent details of particular tool categories/tools, enabling them to make the best choices for their particular research interests.",Jan-13,10.1093/nar/gkn923,https://www.conway.com/
fd495d6cf7c3169bc58550fdf32be6e16e2800f8,Bioconductor: open software development for computational biology and bioinformatics,,R80 - R80,10.1186/gb-2004-5-10-r80,http://harris-jennings.com/list/register.php
90485e0ce54c1ad12a2d01362a007ab107d71063,Biopython: freely available Python tools for computational molecular biology and bioinformatics,"Summary: The Biopython project is a mature open source international collaboration of volunteer developers, providing Python libraries for a wide range of bioinformatics problems. Biopython includes modules for reading and writing different sequence file formats and multiple sequence alignments, dealing with 3D macro molecular structures, interacting with common tools such as BLAST, ClustalW and EMBOSS, accessing key online databases, as well as providing numerical methods for statistical learning. Availability: Biopython is freely available, with documentation and source code at www.biopython.org under the Biopython license. Contact: All queries should be directed to the Biopython mailing lists, see www.biopython.org/wiki/_Mailing_listspeter.cock@scri.ac.uk.",1422 - 1423,10.1093/bioinformatics/btp163,https://www.white.com/terms.php
a41d8c4eddf4054ef080c7edec21b39c492892ee,"Nanopore sequencing technology, bioinformatics and applications",,1348 - 1365,10.1038/s41587-021-01108-x,https://barker.com/post.htm
34d405eaecab40a932108a7ff97e92fb8fd1ae4e,A review of feature selection techniques in bioinformatics,"Feature selection techniques have become an apparent need in many bioinformatics applications. In addition to the large pool of techniques that have already been developed in the machine learning and data mining fields, specific applications in bioinformatics have led to a wealth of newly proposed techniques. In this article, we make the interested reader aware of the possibilities of feature selection, providing a basic taxonomy of feature selection techniques, and discussing their use, variety and potential in a number of both common as well as upcoming bioinformatics applications.","
          2507-17
        ",10.1093/bioinformatics/btm344,http://smith-fernandez.com/wp-content/tag/faq.htm
8002fefd7bcc66c23a8d49aa9a7ae8d4a9885ad3,"Expasy, the Swiss Bioinformatics Resource Portal, as designed by its users","Abstract The SIB Swiss Institute of Bioinformatics (https://www.sib.swiss) creates, maintains and disseminates a portfolio of reliable and state-of-the-art bioinformatics services and resources for the storage, analysis and interpretation of biological data. Through Expasy (https://www.expasy.org), the Swiss Bioinformatics Resource Portal, the scientific community worldwide, freely accesses more than 160 SIB resources supporting a wide range of life science and biomedical research areas. In 2020, Expasy was redesigned through a user-centric approach, known as User-Centred Design (UCD), whose aim is to create user interfaces that are easy-to-use, efficient and targeting the intended community. This approach, widely used in other fields such as marketing, e-commerce, and design of mobile applications, is still scarcely explored in bioinformatics. In total, around 50 people were actively involved, including internal stakeholders and end-users. In addition to an optimised interface that meets users' needs and expectations, the new version of Expasy provides an up-to-date and accurate description of high-quality resources based on a standardised ontology, allowing to connect functionally-related resources.",W216 - W227,10.1093/nar/gkab225,https://www.davis.info/posts/category/faq/
1ff4bd599b950218f0517fb76ee49ad0599e1c53,A Completely Reimplemented MPI Bioinformatics Toolkit with a New HHpred Server at its Core.,,"
          2237-2243
        ",10.1016/j.jmb.2017.12.007,https://brown.com/
7d7735582cfa14efb00d967e9af4d725579e8746,The PATRIC Bioinformatics Resource Center: expanding data and analysis capabilities,"The PathoSystems Resource Integration Center (PATRIC) is the bacterial Bioinformatics Resource Center funded by the National Institute of Allergy and Infectious Diseases (https://www.patricbrc.org). PATRIC supports bioinformatic analyses of all bacteria with a special emphasis on pathogens, offering a rich comparative analysis environment that provides users with access to over 250 000 uniformly annotated and publicly available genomes with curated metadata. PATRIC offers web-based visualization and comparative analysis tools, a private workspace in which users can analyze their own data in the context of the public collections, services that streamline complex bioinformatic workflows and command-line tools for bulk data analysis. Over the past several years, as genomic and other omics-related experiments have become more cost-effective and widespread, we have observed considerable growth in the usage of and demand for easy-to-use, publicly available bioinformatic tools and services. Here we report the recent updates to the PATRIC resource, including new web-based comparative analysis tools, eight new services and the release of a command-line interface to access, query and analyze data.",76-143,10.1093/nar/gkz943,http://wright.com/category/main/main/
70b4af707b1bf5eebacaadcf994335feb3d7a43a,The digestive system is a potential route of 2019-nCov infection: a bioinformatics analysis based on single-cell transcriptomes,"Since December 2019, a newly identified coronavirus (2019 novel coronavirus, 2019-nCov) is causing outbreak of pneumonia in one of largest cities, Wuhan, in Hubei province of China and has draw significant public health attention. The same as severe acute respiratory syndrome coronavirus (SARS-CoV), 2019-nCov enters into host cells via cell receptor angiotensin converting enzyme II (ACE2). In order to dissect the ACE2-expressing cell composition and proportion and explore a potential route of the 2019-nCov infection in digestive system infection, 4 datasets with single-cell transcriptomes of lung, esophagus, gastric, ileum and colon were analyzed. The data showed that ACE2 was not only highly expressed in the lung AT2 cells, esophagus upper and stratified epithelial cells but also in absorptive enterocytes from ileum and colon. These results indicated along with respiratory systems, digestive system is a potential routes for 2019-nCov infection. In conclusion, this study has provided the bioinformatics evidence of the potential route for infection of 2019-nCov in digestive system along with respiratory tract and may have significant impact for our healthy policy setting regards to prevention of 2019-nCoV infection.",55-150,10.1101/2020.01.30.927806,https://davis-wilson.com/category.php
17190dc2a51b4f0edd05753a4cea2a2540059933,The nf-core framework for community-curated bioinformatics pipelines,,276-278,10.1038/s41587-020-0439-x,http://williams-vargas.com/
766d3c1f67728737a255bd88e272f2bacb92d6e9,Protein Sequence Analysis Using the MPI Bioinformatics Toolkit,"The MPI Bioinformatics Toolkit (https://toolkit.tuebingen.mpg.de) provides interactive access to a wide range of the best‐performing bioinformatics tools and databases, including the state‐of‐the‐art protein sequence comparison methods HHblits and HHpred. The Toolkit currently includes 35 external and in‐house tools, covering functionalities such as sequence similarity searching, prediction of sequence features, and sequence classification. Due to this breadth of functionality, the tight interconnection of its constituent tools, and its ease of use, the Toolkit has become an important resource for biomedical research and for teaching protein sequence analysis to students in the life sciences. In this article, we provide detailed information on utilizing the three most widely accessed tools within the Toolkit: HHpred for the detection of homologs, HHpred in conjunction with MODELLER for structure prediction and homology modeling, and CLANS for the visualization of relationships in large sequence datasets. © 2020 The Authors.",24-104,10.1002/cpbi.108,https://www.english.com/tags/list/posts/home/
d6425d11904920cf6fafe895e6073e2131978e60,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),,80-141,10.1007/978-3-642-54420-0-13,http://www.landry-wheeler.biz/tags/tags/register.htm
bc54798db4962ec9ba6a5e87be3dce156454cfef,Augur: a bioinformatics toolkit for phylogenetic analyses of human pathogens,"Summary and statement of need The analysis of human pathogens requires a diverse collection of bioinformatics tools. These tools include standard genomic and phylogenetic software and custom software developed to handle the relatively numerous and short genomes of viruses and bacteria. Researchers increasingly depend on the outputs of these tools to infer transmission dynamics of human diseases and make actionable recommendations to public health officials (Black et al., 2020; Gardy et al., 2015). In order to enable real-time analyses of pathogen evolution, bioinformatics tools must scale rapidly with the number of samples and be flexible enough to adapt to a variety of questions and organisms. To meet these needs, we developed Augur, a bioinformatics toolkit designed for phylogenetic analyses of human pathogens.",31-136,10.21105/joss.02906,http://montgomery-benjamin.com/app/blog/search/terms/
f22e039275da9512fd59109165b99abf7c6910f9,Snakemake - a scalable bioinformatics workflow engine,Summary: Snakemake is a workflow engine that provides a readable Python-based workflow definition language and a powerful execution environment that scales from single-core workstations to compute clusters without modifying the workflow. It is the first system to support the use of automatically inferred multiple named wildcards (or variables) in input and output filenames.,"
          3600
        ",10.1093/bioinformatics/bty350,http://sanford.com/home/
0ff76dd78e47f4534ee148b644f1f2707bc70df5,"Improvements to PATRIC, the all-bacterial Bioinformatics Database and Analysis Resource Center","The Pathosystems Resource Integration Center (PATRIC) is the bacterial Bioinformatics Resource Center (https://www.patricbrc.org). Recent changes to PATRIC include a redesign of the web interface and some new services that provide users with a platform that takes them from raw reads to an integrated analysis experience. The redesigned interface allows researchers direct access to tools and data, and the emphasis has changed to user-created genome-groups, with detailed summaries and views of the data that researchers have selected. Perhaps the biggest change has been the enhanced capability for researchers to analyze their private data and compare it to the available public data. Researchers can assemble their raw sequence reads and annotate the contigs using RASTtk. PATRIC also provides services for RNA-Seq, variation, model reconstruction and differential expression analysis, all delivered through an updated private workspace. Private data can be compared by ‘virtual integration’ to any of PATRIC's public data. The number of genomes available for comparison in PATRIC has expanded to over 80 000, with a special emphasis on genomes with antimicrobial resistance data. PATRIC uses this data to improve both subsystem annotation and k-mer classification, and tags new genomes as having signatures that indicate susceptibility or resistance to specific antibiotics.",D535 - D542,10.1093/nar/gkw1017,https://www.morales.com/terms.php
c67f7d629c4500bd47056b03bdfaf2df2f3c9346,Human Splicing Finder: an online bioinformatics tool to predict splicing signals,"Thousands of mutations are identified yearly. Although many directly affect protein expression, an increasing proportion of mutations is now believed to influence mRNA splicing. They mostly affect existing splice sites, but synonymous, non-synonymous or nonsense mutations can also create or disrupt splice sites or auxiliary cis-splicing sequences. To facilitate the analysis of the different mutations, we designed Human Splicing Finder (HSF), a tool to predict the effects of mutations on splicing signals or to identify splicing motifs in any human sequence. It contains all available matrices for auxiliary sequence prediction as well as new ones for binding sites of the 9G8 and Tra2-β Serine-Arginine proteins and the hnRNP A1 ribonucleoprotein. We also developed new Position Weight Matrices to assess the strength of 5′ and 3′ splice sites and branch points. We evaluated HSF efficiency using a set of 83 intronic and 35 exonic mutations known to result in splicing defects. We showed that the mutation effect was correctly predicted in almost all cases. HSF could thus represent a valuable resource for research, diagnostic and therapeutic (e.g. therapeutic exon skipping) purposes as well as for global studies, such as the GEN2PHEN European Project or the Human Variome Project.",e67 - e67,10.1093/nar/gkp215,http://www.hart-thomas.org/privacy.jsp
cc384cff27d8a609f89f9e915c26bf31c39749a1,Deep learning in bioinformatics,"In the era of big data, transformation of biomedical big data into valuable knowledge has been one of the most important challenges in bioinformatics. Deep learning has advanced rapidly since the early 2000s and now demonstrates state-of-the-art performance in various fields. Accordingly, application of deep learning in bioinformatics to gain insight from data has been emphasized in both academia and industry. Here, we review deep learning in bioinformatics, presenting examples of current research. To provide a useful and comprehensive perspective, we categorize research both by the bioinformatics domain (i.e. omics, biomedical imaging, biomedical signal processing) and deep learning architecture (i.e. deep neural networks, convolutional neural networks, recurrent neural networks, emergent architectures) and present brief descriptions of each study. Additionally, we discuss theoretical and practical issues of deep learning in bioinformatics and suggest future research directions. We believe that this review will provide valuable insights and serve as a starting point for researchers to apply deep learning approaches in their bioinformatics studies.",851–869,10.1093/bib/bbw068,http://butler.com/
053e5c1f175cf385fb8ab551e68446e24b3475a5,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),,32-104,10.1007/978-3-642-40852-6-13,https://fritz.com/tag/about.htm
740c502c5596a570abfcffc1eefea160c2d6112c,BATMAN-TCM: a Bioinformatics Analysis Tool for Molecular mechANism of Traditional Chinese Medicine,,80-136,10.1038/srep21146,https://martin.biz/post.php
5c511ed1011c42a92fe1c4c5fe1fbc7190a5fde9,Piercing the dark matter: bioinformatics of long-range sequencing and mapping,,329-346,10.1038/s41576-018-0003-4,https://munoz-bennett.com/tags/home.php
219bee9fde1303f423fac52ff373f19552c60d53,Single-cell RNA sequencing technologies and bioinformatics pipelines,,17-150,10.1038/s12276-018-0071-8,https://ramsey.com/posts/faq/
13226c692dd6908c64555fb095c4ee968a539a83,Trends in the development of miRNA bioinformatics tools,"Abstract MicroRNAs (miRNAs) are small noncoding RNAs that regulate gene expression via recognition of cognate sequences and interference of transcriptional, translational or epigenetic processes. Bioinformatics tools developed for miRNA study include those for miRNA prediction and discovery, structure, analysis and target prediction. We manually curated 95 review papers and ∼1000 miRNA bioinformatics tools published since 2003. We classified and ranked them based on citation number or PageRank score, and then performed network analysis and text mining (TM) to study the miRNA tools development trends. Five key trends were observed: (1) miRNA identification and target prediction have been hot spots in the past decade; (2) manual curation and TM are the main methods for collecting miRNA knowledge from literature; (3) most early tools are well maintained and widely used; (4) classic machine learning methods retain their utility; however, novel ones have begun to emerge; (5) disease-associated miRNA tools are emerging. Our analysis yields significant insight into the past development and future directions of miRNA tools.",1836 - 1852,10.1093/bib/bby054,http://cervantes.com/
c464387a83a573c5154d551214628c760ff1a2b9,Unipro UGENE: a unified bioinformatics toolkit,"UNLABELLED
Unipro UGENE is a multiplatform open-source software with the main goal of assisting molecular biologists without much expertise in bioinformatics to manage, analyze and visualize their data. UGENE integrates widely used bioinformatics tools within a common user interface. The toolkit supports multiple biological data formats and allows the retrieval of data from remote data sources. It provides visualization modules for biological objects such as annotated genome sequences, Next Generation Sequencing (NGS) assembly data, multiple sequence alignments, phylogenetic trees and 3D structures. Most of the integrated algorithms are tuned for maximum performance by the usage of multithreading and special processor instructions. UGENE includes a visual environment for creating reusable workflows that can be launched on local resources or in a High Performance Computing (HPC) environment. UGENE is written in C++ using the Qt framework. The built-in plugin system and structured UGENE API make it possible to extend the toolkit with new functionality.


AVAILABILITY AND IMPLEMENTATION
UGENE binaries are freely available for MS Windows, Linux and Mac OS X at http://ugene.unipro.ru/download.html. UGENE code is licensed under the GPLv2; the information about the code licensing and copyright of integrated tools can be found in the LICENSE.3rd_party file provided with the source bundle.","
          1166-7
        ",10.1093/bioinformatics/bts091,https://www.newton.biz/tags/app/main/index.htm
4050c80ba36c6f86b0684a59be70182557b0770c,Ensemble deep learning in bioinformatics,,500 - 508,10.1038/s42256-020-0217-y,https://www.mercer-shaffer.com/author/
12467886cc2ab7aa5b0108001144ffaa3e67ded0,Deep learning-based clustering approaches for bioinformatics,"Abstract Clustering is central to many data-driven bioinformatics research and serves a powerful computational method. In particular, clustering helps at analyzing unstructured and high-dimensional data in the form of sequences, expressions, texts and images. Further, clustering is used to gain insights into biological processes in the genomics level, e.g. clustering of gene expressions provides insights on the natural structure inherent in the data, understanding gene functions, cellular processes, subtypes of cells and understanding gene regulations. Subsequently, clustering approaches, including hierarchical, centroid-based, distribution-based, density-based and self-organizing maps, have long been studied and used in classical machine learning settings. In contrast, deep learning (DL)-based representation and feature learning for clustering have not been reviewed and employed extensively. Since the quality of clustering is not only dependent on the distribution of data points but also on the learned representation, deep neural networks can be effective means to transform mappings from a high-dimensional data space into a lower-dimensional feature space, leading to improved clustering results. In this paper, we review state-of-the-art DL-based approaches for cluster analysis that are based on representation learning, which we hope to be useful, particularly for bioinformatics research. Further, we explore in detail the training procedures of DL-based clustering algorithms, point out different clustering quality metrics and evaluate several DL-based approaches on three bioinformatics use cases, including bioimaging, cancer genomics and biomedical text mining. We believe this review and the evaluation results will provide valuable insights and serve a starting point for researchers wanting to apply DL-based unsupervised methods to solve emerging bioinformatics research problems.",393 - 415,10.1093/bib/bbz170,https://www.rodriguez.com/privacy.htm
1e05e8c73a099b7ce11d4764706ba657c0e0d37b,The bioinformatics toolbox for circRNA discovery and analysis,"Abstract Circular RNAs (circRNAs) are a unique class of RNA molecule identified more than 40 years ago which are produced by a covalent linkage via back-splicing of linear RNA. Recent advances in sequencing technologies and bioinformatics tools have led directly to an ever-expanding field of types and biological functions of circRNAs. In parallel with technological developments, practical applications of circRNAs have arisen including their utilization as biomarkers of human disease. Currently, circRNA-associated bioinformatics tools can support projects including circRNA annotation, circRNA identification and network analysis of competing endogenous RNA (ceRNA). In this review, we collected about 100 circRNA-associated bioinformatics tools and summarized their current attributes and capabilities. We also performed network analysis and text mining on circRNA tool publications in order to reveal trends in their ongoing development.",1706 - 1728,10.1093/bib/bbaa001,http://www.mcdowell-wong.com/category/blog/tag/index/
fef2bd94ea0ece037c7f983b9d51a6a305482f07,Bioinformatics Methods for Mass Spectrometry-Based Proteomics Data Analysis,"Recent advances in mass spectrometry (MS)-based proteomics have enabled tremendous progress in the understanding of cellular mechanisms, disease progression, and the relationship between genotype and phenotype. Though many popular bioinformatics methods in proteomics are derived from other omics studies, novel analysis strategies are required to deal with the unique characteristics of proteomics data. In this review, we discuss the current developments in the bioinformatics methods used in proteomics and how they facilitate the mechanistic understanding of biological processes. We first introduce bioinformatics software and tools designed for mass spectrometry-based protein identification and quantification, and then we review the different statistical and machine learning methods that have been developed to perform comprehensive analysis in proteomics studies. We conclude with a discussion of how quantitative protein data can be used to reconstruct protein interactions and signaling networks.",72-130,10.3390/ijms21082873,https://shaw.org/login.php
32dc2a2ea7d1735b3c0cf5f782200453215fe6cd,The EMBL-EBI bioinformatics web and programmatic tools framework,"Since 2009 the EMBL-EBI Job Dispatcher framework has provided free access to a range of mainstream sequence analysis applications. These include sequence similarity search services (https://www.ebi.ac.uk/Tools/sss/) such as BLAST, FASTA and PSI-Search, multiple sequence alignment tools (https://www.ebi.ac.uk/Tools/msa/) such as Clustal Omega, MAFFT and T-Coffee, and other sequence analysis tools (https://www.ebi.ac.uk/Tools/pfa/) such as InterProScan. Through these services users can search mainstream sequence databases such as ENA, UniProt and Ensembl Genomes, utilising a uniform web interface or systematically through Web Services interfaces (https://www.ebi.ac.uk/Tools/webservices/) using common programming languages, and obtain enriched results with novel visualisations. Integration with EBI Search (https://www.ebi.ac.uk/ebisearch/) and the dbfetch retrieval service (https://www.ebi.ac.uk/Tools/dbfetch/) further expands the usefulness of the framework. New tools and updates such as NCBI BLAST+, InterProScan 5 and PfamScan, new categories such as RNA analysis tools (https://www.ebi.ac.uk/Tools/rna/), new databases such as ENA non-coding, WormBase ParaSite, Pfam and Rfam, and new workflow methods, together with the retirement of depreciated services, ensure that the framework remains relevant to today's biological community.",W580 - W584,10.1093/nar/gkv279,https://hernandez.com/about/
6966d2b49fa885897e0b2e18ce144831c55f3e5f,"Deep learning in bioinformatics: introduction, application, and perspective in big data era","Deep learning, which is especially formidable in handling big data, has achieved great success in various fields, including bioinformatics. With the advances of the big data era in biology, it is foreseeable that deep learning will become increasingly important in the field and will be incorporated in vast majorities of analysis pipelines. In this review, we provide both the exoteric introduction of deep learning, and concrete examples and implementations of its representative applications in bioinformatics. We start from the recent achievements of deep learning in the bioinformatics field, pointing out the problems which are suitable to use deep learning. After that, we introduce deep learning in an easy-to-understand fashion, from shallow neural networks to legendary convolutional neural networks, legendary recurrent neural networks, graph neural networks, generative adversarial networks, variational autoencoder, and the most recent state-of-the-art architectures. After that, we provide eight examples, covering five bioinformatics research directions and all the four kinds of data type, with the implementation written in Tensorflow and Keras. Finally, we discuss the common issues, such as overfitting and interpretability, that users will encounter when adopting deep learning methods and provide corresponding suggestions. The implementations are freely available at https://github.com/lykaust15/Deep_learning_examples.",32-149,10.1101/563601,http://ingram-collier.com/explore/wp-content/posts/terms.html
d0da9ce3ca989bce2579b64be9aed518265a8994,Bioinformatics and Computational Biology Solutions Using R and Bioconductor,"the difficulty of assessing utilities, “a formal decision model can. . . synthesize current best evidence and clinical judgment. . . and via its utility component, link this evidence to clinical decisions. . . Decision makers are. . . reluctant to delegate [decisions]. . . to model-based formalisms [but are]. . . increasingly. . . relying on structured approaches to make more informed decisions” (p. 69). The “Decision Making” chapter continues with a presentation of the main mathematical details of maximizing SEU that includes as an example a costeffectiveness analysis of stroke interventions combining QUALYs and the Markov model developed in earlier sections. The chapter concludes with a presentation of hypothesis testing and a brief mention of sample size selection. The final chapter of the “Methods” section, “Simulation,” introduces Markov chain Monte Carlo and includes a section on Monte Carlo estimation of expected utility. The second section, “Case Studies,” has chapters titled “Meta-Analysis,” “Decision Trees,” and “Chronic Disease Modeling.” In the first of these, hierarchical Bayesian meta-analysis of exchangeable studies is developed in some detail, including WinBUGS-style directed graphs and code. Topics include combination of continuous and dichotomous endpoints using latent variables and sensitivity to prior specification. The chapter summary (p. 124) suggests that the resulting posterior distributions of effect magnitudes, “can. . . become components of a formal decision analysis [or]. . . of a comprehensive decision model.” However, no such examples are presented, and this chapter, while an excellent, practical introduction to Bayesian meta-analysis, is not well integrated with the main topic of the book. The “Decision Trees” chapter is organized around a case study of the decision between axillary lymph node dissection or not followed by the choice of none or one of three adjuvant therapies in the treatment of early-stage breast cancer. It is a good, clear presentation of backward induction and uses most of the machinery set up in the methods chapters: posterior distributions, predictive distributions, QUALYs, and SEU maximization. The final case study, “Chronic Disease Modeling,” deals with optimizing the frequency of radiological screening for breast cancer. The core models are a continuous-time four-state Markov model (healthy, preclinical, clinical, and dead) for which the inferential component is the estimation of age-dependent transition densities (through sojourn-time distributions), a sensitivity model that may depend on age and tumor size, a model of the number of auxiliary lymph nodes involved that depends on age and tumor size, and a survival model that depends on node involvement and a vector of other prognostic variables. The decision to be made is the screening schedule as a function of age, and in theory, the trade-off is between the cost of screening and the expected gain in QUALYs. Needless to say, this case study is only sketched, but the sketch is sufficiently detailed to give an idea of the modeling strategies and how the backward induction was implemented. I have one small quibble: regarding the statement on page 33: “to speak of a probability distribution for [the parameter] β we need to imagine a metapopulation, or a universe of possible populations, each with a different [value of the parameter],” I believe that this is not at all what Bayesians mean by probability. Rather, Bayesian inference treats probabilities as epistemic—referring to states of uncertainty based on incomplete information, not to alternate universes. To say, for example, that the speed of light, C, has a distribution is to say that I am uncertain about the exact value of C in this universe and that my uncertainty is described by a probability distribution. Speaking as a statistician, Modeling in Medical Decision Making: A Bayesian Approach would be good to use as one component in a graduate course on decision making following an introductory Bayesian course. It assumes an understanding of calculus, or at minimum, calculus notation. For established statisticians and biostatisticians, the book is a good way to get up to speed on Bayesian decision analysis in health care and could serve as an entry point into the large published literature alluded to at the beginning of this review.",388 - 389,10.1198/jasa.2007.s179,https://keller-leon.biz/post.jsp
bfada4dd6e30ed53835b0ab326f16140922ebf31,ExPASy: SIB bioinformatics resource portal,"ExPASy (http://www.expasy.org) has worldwide reputation as one of the main bioinformatics resources for proteomics. It has now evolved, becoming an extensible and integrative portal accessing many scientific resources, databases and software tools in different areas of life sciences. Scientists can henceforth access seamlessly a wide range of resources in many different domains, such as proteomics, genomics, phylogeny/evolution, systems biology, population genetics, transcriptomics, etc. The individual resources (databases, web-based and downloadable software tools) are hosted in a ‘decentralized’ way by different groups of the SIB Swiss Institute of Bioinformatics and partner institutions. Specifically, a single web portal provides a common entry point to a wide range of resources developed and operated by different SIB groups and external institutions. The portal features a search function across ‘selected’ resources. Additionally, the availability and usage of resources are monitored. The portal is aimed for both expert users and people who are not familiar with a specific domain in life sciences. The new web interface provides, in particular, visual guidance for newcomers to ExPASy.",W597 - W603,10.1093/nar/gks400,https://larson.com/tag/list/tag/main.jsp
4246613a89c19dbd326eee6cbadf6a933de21eb9,BMC Bioinformatics,"BMC Bioinformatics is part of the BMC series which publishes subject-specific journals focused on the needs of individual research communities across all areas of biology and medicine. We do not make editorial decisions on the basis of the interest of a study or its likely impact. Studies must be scientifically valid; for research articles this includes a scientifically sound research question, the use of suitable methods and analysis, and following community-agreed standards relevant to the research field.  Specific criteria for other article types can be found in the submission guidelines.  BMC series open, inclusive and trusted.",71-136,452-57136-7,https://www.holt.com/category/
fe540ba862a10e06c8b75b751745e8b2d6161c73,Perseus: A Bioinformatics Platform for Integrative Analysis of Proteomics Data in Cancer Research.,,"
          133-148
        ",10.1007/978-1-4939-7493-1_7,https://www.wheeler.net/privacy.jsp
10b40befe5942e997f88ab40bac1acd931147893,"PATRIC, the bacterial bioinformatics database and analysis resource","The Pathosystems Resource Integration Center (PATRIC) is the all-bacterial Bioinformatics Resource Center (BRC) (http://www.patricbrc.org). A joint effort by two of the original National Institute of Allergy and Infectious Diseases-funded BRCs, PATRIC provides researchers with an online resource that stores and integrates a variety of data types [e.g. genomics, transcriptomics, protein–protein interactions (PPIs), three-dimensional protein structures and sequence typing data] and associated metadata. Datatypes are summarized for individual genomes and across taxonomic levels. All genomes in PATRIC, currently more than 10 000, are consistently annotated using RAST, the Rapid Annotations using Subsystems Technology. Summaries of different data types are also provided for individual genes, where comparisons of different annotations are available, and also include available transcriptomic data. PATRIC provides a variety of ways for researchers to find data of interest and a private workspace where they can store both genomic and gene associations, and their own private data. Both private and public data can be analyzed together using a suite of tools to perform comparative genomic or transcriptomic analysis. PATRIC also includes integrated information related to disease and PPIs. All the data and integrated analysis and visualization tools are freely available. This manuscript describes updates to the PATRIC since its initial report in the 2007 NAR Database Issue.",D581 - D591,10.1093/nar/gkt1099,http://mason-mcmahon.com/
424b9225684cf54ef6d3939e2a4d7514bc90e67e,The Bio3D packages for structural bioinformatics,"Bio3D is a family of R packages for the analysis of biomolecular sequence, structure, and dynamics. Major functionality includes biomolecular database searching and retrieval, sequence and structure conservation analysis, ensemble normal mode analysis, protein structure and correlation network analysis, principal component, and related multivariate analysis methods. Here, we review recent package developments, including a new underlying segregation into separate packages for distinct analysis, and introduce a new method for structure analysis named ensemble difference distance matrix analysis (eDDM). The eDDM approach calculates and compares atomic distance matrices across large sets of homologous atomic structures to help identify the residue wise determinants underlying specific functional processes. An eDDM workflow is detailed along with an example application to a large protein family. As a new member of the Bio3D family, the Bio3D‐eddm package supports both experimental and theoretical simulation‐generated structures, is integrated with other methods for dissecting sequence‐structure–function relationships, and can be used in a highly automated and reproducible manner. Bio3D is distributed as an integrated set of platform independent open source R packages available from: http://thegrantlab.org/bio3d/.",20 - 30,10.1002/pro.3923,https://www.owens-payne.info/register/
38080784dcde5f8bdb86af9a186b47117db37133,Bioinformatics and Computational Tools for Next-Generation Sequencing Analysis in Clinical Genetics,"Clinical genetics has an important role in the healthcare system to provide a definitive diagnosis for many rare syndromes. It also can have an influence over genetics prevention, disease prognosis and assisting the selection of the best options of care/treatment for patients. Next-generation sequencing (NGS) has transformed clinical genetics making possible to analyze hundreds of genes at an unprecedented speed and at a lower price when comparing to conventional Sanger sequencing. Despite the growing literature concerning NGS in a clinical setting, this review aims to fill the gap that exists among (bio)informaticians, molecular geneticists and clinicians, by presenting a general overview of the NGS technology and workflow. First, we will review the current NGS platforms, focusing on the two main platforms Illumina and Ion Torrent, and discussing the major strong points and weaknesses intrinsic to each platform. Next, the NGS analytical bioinformatic pipelines are dissected, giving some emphasis to the algorithms commonly used to generate process data and to analyze sequence variants. Finally, the main challenges around NGS bioinformatics are placed in perspective for future developments. Even with the huge achievements made in NGS technology and bioinformatics, further improvements in bioinformatic algorithms are still required to deal with complex and genetically heterogeneous disorders.",61-146,10.3390/jcm9010132,http://alvarez.com/categories/explore/search/register/
04b71445fca3e98d4f572284a15ccb7d6c7cb917,Modern deep learning in bioinformatics,"Deep learning (DL) has shown explosive growth in its application to bioinformatics and has demonstrated thrillingly promising power to mine the complex relationship hidden in large-scale biological and biomedical data. A number of comprehensive reviews have been published on such applications, ranging from high-level reviews with future perspectives to those mainly serving as tutorials. These reviews have provided an excellent introduction to and guideline for applications of DL in bioinformatics, covering multiple types of machine learning (ML) problems, different DL architectures, and ranges of biological/biomedical problems. However, most of these reviews have focused on previous research, whereas current trends in the principled DL field and perspectives on their future developments and potential new applications to biology and biomedicine are still scarce. We will focus on modern DL, the ongoing trends and future directions of the principled DL field, and postulate new and major applications in bioinformatics.",823 - 827,10.1093/jmcb/mjaa030,https://www.watson.com/
a6409a9192845e3f321edb0c147d263d9fad783c,Recent Advances of Deep Learning in Bioinformatics and Computational Biology,"Extracting inherent valuable knowledge from omics big data remains as a daunting problem in bioinformatics and computational biology. Deep learning, as an emerging branch from machine learning, has exhibited unprecedented performance in quite a few applications from academia and industry. We highlight the difference and similarity in widely utilized models in deep learning studies, through discussing their basic structures, and reviewing diverse applications and disadvantages. We anticipate the work can serve as a meaningful perspective for further development of its theory, algorithm and application in bioinformatic and computational biology.",15-135,10.3389/fgene.2019.00214,https://www.vargas.biz/
224a97657b897bbae36494025e4aeef5bb18d155,Programmatic access to bioinformatics tools from EMBL-EBI update: 2017,"Abstract Since 2009 the EMBL-EBI provides free and unrestricted access to several bioinformatics tools via the user's browser as well as programmatically via Web Services APIs. Programmatic access to these tools, which is fundamental to bioinformatics, is increasingly important as more high-throughput data is generated, e.g. from proteomics and metagenomic experiments. Access is available using both the SOAP and RESTful approaches and their usage is reviewed regularly in order to ensure that the best, supported tools are available to all users. We present here an update describing the latest enhancement to the Job Dispatcher APIs as well as the governance under it.",W550 - W553,10.1093/nar/gkx273,https://www.nguyen.net/
6fd80c1f69da5ffae7b395e9d9d85ff8c061f885,VectorBase: an updated bioinformatics resource for invertebrate vectors and other organisms related with human diseases,"VectorBase is a National Institute of Allergy and Infectious Diseases supported Bioinformatics Resource Center (BRC) for invertebrate vectors of human pathogens. Now in its 11th year, VectorBase currently hosts the genomes of 35 organisms including a number of non-vectors for comparative analysis. Hosted data range from genome assemblies with annotated gene features, transcript and protein expression data to population genetics including variation and insecticide-resistance phenotypes. Here we describe improvements to our resource and the set of tools available for interrogating and accessing BRC data including the integration of Web Apollo to facilitate community annotation and providing Galaxy to support user-based workflows. VectorBase also actively supports our community through hands-on workshops and online tutorials. All information and data are freely available from our website at https://www.vectorbase.org/.",D707 - D713,10.1093/nar/gku1117,http://www.reyes.com/categories/category/search/search.asp
80e86c8ca16da71d7ff279619d7b72c82a14f564,A new bioinformatics analysis tools framework at EMBL–EBI,"The EMBL-EBI provides access to various mainstream sequence analysis applications. These include sequence similarity search services such as BLAST, FASTA, InterProScan and multiple sequence alignment tools such as ClustalW, T-Coffee and MUSCLE. Through the sequence similarity search services, the users can search mainstream sequence databases such as EMBL-Bank and UniProt, and more than 2000 completed genomes and proteomes. We present here a new framework aimed at both novice as well as expert users that exposes novel methods of obtaining annotations and visualizing sequence analysis results through one uniform and consistent interface. These services are available over the web and via Web Services interfaces for users who require systematic access or want to interface with customized pipe-lines and workflows using common programming languages. The framework features novel result visualizations and integration of domain and functional predictions for protein database searches. It is available at http://www.ebi.ac.uk/Tools/sss for sequence similarity searches and at http://www.ebi.ac.uk/Tools/msa for multiple sequence alignments.",W695 - W699,10.1093/nar/gkq313,https://www.miller.com/terms.html
0369a5e1ab11f46979d4b61419884724a022d62e,DAVID Bioinformatics Resources: expanded annotation database and novel algorithms to better extract biology from large gene lists,"All tools in the DAVID Bioinformatics Resources aim to provide functional interpretation of large lists of genes derived from genomic studies. The newly updated DAVID Bioinformatics Resources consists of the DAVID Knowledgebase and five integrated, web-based functional annotation tool suites: the DAVID Gene Functional Classification Tool, the DAVID Functional Annotation Tool, the DAVID Gene ID Conversion Tool, the DAVID Gene Name Viewer and the DAVID NIAID Pathogen Genome Browser. The expanded DAVID Knowledgebase now integrates almost all major and well-known public bioinformatics resources centralized by the DAVID Gene Concept, a single-linkage method to agglomerate tens of millions of diverse gene/protein identifiers and annotation terms from a variety of public bioinformatics databases. For any uploaded gene list, the DAVID Resources now provides not only the typical gene-term enrichment analysis, but also new tools and functions that allow users to condense large gene lists into gene functional groups, convert between gene/protein identifiers, visualize many-genes-to-many-terms relationships, cluster redundant and heterogeneous terms into groups, search for interesting and related genes or terms, dynamically view genes from their lists on bio-pathways and more. With DAVID (http://david.niaid.nih.gov), investigators gain more power to interpret the biological mechanisms associated with large gene lists.",W169 - W175,10.1093/nar/gkm415,http://higgins-faulkner.net/main/app/blog/post.jsp
0d67f9364948d1d48f3c6cb9b88e60afd5f6d460,The MPI bioinformatics Toolkit as an integrative platform for advanced protein sequence and structure analysis,"The MPI Bioinformatics Toolkit (http://toolkit.tuebingen.mpg.de) is an open, interactive web service for comprehensive and collaborative protein bioinformatic analysis. It offers a wide array of interconnected, state-of-the-art bioinformatics tools to experts and non-experts alike, developed both externally (e.g. BLAST+, HMMER3, MUSCLE) and internally (e.g. HHpred, HHblits, PCOILS). While a beta version of the Toolkit was released 10 years ago, the current production-level release has been available since 2008 and has serviced more than 1.6 million external user queries. The usage of the Toolkit has continued to increase linearly over the years, reaching more than 400 000 queries in 2015. In fact, through the breadth of its tools and their tight interconnection, the Toolkit has become an excellent platform for experimental scientists as well as a useful resource for teaching bioinformatic inquiry to students in the life sciences. In this article, we report on the evolution of the Toolkit over the last ten years, focusing on the expansion of the tool repertoire (e.g. CS-BLAST, HHblits) and on infrastructural work needed to remain operative in a changing web environment.",W410 - W415,10.1093/nar/gkw348,http://martinez.com/author.asp
dfa2727c776fc5b8dcd4d9217e4564e578ddb5a5,Bioinformatics and Computational Biology Solutions Using R and Bioconductor (Statistics for Biology and Health),,35-108,10.1007/0-387-29362-0,http://www.clayton-baker.org/register/
602ab24e8134815630da74c746405b3693836d6e,A Survey of Data Mining and Deep Learning in Bioinformatics,,Jan-20,10.1007/s10916-018-1003-9,https://www.hughes-andrews.com/app/list/main/category/
6be2e6dca6c66f3ad74eb01c168e3c104119f41c,Impacts of bioinformatics to medicinal chemistry.,"Facing the explosive growth of biological sequence data, such as those of protein/peptide and DNA/RNA, generated in the post-genomic age, many bioinformatical and mathematical approaches as well as physicochemical concepts have been introduced to timely derive useful informations from these biological sequences, in order to stimulate the development of medical science and drug design. Meanwhile, because of the rapid penetrations from these disciplines, medicinal chemistry is currently undergoing an unprecedented revolution. In this minireview, we are to summarize the progresses by focusing on the following six aspects. (1) Use the pseudo amino acid composition or PseAAC to predict various attributes of protein/peptide sequences that are useful for drug development. (2) Use pseudo oligonucleotide composition or PseKNC to do the same for DNA/RNA sequences. (3) Introduce the multi-label approach to study those systems where the constituent elements bear multiple characters and functions. (4) Utilize the graphical rules and ""wenxiang"" diagrams to analyze complicated biomedical systems. (5) Recent development in identifying the interactions of drugs with its various types of target proteins in cellular networking. (6) Distorted key theory and its application in developing peptide drugs.","
          218-34
        ",10.2174/1573406411666141229162834,http://pratt.com/blog/category/search/register/
36789799e464aa7465125ff8e778939843a0e89b,Taverna: a tool for the composition and enactment of bioinformatics workflows,"MOTIVATION
In silico experiments in bioinformatics involve the co-ordinated use of computational tools and information repositories. A growing number of these resources are being made available with programmatic access in the form of Web services. Bioinformatics scientists will need to orchestrate these Web services in workflows as part of their analyses.


RESULTS
The Taverna project has developed a tool for the composition and enactment of bioinformatics workflows for the life sciences community. The tool includes a workbench application which provides a graphical user interface for the composition of workflows. These workflows are written in a new language called the simple conceptual unified flow language (Scufl), where by each step within a workflow represents one atomic task. Two examples are used to illustrate the ease by which in silico experiments can be represented as Scufl workflows using the workbench application.","
          3045-54
        ",10.1093/BIOINFORMATICS/BTH361,http://silva-barnes.biz/privacy/
1279fe26020af034fd6f16b04a1c23427c127db3,Data-driven advice for applying machine learning to bioinformatics problems,"As the bioinformatics field grows, it must keep pace not only with new data but with new algorithms. Here we contribute a thorough analysis of 13 state-of-the-art, commonly used machine learning algorithms on a set of 165 publicly available classification problems in order to provide data-driven algorithm recommendations to current researchers. We present a number of statistical and visual comparisons of algorithm performance and quantify the effect of model selection and algorithm tuning for each algorithm and dataset. The analysis culminates in the recommendation of five algorithms with hyperparameters that maximize classifier performance across the tested problems, as well as general guidelines for applying machine learning to supervised classification problems.","
          192-203
        ",10.1142/9789813235533_0018,https://taylor.com/register/
d26f49d123b114ba7ec83164bd4edca15d398677,Sequence clustering in bioinformatics: an empirical study,"Sequence clustering is a basic bioinformatics task that is attracting renewed attention with the development of metagenomics and microbiomics. The latest sequencing techniques have decreased costs and as a result, massive amounts of DNA/RNA sequences are being produced. The challenge is to cluster the sequence data using stable, quick and accurate methods. For microbiome sequencing data, 16S ribosomal RNA operational taxonomic units are typically used. However, there is often a gap between algorithm developers and bioinformatics users. Different software tools can produce diverse results and users can find them difficult to analyze. Understanding the different clustering mechanisms is crucial to understanding the results that they produce. In this review, we selected several popular clustering tools, briefly explained the key computing principles, analyzed their characters and compared them using two independent benchmark datasets. Our aim is to assist bioinformatics users in employing suitable clustering tools effectively to analyze big sequencing data. Related data, codes and software tools were accessible at the link http://lab.malab.cn/∼lg/clustering/.",53-125,10.1093/bib/bby090,https://gonzalez.com/faq.html
e8df0290e84a7c7ff4df03e648126eb99085e2f8,Influenza Research Database: An integrated bioinformatics resource for influenza virus research,"The Influenza Research Database (IRD) is a U.S. National Institute of Allergy and Infectious Diseases (NIAID)-sponsored Bioinformatics Resource Center dedicated to providing bioinformatics support for influenza virus research. IRD facilitates the research and development of vaccines, diagnostics and therapeutics against influenza virus by providing a comprehensive collection of influenza-related data integrated from various sources, a growing suite of analysis and visualization tools for data mining and hypothesis generation, personal workbench spaces for data storage and sharing, and active user community support. Here, we describe the recent improvements in IRD including the use of cloud and high performance computing resources, analysis and visualization of user-provided sequence data with associated metadata, predictions of novel variant proteins, annotations of phenotype-associated sequence markers and their predicted phenotypic effects, hemagglutinin (HA) clade classifications, an automated tool for HA subtype numbering conversion, linkouts to disease event data and the addition of host factor and antiviral drug components. All data and tools are freely available without restriction from the IRD website at https://www.fludb.org.",D466 - D474,10.1093/nar/gkw857,https://www.owen-morales.com/author.html
e6a5bcdb576f2b0d965a9f71f24514553966716b,"Next Generation Sequencing and Bioinformatics Methodologies for Infectious Disease Research and Public Health: Approaches, Applications, and Considerations for Development of Laboratory Capacity.","Next generation sequencing (NGS) combined with bioinformatics has successfully been used in a vast array of analyses for infectious disease research of public health relevance. For instance, NGS and bioinformatics approaches have been used to identify outbreak origins, track transmissions, investigate epidemic dynamics, determine etiological agents of a disease, and discover novel human pathogens. However, implementation of high-quality NGS and bioinformatics in research and public health laboratories can be challenging. These challenges mainly include the choice of the sequencing platform and the sequencing approach, the choice of bioinformatics methodologies, access to the appropriate computation and information technology infrastructure, and recruiting and retaining personnel with the specialized skills and experience in this field. In this review, we summarize the most common NGS and bioinformatics workflows in the context of infectious disease genomic surveillance and pathogen discovery, and highlight the main challenges and considerations for setting up an NGS and bioinformatics-focused infectious disease research public health laboratory. We describe the most commonly used sequencing platforms and review their strengths and weaknesses. We review sequencing approaches that have been used for various pathogens and study questions, as well as the most common difficulties associated with these approaches that should be considered when implementing in a public health or research setting. In addition, we provide a review of some common bioinformatics tools and procedures used for pathogen discovery and genome assembly, along with the most common challenges and solutions. Finally, we summarize the bioinformatics of advanced viral, bacterial, and parasite pathogen characterization, including types of study questions that can be answered when utilizing NGS and bioinformatics.",74-112,10.1093/infdis/jiz286,https://gilmore.com/register.asp
a885fc8bcd9f398acdfeb1d64c37b523063b585e,A novel features ranking metric with application to scalable visual and bioinformatics data classification,,346-354,10.1016/j.neucom.2014.12.123,http://garrett-walker.org/
9ade9dace9c2351280b8be050b69ad2abe5e7d9d,A brief history of bioinformatics,"It is easy for today's students and researchers to believe that modern bioinformatics emerged recently to assist next-generation sequencing data analysis. However, the very beginnings of bioinformatics occurred more than 50 years ago, when desktop computers were still a hypothesis and DNA could not yet be sequenced. The foundations of bioinformatics were laid in the early 1960s with the application of computational methods to protein sequence analysis (notably, de novo sequence assembly, biological sequence databases and substitution models). Later on, DNA analysis also emerged due to parallel advances in (i) molecular biology methods, which allowed easier manipulation of DNA, as well as its sequencing, and (ii) computer science, which saw the rise of increasingly miniaturized and more powerful computers, as well as novel software better suited to handle bioinformatics tasks. In the 1990s through the 2000s, major improvements in sequencing technology, along with reduced costs, gave rise to an exponential increase of data. The arrival of 'Big Data' has laid out new challenges in terms of data mining and management, calling for more expertise from computer science into the field. Coupled with an ever-increasing amount of bioinformatics tools, biological Big Data had (and continues to have) profound implications on the predictive power and reproducibility of bioinformatics results. To overcome this issue, universities are now fully integrating this discipline into the curriculum of biology students. Recent subdisciplines such as synthetic biology, systems biology and whole-cell modeling have emerged from the ever-increasing complementarity between computer science and biology.",93-146,10.1093/bib/bby063,http://little-zuniga.org/login.asp
2392f7f8b64c2016bf57957d51f32b7cef2acbcd,An overview of topic modeling and its current applications in bioinformatics,,70-129,10.1186/s40064-016-3252-8,http://www.young.com/blog/main/post.asp
0a27e9b4d0e03c6d0b03ec0b900b48b0e4079969,Protein Bioinformatics Databases and Resources.,,"
          3-39
        ",10.1007/978-1-4939-6783-4_1,http://rivers.com/
632df9a1308ec631143f2897f79eeaf04208319d,Metabolomics technology and bioinformatics for precision medicine,"Precision medicine is rapidly emerging as a strategy to tailor medical treatment to a small group or even individual patients based on their genetics, environment and lifestyle. Precision medicine relies heavily on developments in systems biology and omics disciplines, including metabolomics. Combination of metabolomics with sophisticated bioinformatics analysis and mathematical modeling has an extreme power to provide a metabolic snapshot of the patient over the course of disease and treatment or classifying patients into subpopulations and subgroups requiring individual medical treatment. Although a powerful approach, metabolomics have certain limitations in technology and bioinformatics. We will review various aspects of metabolomics technology and bioinformatics, from data generation, bioinformatics analysis, data fusion and mathematical modeling to data management, in the context of precision medicine.",88-108,10.1093/bib/bbx170,http://smith.com/category/app/privacy/
8f6bf0f8ea557aec5376ba73eb39e674156c8bd1,A comparison of sequencing platforms and bioinformatics pipelines for compositional analysis of the gut microbiome,,40-140,10.1186/s12866-017-1101-8,https://www.henderson-roach.net/list/home/
cd8156fc9f17146b39dfaf47fcd20f1e3ab70791,Manual for Using Homomorphic Encryption for Bioinformatics,"Biological data science is an emerging field facing multiple challenges for hosting, sharing, computing on, and interacting with large data sets. Privacy regulations and concerns about the risks of leaking sensitive personal health and genomic data add another layer of complexity to the problem. Recent advances in cryptography over the last five years have yielded a tool, homomorphic encryption, which can be used to encrypt data in such a way that storage can be outsourced to an untrusted cloud, and the data can be computed on in a meaningful way in encrypted form, without access to decryption keys. This paper introduces homomorphic encryption to the bioinformatics community, and presents an informal “manual” for using the Simple Encrypted Arithmetic Library (SEAL), which we have made publicly available for bioinformatic, genomic, and other research purposes.",552-567,10.1109/JPROC.2016.2622218,https://www.shepard.com/wp-content/explore/login.html
526cdbcd0d3063a4743fcd8033d9e76bd356d9bd,A cloud-compatible bioinformatics pipeline for ultrarapid pathogen identification from next-generation sequencing of clinical samples,"Unbiased next-generation sequencing (NGS) approaches enable comprehensive pathogen detection in the clinical microbiology laboratory and have numerous applications for public health surveillance, outbreak investigation, and the diagnosis of infectious diseases. However, practical deployment of the technology is hindered by the bioinformatics challenge of analyzing results accurately and in a clinically relevant timeframe. Here we describe SURPI (“sequence-based ultrarapid pathogen identification”), a computational pipeline for pathogen identification from complex metagenomic NGS data generated from clinical samples, and demonstrate use of the pipeline in the analysis of 237 clinical samples comprising more than 1.1 billion sequences. Deployable on both cloud-based and standalone servers, SURPI leverages two state-of-the-art aligners for accelerated analyses, SNAP and RAPSearch, which are as accurate as existing bioinformatics tools but orders of magnitude faster in performance. In fast mode, SURPI detects viruses and bacteria by scanning data sets of 7–500 million reads in 11 min to 5 h, while in comprehensive mode, all known microorganisms are identified, followed by de novo assembly and protein homology searches for divergent viruses in 50 min to 16 h. SURPI has also directly contributed to real-time microbial diagnosis in acutely ill patients, underscoring its potential key role in the development of unbiased NGS-based clinical assays in infectious diseases that demand rapid turnaround times.",1180 - 1192,10.1101/gr.171934.113,http://torres.com/home/
042328a210e2a2301bf1bd81b238f8cc3bd82ba5,Performance measures in evaluating machine learning based bioinformatics predictors for classifications,,320-330,10.1007/s40484-016-0081-2,https://jones.com/category/
d7e1225452deed203a2f92d7cca29bcde27866e5,A Review of Bioinformatics Tools for Bio-Prospecting from Metagenomic Sequence Data,"The microbiome can be defined as the community of microorganisms that live in a particular environment. Metagenomics is the practice of sequencing DNA from the genomes of all organisms present in a particular sample, and has become a common method for the study of microbiome population structure and function. Increasingly, researchers are finding novel genes encoded within metagenomes, many of which may be of interest to the biotechnology and pharmaceutical industries. However, such “bioprospecting” requires a suite of sophisticated bioinformatics tools to make sense of the data. This review summarizes the most commonly used bioinformatics tools for the assembly and annotation of metagenomic sequence data with the aim of discovering novel genes.",18-134,10.3389/fgene.2017.00023,http://phillips-williams.net/search.html
02f42cf7470fbe514c6426e63631fc1ab5a2e350,Bioinformatics and peptidomics approaches to the discovery and analysis of food-derived bioactive peptides,,3463-3472,10.1007/s00216-018-0974-1,http://www.choi-holmes.com/search.html
f54d849fb9a111c0113f6c362d82d484b489193c,CLIMB (the Cloud Infrastructure for Microbial Bioinformatics): an online resource for the medical microbiology community,"The increasing availability and decreasing cost of high-throughput sequencing has transformed academic medical microbiology, delivering an explosion in available genomes while also driving advances in bioinformatics. However, many microbiologists are unable to exploit the resulting large genomics datasets because they do not have access to relevant computational resources and to an appropriate bioinformatics infrastructure. Here, we present the Cloud Infrastructure for Microbial Bioinformatics (CLIMB) facility, a shared computing infrastructure that has been designed from the ground up to provide an environment where microbiologists can share and reuse methods and data. DATA SUMMARY The paper describes a new, freely available public resource and therefore no data has been generated. The resource can be accessed at http://www.climb.ac.uk. Source code for software developed for the project can be found at http://github.com/MRC-CLIMB/ I/We confirm all supporting data, code and protocols have been provided within the article or through supplementary data files. IMPACT STATEMENT Technological advances mean that genome sequencing is now relatively simple, quick, and affordable. However, handling large genome datasets remains a significant challenge for many microbiologists, with substantial requirements for computational resources and expertise in data storage and analysis. This has led to fragmentary approaches to software development and data sharing that reduce the reproducibility of research and limits opportunities for bioinformatics training. Here, we describe a nationwide electronic infrastructure that has been designed to support the UK microbiology community, providing simple mechanisms for accessing large, shared, computational resources designed to meet the bioinformatic needs of microbiologists.",85-129,10.1099/mgen.0.000086,http://www.baker-garcia.org/home.php
e785892cf00aea8b58e638994b31a366d31b794e,Encyclopedia of Bioinformatics and Computational Biology: ABC of Bioinformatics,,44-112,10-592890-8,http://watson-hughes.com/post/
0cc0a1ba893e35adf01342ed8f91bfa0ad940416,An overview of bioinformatics tools for epitope prediction: Implications on vaccine development,,"
          405-14
        ",10.1016/j.jbi.2014.11.003,https://cole-perez.org/tag/category.html
e67a2993f38c125539a17c16791a6c73dbcc586d,A novel hierarchical selective ensemble classifier with bioinformatics application,,"
          82-90
        ",10.1016/j.artmed.2017.02.005,https://smith.com/list/main/
aca8d8cc3fec56b9b41dc1853a75c1f0c0bd7d91,Network Inference and Reconstruction in Bioinformatics,,805-813,10.1016/B978-0-12-809633-8.20290-2,https://www.santos.com/wp-content/posts/app/privacy.html
cdfe58e8c16884bf01989d7c35b0b7544710208f,Snakemake - a scalable bioinformatics workflow engine,"SUMMARY
Snakemake is a workflow engine that provides a readable Python-based workflow definition language and a powerful execution environment that scales from single-core workstations to compute clusters without modifying the workflow. It is the first system to support the use of automatically inferred multiple named wildcards (or variables) in input and output filenames.


AVAILABILITY
http://snakemake.googlecode.com.


CONTACT
johannes.koester@uni-due.de.","
          2520-2
        ",10.1093/bioinformatics/bts480,https://www.king.com/tag/login.html
cca5e74bf5303d9504e78bc58d61d9fe653d160d,The Road to Metagenomics: From Microbiology to DNA Sequencing Technologies and Bioinformatics,"The study of microorganisms that pervade each and every part of this planet has encountered many challenges through time such as the discovery of unknown organisms and the understanding of how they interact with their environment. The aim of this review is to take the reader along the timeline and major milestones that led us to modern metagenomics. This new and thriving area is likely to be an important contributor to solve different problems. The transition from classical microbiology to modern metagenomics studies has required the development of new branches of knowledge and specialization. Here, we will review how the availability of high-throughput sequencing technologies has transformed microbiology and bioinformatics and how to tackle the inherent computational challenges that arise from the DNA sequencing revolution. New computational methods are constantly developed to collect, process, and extract useful biological information from a variety of samples and complex datasets, but metagenomics needs the integration of several of these computational methods. Despite the level of specialization needed in bioinformatics, it is important that life-scientists have a good understanding of it for a correct experimental design, which allows them to reveal the information in a metagenome.",53-124,10.3389/fgene.2015.00348,https://www.jones-hatfield.net/
a60d075f1f31c25a7ed32ebeca877d19233dde26,Text Mining for Bioinformatics Using Biomedical Literature,,602-611,10.1016/B978-0-12-809633-8.20409-3,http://doyle-bradley.com/
e2991da06cec4ca1f8f9727279674cfccdeee93c,A global perspective on evolving bioinformatics and data science training needs,"Abstract Bioinformatics is now intrinsic to life science research, but the past decade has witnessed a continuing deficiency in this essential expertise. Basic data stewardship is still taught relatively rarely in life science education programmes, creating a chasm between theory and practice, and fuelling demand for bioinformatics training across all educational levels and career roles. Concerned by this, surveys have been conducted in recent years to monitor bioinformatics and computational training needs worldwide. This article briefly reviews the principal findings of a number of these studies. We see that there is still a strong appetite for short courses to improve expertise and confidence in data analysis and interpretation; strikingly, however, the most urgent appeal is for bioinformatics to be woven into the fabric of life science degree programmes. Satisfying the relentless training needs of current and future generations of life scientists will require a concerted response from stakeholders across the globe, who need to deliver sustainable solutions capable of both transforming education curricula and cultivating a new cadre of trainer scientists.",398 - 404,10.1093/bib/bbx100,https://www.watson.biz/category.html
8a8ec7b22ded7e455b1d156d0a57f3eba9cf38b0,Feature selection methods for big data bioinformatics: A survey from the search perspective.,,"
          21-31
        ",10.1016/j.ymeth.2016.08.014,http://www.smith.biz/index/
334c19412ba69307b5f25408c79aac4cb8f94906,Bioinformatics research at BGRS-2018,,92-131,10.1186/s12859-018-2566-7,http://gibson-roth.com/
a0672457c2687759fe88a44541cc53c81738c0da,The development and application of bioinformatics core competencies to improve bioinformatics training and education,"Bioinformatics is recognized as part of the essential knowledge base of numerous career paths in biomedical research and healthcare. However, there is little agreement in the field over what that knowledge entails or how best to provide it. These disagreements are compounded by the wide range of populations in need of bioinformatics training, with divergent prior backgrounds and intended application areas. The Curriculum Task Force of the International Society of Computational Biology (ISCB) Education Committee has sought to provide a framework for training needs and curricula in terms of a set of bioinformatics core competencies that cut across many user personas and training programs. The initial competencies developed based on surveys of employers and training programs have since been refined through a multiyear process of community engagement. This report describes the current status of the competencies and presents a series of use cases illustrating how they are being applied in diverse training contexts. These use cases are intended to demonstrate how others can make use of the competencies and engage in the process of their continuing refinement and application. The report concludes with a consideration of remaining challenges and future plans.",28-113,10.1371/journal.pcbi.1005772,http://cain.com/register/
8c5c522a0d02b0487929412be41eb385ff1eb991,Aptamer Bioinformatics,"Aptamers are short nucleic acid sequences capable of specific, high-affinity molecular binding. They are isolated via SELEX (Systematic Evolution of Ligands by Exponential Enrichment), an evolutionary process that involves iterative rounds of selection and amplification before sequencing and aptamer characterization. As aptamers are genetic in nature, bioinformatic approaches have been used to improve both aptamers and their selection. This review will discuss the advancements made in several enclaves of aptamer bioinformatics, including simulation of aptamer selection, fragment-based aptamer design, patterning of libraries, identification of lead aptamers from high-throughput sequencing (HTS) data and in silico aptamer optimization.",35-104,10.3390/ijms18122516,https://wilson.com/app/blog/search/
fd23cfb37f4f69932edce56a13578b60b029db3a,PipeCraft: Flexible open‐source toolkit for bioinformatics analysis of custom high‐throughput amplicon sequencing data,"High‐throughput sequencing methods have become a routine analysis tool in environmental sciences as well as in public and private sector. These methods provide vast amount of data, which need to be analysed in several steps. Although the bioinformatics may be applied using several public tools, many analytical pipelines allow too few options for the optimal analysis for more complicated or customized designs. Here, we introduce PipeCraft, a flexible and handy bioinformatics pipeline with a user‐friendly graphical interface that links several public tools for analysing amplicon sequencing data. Users are able to customize the pipeline by selecting the most suitable tools and options to process raw sequences from Illumina, Pacific Biosciences, Ion Torrent and Roche 454 sequencing platforms. We described the design and options of PipeCraft and evaluated its performance by analysing the data sets from three different sequencing platforms. We demonstrated that PipeCraft is able to process large data sets within 24 hr. The graphical user interface and the automated links between various bioinformatics tools enable easy customization of the workflow. All analytical steps and options are recorded in log files and are easily traceable.",e234 - e240,10.1111/1755-0998.12692,http://www.wilson.net/list/app/homepage.html
4edf4880e93783901df1118ff174c839ffe2986f,Enabling the democratization of the genomics revolution with a fully integrated web-based bioinformatics platform,"Continued advancements in sequencing technologies have fueled the development of new sequencing applications and promise to flood current databases with raw data. A number of factors prevent the seamless and easy use of these data, including the breadth of project goals, the wide array of tools that individually perform fractions of any given analysis, the large number of associated software/hardware dependencies, and the detailed expertise required to perform these analyses. To address these issues, we have developed an intuitive web-based environment with a wide assortment of integrated and cutting-edge bioinformatics tools. These preconfigured workflows provide even novice next-generation sequencing users with the ability to perform many complex analyses with only a few mouse clicks, and, within the context of the same environment, to visualize and further interrogate their results. This bioinformatics platform is an initial attempt at Empowering the Development of Genomics Expertise (EDGE) in a wide range of applications.",67 - 80,10.1093/nar/gkw1027,https://www.williams.info/main/posts/main/
a541fb091828a8a52ab0bc5914c9f68ca89df3e5,"H3ABioNet, a sustainable pan-African bioinformatics network for human heredity and health in Africa","The application of genomics technologies to medicine and biomedical research is increasing in popularity, made possible by new high-throughput genotyping and sequencing technologies and improved data analysis capabilities. Some of the greatest genetic diversity among humans, animals, plants, and microbiota occurs in Africa, yet genomic research outputs from the continent are limited. The Human Heredity and Health in Africa (H3Africa) initiative was established to drive the development of genomic research for human health in Africa, and through recognition of the critical role of bioinformatics in this process, spurred the establishment of H3ABioNet, a pan-African bioinformatics network for H3Africa. The limitations in bioinformatics capacity on the continent have been a major contributory factor to the lack of notable outputs in high-throughput biology research. Although pockets of high-quality bioinformatics teams have existed previously, the majority of research institutions lack experienced faculty who can train and supervise bioinformatics students. H3ABioNet aims to address this dire need, specifically in the area of human genetics and genomics, but knock-on effects are ensuring this extends to other areas of bioinformatics. Here, we describe the emergence of genomics research and the development of bioinformatics in Africa through H3ABioNet.",271 - 277,10.1101/gr.196295.115,http://wilcox.org/blog/post.html
bf4e29b5b8afe5aa7dbf091f11d488a9b34c7e90,The Impact of Bioinformatics on Vaccine Design and Development,"Vaccines are the pharmaceutical products that offer the best cost‐benefit ratio in the pre‐ vention or treatment of diseases. In that a vaccine is a pharmaceutical product, vaccine development and production are costly and it takes years for this to be accomplished. Several approaches have been applied to reduce the times and costs of vaccine develop‐ ment, mainly focusing on the selection of appropriate antigens or antigenic structures, carriers, and adjuvants. One of these approaches is the incorporation of bioinformatics methods and analyses into vaccine development. This chapter provides an overview of the application of bioinformatics strategies in vaccine design and development, supply‐ ing some successful examples of vaccines in which bioinformatics has furnished a cutting edge in their development. Reverse vaccinology, immunoinformatics, and structural vac ‐ cinology are described and addressed in the design and development of specific vaccines against infectious diseases caused by bacteria, viruses, and parasites. These include some emerging or re‐emerging infectious diseases, as well as therapeutic vaccines to fight can‐ cer, allergies, and substance abuse, which have been facilitated and improved by using bioinformatics tools or which are under development based on bioinformatics strategies. antigenic B‐cell (IEDB) and CTL epitopes (NetCTL.1.2 server). They determined, by in silico studies, surface accessibility, surface flexibility, hydrophilicity, homology modeling (MODELLER ver. 9.12, CHARMM, WhatIF, PROCHECK, Verify 3D), and structure‐based epitope prediction for E protein, NS3, and NS5. They performed molecular docking of the ZIKV‐E protein with HLA‐A0201, of the ZIKV‐NS3 protein with HLA‐B2705, and of the ZIKV‐NS5 protein with HLA‐C0801 (PatchDock rigid‐body docking server, FireDock server). these",15-133,10.5772/INTECHOPEN.69273,https://harris.com/terms/
27464183a5d63c06d4a8fafb0ba4e61bbdb59b70,Single-Cell Transcriptomics Bioinformatics and Computational Challenges,"The emerging single-cell RNA-Seq (scRNA-Seq) technology holds the promise to revolutionize our understanding of diseases and associated biological processes at an unprecedented resolution. It opens the door to reveal intercellular heterogeneity and has been employed to a variety of applications, ranging from characterizing cancer cells subpopulations to elucidating tumor resistance mechanisms. Parallel to improving experimental protocols to deal with technological issues, deriving new analytical methods to interpret the complexity in scRNA-Seq data is just as challenging. Here, we review current state-of-the-art bioinformatics tools and methods for scRNA-Seq analysis, as well as addressing some critical analytical challenges that the field faces.",36-118,10.3389/fgene.2016.00163,https://giles-benjamin.com/blog/search/categories/index/
b48c077e8f368857a2453a5ac6a62108308dbe89,Systems Bioinformatics: increasing precision of computational diagnostics and therapeutics through network-based approaches,"Abstract Systems Bioinformatics is a relatively new approach, which lies in the intersection of systems biology and classical bioinformatics. It focuses on integrating information across different levels using a bottom-up approach as in systems biology with a data-driven top-down approach as in bioinformatics. The advent of omics technologies has provided the stepping-stone for the emergence of Systems Bioinformatics. These technologies provide a spectrum of information ranging from genomics, transcriptomics and proteomics to epigenomics, pharmacogenomics, metagenomics and metabolomics. Systems Bioinformatics is the framework in which systems approaches are applied to such data, setting the level of resolution as well as the boundary of the system of interest and studying the emerging properties of the system as a whole rather than the sum of the properties derived from the system’s individual components. A key approach in Systems Bioinformatics is the construction of multiple networks representing each level of the omics spectrum and their integration in a layered network that exchanges information within and between layers. Here, we provide evidence on how Systems Bioinformatics enhances computational therapeutics and diagnostics, hence paving the way to precision medicine. The aim of this review is to familiarize the reader with the emerging field of Systems Bioinformatics and to provide a comprehensive overview of its current state-of-the-art methods and technologies. Moreover, we provide examples of success stories and case studies that utilize such methods and tools to significantly advance research in the fields of systems biology and systems medicine.",806 - 824,10.1093/bib/bbx151,https://mercado-brown.biz/tag/register/
75c52a70ec235ceb8a522a1e7e884c9792211d47,Bioinformatics core competencies for undergraduate life sciences education,"Bioinformatics is becoming increasingly central to research in the life sciences. However, despite its importance, bioinformatics skills and knowledge are not well integrated in undergraduate biology education. This curricular gap prevents biology students from harnessing the full potential of their education, limiting their career opportunities and slowing genomic research innovation. To advance the integration of bioinformatics into life sciences education, a framework of core bioinformatics competencies is needed. To that end, we here report the results of a survey of life sciences faculty in the United States about teaching bioinformatics to undergraduate life scientists. Responses were received from 1,260 faculty representing institutions in all fifty states with a combined capacity to educate hundreds of thousands of students every year. Results indicate strong, widespread agreement that bioinformatics knowledge and skills are critical for undergraduate life scientists, as well as considerable agreement about which skills are necessary. Perceptions of the importance of some skills varied with the respondent’s degree of training, time since degree earned, and/or the Carnegie classification of the respondent’s institution. To assess which skills are currently being taught, we analyzed syllabi of courses with bioinformatics content submitted by survey respondents. Finally, we used the survey results, the analysis of syllabi, and our collective research and teaching expertise to develop a set of bioinformatics core competencies for undergraduate life sciences students. These core competencies are intended to serve as a guide for institutions as they work to integrate bioinformatics into their life sciences curricula. Significance Statement Bioinformatics, an interdisciplinary field that uses techniques from computer science and mathematics to store, manage, and analyze biological data, is becoming increasingly central to modern biology research. Given the widespread use of bioinformatics and its impacts on societal problem-solving (e.g., in healthcare, agriculture, and natural resources management), there is a growing need for the integration of bioinformatics competencies into undergraduate life sciences education. Here, we present a set of bioinformatics core competencies for undergraduate life scientists developed using the results of a large national survey and the expertise of our working group of bioinformaticians and educators. We also present results from the survey on the importance of bioinformatics skills and the current state of integration of bioinformatics into biology education.",55-145,10.1371/journal.pone.0196878,http://www.flores-snow.com/category/
4332d830f827f3b59b829b523f0e130aad75baae,"Review of Current Methods, Applications, and Data Management for the Bioinformatics Analysis of Whole Exome Sequencing","The advent of next-generation sequencing technologies has greatly promoted advances in the study of human diseases at the genomic, transcriptomic, and epigenetic levels. Exome sequencing, where the coding region of the genome is captured and sequenced at a deep level, has proven to be a cost-effective method to detect disease-causing variants and discover gene targets. In this review, we outline the general framework of whole exome sequence data analysis. We focus on established bioinformatics tools and applications that support five analytical steps: raw data quality assessment, preprocessing, alignment, post-processing, and variant analysis (detection, annotation, and prioritization). We evaluate the performance of open-source alignment programs and variant calling tools using simulated and benchmark datasets, and highlight the challenges posed by the lack of concordance among variant detection tools. Based on these results, we recommend adopting multiple tools and resources to reduce false positives and increase the sensitivity of variant calling. In addition, we briefly discuss the current status and solutions for big data management, analysis, and summarization in the field of bioinformatics.",67 - 82,10.4137/CIN.S13779,http://www.gross.net/posts/index.html
fbae68c4166d297cb4cc7b014906d559dfce484a,Bioinformatics applications on Apache Spark,"Abstract With the rapid development of next-generation sequencing technology, ever-increasing quantities of genomic data pose a tremendous challenge to data processing. Therefore, there is an urgent need for highly scalable and powerful computational systems. Among the state-of–the-art parallel computing platforms, Apache Spark is a fast, general-purpose, in-memory, iterative computing framework for large-scale data processing that ensures high fault tolerance and high scalability by introducing the resilient distributed dataset abstraction. In terms of performance, Spark can be up to 100 times faster in terms of memory access and 10 times faster in terms of disk access than Hadoop. Moreover, it provides advanced application programming interfaces in Java, Scala, Python, and R. It also supports some advanced components, including Spark SQL for structured data processing, MLlib for machine learning, GraphX for computing graphs, and Spark Streaming for stream computing. We surveyed Spark-based applications used in next-generation sequencing and other biological domains, such as epigenetics, phylogeny, and drug discovery. The results of this survey are used to provide a comprehensive guideline allowing bioinformatics researchers to apply Spark in their own fields.",40-141,10.1093/gigascience/giy098,http://rowe.org/privacy.html
ff0e69dad3d3c0c4f77c0ffdfdb986e2bfb1533a,A primer on microbial bioinformatics for nonbioinformaticians.,,"
          342-349
        ",10.1016/j.cmi.2017.12.015,https://morse.com/
aa8cd9b6126da24d6e772a6973e82b5443537780,Bioinformatics and Drug Discovery,"Bioinformatic analysis can not only accelerate drug target identification and drug candidate screening and refinement, but also facilitate characterization of side effects and predict drug resistance. High-throughput data such as genomic, epigenetic, genome architecture, cistromic, transcriptomic, proteomic, and ribosome profiling data have all made significant contribution to mechanism-based drug discovery and drug repurposing. Accumulation of protein and RNA structures, as well as development of homology modeling and protein structure simulation, coupled with large structure databases of small molecules and metabolites, paved the way for more realistic protein-ligand docking experiments and more informative virtual screening. I present the conceptual framework that drives the collection of these high-throughput data, summarize the utility and potential of mining these data in drug discovery, outline a few inherent limitations in data and software mining these data, point out news ways to refine analysis of these diverse types of data, and highlight commonly used software and databases relevant to drug discovery.",1709 - 1726,10.2174/1568026617666161116143440,http://page.com/main.html
a066d6337183fba874dd6f90805df9178f00d847,Designing a course model for distance-based online bioinformatics training in Africa: The H3ABioNet experience,"Africa is not unique in its need for basic bioinformatics training for individuals from a diverse range of academic backgrounds. However, particular logistical challenges in Africa, most notably access to bioinformatics expertise and internet stability, must be addressed in order to meet this need on the continent. H3ABioNet (www.h3abionet.org), the Pan African Bioinformatics Network for H3Africa, has therefore developed an innovative, free-of-charge “Introduction to Bioinformatics” course, taking these challenges into account as part of its educational efforts to provide on-site training and develop local expertise inside its network. A multiple-delivery–mode learning model was selected for this 3-month course in order to increase access to (mostly) African, expert bioinformatics trainers. The content of the course was developed to include a range of fundamental bioinformatics topics at the introductory level. For the first iteration of the course (2016), classrooms with a total of 364 enrolled participants were hosted at 20 institutions across 10 African countries. To ensure that classroom success did not depend on stable internet, trainers pre-recorded their lectures, and classrooms downloaded and watched these locally during biweekly contact sessions. The trainers were available via video conferencing to take questions during contact sessions, as well as via online “question and discussion” forums outside of contact session time. This learning model, developed for a resource-limited setting, could easily be adapted to other settings.",70-126,10.1371/journal.pcbi.1005715,http://price.biz/privacy/
13043cbc3317220e57f89b70344d9b3f63d3a347,Bioinformatics for precision oncology,"Abstract Molecular profiling of tumor biopsies plays an increasingly important role not only in cancer research, but also in the clinical management of cancer patients. Multi-omics approaches hold the promise of improving diagnostics, prognostics and personalized treatment. To deliver on this promise of precision oncology, appropriate bioinformatics methods for managing, integrating and analyzing large and complex data are necessary. Here, we discuss the specific requirements of bioinformatics methods and software that arise in the setting of clinical oncology, owing to a stricter regulatory environment and the need for rapid, highly reproducible and robust procedures. We describe the workflow of a molecular tumor board and the specific bioinformatics support that it requires, from the primary analysis of raw molecular profiling data to the automatic generation of a clinical report and its delivery to decision-making clinical oncologists. Such workflows have to various degrees been implemented in many clinical trials, as well as in molecular tumor boards at specialized cancer centers and university hospitals worldwide. We review these and more recent efforts to include other high-dimensional multi-omics patient profiles into the tumor board, as well as the state of clinical decision support software to translate molecular findings into treatment recommendations.",778 - 788,10.1093/bib/bbx143,https://glenn-jackson.com/terms.php
841dbff787e715ffb5b4c6b5a9841e0a2da0f1a0,The European Bioinformatics Institute in 2016: Data growth and integration,"New technologies are revolutionising biological research and its applications by making it easier and cheaper to generate ever-greater volumes and types of data. In response, the services and infrastructure of the European Bioinformatics Institute (EMBL-EBI, www.ebi.ac.uk) are continually expanding: total disk capacity increases significantly every year to keep pace with demand (75 petabytes as of December 2015), and interoperability between resources remains a strategic priority. Since 2014 we have launched two new resources: the European Variation Archive for genetic variation data and EMPIAR for two-dimensional electron microscopy data, as well as a Resource Description Framework platform. We also launched the Embassy Cloud service, which allows users to run large analyses in a virtual environment next to EMBL-EBI's vast public data resources.",D20 - D26,10.1093/nar/gkv1352,https://leon.com/terms.php
1a8bfb3b927778f643ae612562e0b9b915f614d3,Microarray bioinformatics in cancer- a review.,"Bioinformatics is one of the newest fields of biological research, and should be viewed broadly as the use of mathematical, statistical, and computational methods for the processing and analysis of biological data. Over the last decade, the rapid growth of information and technology in both ""genomics"" and ""omics"" eras has been overwhelming for the laboratory scientists to process experimental results. Traditional gene-by-gene approaches in research are insufficient to meet the growth and demand of biological research in understanding the true biology. The massive amounts of data generated by new technologies as genomic sequencing and microarray chips make the management of data and the integration of multiple platforms of high importance; this is then followed by data analysis and interpretation to achieve biological understanding and therapeutic progress. Global views of analyzing the magnitude of information are necessary and traditional approaches to lab work have steadily been changing towards a bioinformatics era. Research is moving from being restricted to a laboratory environment to working with computers in a ""virtual lab"" environment. The present review article shall put light on this emerging field and its applicability towards cancer research.","
          838-843
        ",8363-4579-6,https://green.net/main/search/categories/about.html
6696681640fb9c9dc5f81ea73a433f09bfe9886a,Developing reproducible bioinformatics analysis workflows for heterogeneous computing environments to support African genomics,,25-148,10.1186/s12859-018-2446-1,https://www.joyce.info/about.html
8263465748cb85e0945d5c530812b87f2b962bde,Metagenomics and Bioinformatics in Microbial Ecology: Current Status and Beyond,"Metagenomic approaches are now commonly used in microbial ecology to study microbial communities in more detail, including many strains that cannot be cultivated in the laboratory. Bioinformatic analyses make it possible to mine huge metagenomic datasets and discover general patterns that govern microbial ecosystems. However, the findings of typical metagenomic and bioinformatic analyses still do not completely describe the ecology and evolution of microbes in their environments. Most analyses still depend on straightforward sequence similarity searches against reference databases. We herein review the current state of metagenomics and bioinformatics in microbial ecology and discuss future directions for the field. New techniques will allow us to go beyond routine analyses and broaden our knowledge of microbial ecosystems. We need to enrich reference databases, promote platforms that enable meta- or comprehensive analyses of diverse metagenomic datasets, devise methods that utilize long-read sequence information, and develop more powerful bioinformatic methods to analyze data from diverse perspectives.",204 - 212,10.1264/jsme2.ME16024,http://www.gonzalez.com/category.jsp
3174838e6064eece0d05c0a29d42d6284d594b2d,Reproducible bioinformatics project: a community for reproducible bioinformatics analysis pipelines,,47-149,10.1186/s12859-018-2296-x,http://www.smith.org/app/terms/
a164044fe875d7d9d23a9f569c5b2c82660add4e,Bioinformatics in translational drug discovery,"Bioinformatics approaches are becoming ever more essential in translational drug discovery both in academia and within the pharmaceutical industry. Computational exploitation of the increasing volumes of data generated during all phases of drug discovery is enabling key challenges of the process to be addressed. Here, we highlight some of the areas in which bioinformatics resources and methods are being developed to support the drug discovery pipeline. These include the creation of large data warehouses, bioinformatics algorithms to analyse ‘big data’ that identify novel drug targets and/or biomarkers, programs to assess the tractability of targets, and prediction of repositioning opportunities that use licensed drugs to treat additional indications.",25-116,10.1042/BSR20160180,http://mccoy.biz/post/
262758686200e7d847e5489fff55b6a5919fcb1d,"Bioinformatics approaches, prospects and challenges of food bioactive peptide research",,137-143,10.1016/J.TIFS.2014.02.004,https://campbell-martinez.com/wp-content/blog/search/home/
6e1ca21d92705c43a285bfbd01391b45c6140ead,Proceedings of the 16th Annual UT-KBRIN Bioinformatics Summit 2016: bioinformatics,,90-144,10.1186/s12859-017-1781-y,http://becker.com/main/blog/tags/about.html
dfdd9d364b6c7860fd9e2df37b7970a230a646df,Barriers to integration of bioinformatics into undergraduate life sciences education: A national study of US life sciences faculty uncover significant barriers to integrating bioinformatics into undergraduate instruction,"Bioinformatics, a discipline that combines aspects of biology, statistics, and computer science, is increasingly important for biological research. However, bioinformatics instruction is rarely integrated into life sciences curricula at the undergraduate level. To understand why, the Network for Integrating Bioinformatics into Life Sciences Education (NIBLSE, “nibbles”) recently undertook an extensive survey of life sciences faculty in the United States. The survey responses to open-ended questions about barriers to integration were subjected to keyword analysis. The barrier most frequently reported by the ~1,260 respondents was lack of faculty training. Faculty at associate’s-granting institutions report the least training in bioinformatics and the least integration of bioinformatics into their teaching. Faculty from underrepresented minority groups (URMs) in STEM reported training barriers at a higher rate than others, although the number of URM respondents was small. Interestingly, the cohort of faculty with the most recently awarded PhD degrees reported the most training but were teaching bioinformatics at a lower rate than faculty who earned their degrees in previous decades. Other barriers reported included lack of student interest in bioinformatics; lack of student preparation in mathematics, statistics, and computer science; already overly full curricula; and limited access to resources, including hardware, software, and vetted teaching materials. The results of the survey, the largest to date on bioinformatics education, will guide efforts to further integrate bioinformatics instruction into undergraduate life sciences education.",26-135,10.1101/204420,http://www.king-holland.com/index.htm
78dbf5d24f1788ec47b723904e85dc68d74d77b7,Bioinformatics for clinical next generation sequencing.,"BACKGROUND
Next generation sequencing (NGS)-based assays continue to redefine the field of genetic testing. Owing to the complexity of the data, bioinformatics has become a necessary component in any laboratory implementing a clinical NGS test.


CONTENT
The computational components of an NGS-based work flow can be conceptualized as primary, secondary, and tertiary analytics. Each of these components addresses a necessary step in the transformation of raw data into clinically actionable knowledge. Understanding the basic concepts of these analysis steps is important in assessing and addressing the informatics needs of a molecular diagnostics laboratory. Equally critical is a familiarity with the regulatory requirements addressing the bioinformatics analyses. These and other topics are covered in this review article.


SUMMARY
Bioinformatics has become an important component in clinical laboratories generating, analyzing, maintaining, and interpreting data from molecular genetics testing. Given the rapid adoption of NGS-based clinical testing, service providers must develop informatics work flows that adhere to the rigor of clinical laboratory standards, yet are flexible to changes as the chemistry and software for analyzing sequencing data mature.","
          124-35
        ",10.1373/clinchem.2014.224360,https://www.yang-lee.info/
02f62fe778f1cdcdeb1e975f84ba054895202c26,"Knowledge Discovery and interactive Data Mining in Bioinformatics - State-of-the-Art, future challenges and research directions",,I1 - I1,10.1186/1471-2105-15-S6-I1,http://estrada.com/category.php
9de3f1ed6352262b24c63c3dad62e15a3ef5a653,Spectral graph theory,"With every graph (or digraph) one can associate several different matrices. We have already seen the vertex-edge incidence matrix, the Laplacian and the adjacency matrix of a graph. Here we shall concentrate mainly on the adjacency matrix of (undirected) graphs, and also discuss briefly the Laplacian. We shall show that spectral properies (the eigenvalues and eigenvectors) of these matrices provide useful information about the structure of the graph. It turns out that for regular graphs, the information one can deduce from one matrix representation (e.g., the adjacency matrix) is similar to the information one can deduce from other representations (such as the Laplacian). We remark that for nonregular graphs, this is not the case, and the choice of matrix representation may make a significant difference. We shall not elaborate on this issue further, as our main concern here will be either with regular or nearly regular graph. The adjacency matrix of a connected undirected graph is nonnegative, symmetric and irreducible (namely, it cannot be decomposed into two diagonal blocks and two off-diagonal blocks, one of which is all-0). As such, standard results n linear algebra, including the Perron-Frobenius theorem, imply that:",22-118,10.1090/cbms/129/05,https://bennett-barrera.info/list/category/about/
1ee0abcb8f0afd74d602255d529d7c2a036a8f02,Graph theory,,"I-V, 1-274",10.21236/ad0705364,http://www.ingram.org/app/index.asp
b07c157e7d40e06a4f2d486b16d5180d8b24acb9,Algebraic Graph Theory,,"I-XIX, 1-439",10.1007/978-1-4613-0163-9,https://www.howard-rodriguez.com/faq/
6bc77c4dc6075ee81c05f0f5f43e44b2a34a5876,Graph Theory with Applications,"When I first entered the world of Mathematics, I became aware of a strange and little-regarded sect of ""Graph Theorists"", inhabiting a shadowy borderland known to the rest of the community as the ""slums of Topology"". What changes there have been in a few short years! That shadowy borderland has become a thriving metropolis. International conferences on Graph Theory occur with almost embarrassing frequency. Journals on Graph Theory abound: I once counted the Editorial Offices of three of them in one of the mathematical departments of one of the Universities of one of the smaller cities of Canada. Any connection with Topology is likely to be firmly repudiated as soon as noted. I became aware of the burgeoning of Graph Theory when I studied the 1940 paper of Brooks, Smith, Stone and Tutte in the Duke Mathematical Journal, ostensibly on squared rectangles. They wrote of trees and Kirchhoffs Laws, of 3-connection and planarity, of duality and symmetry, of determinantal identities and coprime integers, ~ all in the Quest of the Perfect Square. I invariably recommend that paper to my students. ""Go to it"", I say, ""you will",28-125,938187-06-8,http://beasley.net/category/search/main/
54006d6fb211a090c98fee9d8103479b022c0db2,Introduction to graph theory,,73,10.1002/(SICI)1097-0037(199708)30:1%3C73::AID-NET8%3E3.0.CO;2-I,http://whitehead-chapman.com/index/
95d6ff6279fa0f92df6fae0e6bd4c259acfc8f09,Spectral Graph Theory,"Eigenvalues and the Laplacian of a graph Isoperimetric problems Diameters and eigenvalues Paths, flows, and routing Eigenvalues and quasi-randomness Expanders and explicit constructions Eigenvalues of symmetrical graphs Eigenvalues of subgraphs with boundary conditions Harnack inequalities Heat kernels Sobolev inequalities Advanced techniques for random walks on graphs Bibliography Index.",95-104,10.1090/cbms/092,https://www.villarreal-campos.net/wp-content/tag/tag/about/
c2f21a6b917286c7e904e0f168b53bbaa2bda4ba,Application of Graph Theory for Identifying Connectivity Patterns in Human Brain Networks: A Systematic Review,"Background: Analysis of the human connectome using functional magnetic resonance imaging (fMRI) started in the mid-1990s and attracted increasing attention in attempts to discover the neural underpinnings of human cognition and neurological disorders. In general, brain connectivity patterns from fMRI data are classified as statistical dependencies (functional connectivity) or causal interactions (effective connectivity) among various neural units. Computational methods, especially graph theory-based methods, have recently played a significant role in understanding brain connectivity architecture. Objectives: Thanks to the emergence of graph theoretical analysis, the main purpose of the current paper is to systematically review how brain properties can emerge through the interactions of distinct neuronal units in various cognitive and neurological applications using fMRI. Moreover, this article provides an overview of the existing functional and effective connectivity methods used to construct the brain network, along with their advantages and pitfalls. Methods: In this systematic review, the databases Science Direct, Scopus, arXiv, Google Scholar, IEEE Xplore, PsycINFO, PubMed, and SpringerLink are employed for exploring the evolution of computational methods in human brain connectivity from 1990 to the present, focusing on graph theory. The Cochrane Collaboration's tool was used to assess the risk of bias in individual studies. Results: Our results show that graph theory and its implications in cognitive neuroscience have attracted the attention of researchers since 2009 (as the Human Connectome Project launched), because of their prominent capability in characterizing the behavior of complex brain systems. Although graph theoretical approach can be generally applied to either functional or effective connectivity patterns during rest or task performance, to date, most articles have focused on the resting-state functional connectivity. Conclusions: This review provides an insight into how to utilize graph theoretical measures to make neurobiological inferences regarding the mechanisms underlying human cognition and behavior as well as different brain disorders.",93-101,10.3389/fnins.2019.00585,http://hurley-singleton.org/
1034712495a5d8f54d489674c3eac69250bf107e,Graph Theory,"Gaph Teory Fourth Edition Th is standard textbook of modern graph theory, now in its fourth edition, combines the authority of a classic with the engaging freshness of style that is the hallmark of active mathematics. It covers the core material of the subject with concise yet reliably complete proofs, while offering glimpses of more advanced methods in each fi eld by one or two deeper results, again with proofs given in full detail.",93-135,10.4171/owr/2005/03,http://pollard.info/main/category/about/
2e75dd6fb569ee917b6571d89afadd980af499c0,A Guide to Conquer the Biological Network Era Using Graph Theory,"Networks are one of the most common ways to represent biological systems as complex sets of binary interactions or relations between different bioentities. In this article, we discuss the basic graph theory concepts and the various graph types, as well as the available data structures for storing and reading graphs. In addition, we describe several network properties and we highlight some of the widely used network topological features. We briefly mention the network patterns, motifs and models, and we further comment on the types of biological and biomedical networks along with their corresponding computer- and human-readable file formats. Finally, we discuss a variety of algorithms and metrics for network analyses regarding graph drawing, clustering, visualization, link prediction, perturbation, and network alignment as well as the current state-of-the-art tools. We expect this review to reach a very broad spectrum of readers varying from experts to beginners while encouraging them to enhance the field further.",47-121,10.3389/fbioe.2020.00034,http://aguilar.com/main/search/login/
7624a21a8d962b4e0aaa90d2c9b7b70f98dd008f,Introduction to Graph Theory,"1. Fundamental Concepts. What Is a Graph? Paths, Cycles, and Trails. Vertex Degrees and Counting. Directed Graphs. 2. Trees and Distance. Basic Properties. Spanning Trees and Enumeration. Optimization and Trees. 3. Matchings and Factors. Matchings and Covers. Algorithms and Applications. Matchings in General Graphs. 4. Connectivity and Paths. Cuts and Connectivity. k-connected Graphs. Network Flow Problems. 5. Coloring of Graphs. Vertex Colorings and Upper Bounds. Structure of k-chromatic Graphs. Enumerative Aspects. 6. Planar Graphs. Embeddings and Euler's Formula. Characterization of Planar Graphs. Parameters of Planarity. 7. Edges and Cycles. Line Graphs and Edge-Coloring. Hamiltonian Cycles. Planarity, Coloring, and Cycles. 8. Additional Topics (Optional). Perfect Graphs. Matroids. Ramsey Theory. More Extremal Problems. Random Graphs. Eigenvalues of Graphs. Appendix A: Mathematical Background. Appendix B: Optimization and Complexity. Appendix C: Hints for Selected Exercises. Appendix D: Glossary of Terms. Appendix E: Supplemental Reading. Appendix F: References. Indices.",92-103,7730-8849-9,https://love-savage.com/blog/author/
8851f2f404b3322170c3c660fb3cf30a2911d6ec,Graph Theory,,40-128,10.1017/9781108568708.012,http://www.smith.com/posts/main/tag/login/
88648d2ae7c18ee6b79d1d2754cf92724787f8ce,Graph theory methods: applications in brain networks,"Network neuroscience is a thriving and rapidly expanding field. Empirical data on brain networks, from molecular to behavioral scales, are ever increasing in size and complexity. These developments lead to a strong demand for appropriate tools and methods that model and analyze brain network data, such as those provided by graph theory. This brief review surveys some of the most commonly used and neurobiologically insightful graph measures and techniques. Among these, the detection of network communities or modules, and the identification of central network elements that facilitate communication and signal transfer, are particularly salient. A number of emerging trends are the growing use of generative models, dynamic (time-varying) and multilayer networks, as well as the application of algebraic topology. Overall, graph theory methods are centrally important to understanding the architecture, development, and evolution of brain networks.",111 - 121,10.31887/DCNS.2018.20.2/osporns,http://www.franklin.com/search/terms.html
72744fb5963b168b4590e5eaa148c8d0e5eafe38,"Electrical Networks and Algebraic Graph Theory: Models, Properties, and Applications","Algebraic graph theory is a cornerstone in the study of electrical networks ranging from miniature integrated circuits to continental-scale power systems. Conversely, many fundamental results of algebraic graph theory were laid out by early electrical circuit analysts. In this paper, we survey some fundamental and historic as well as recent results on how algebraic graph theory informs electrical network analysis, dynamics, and design. In particular, we review the algebraic and spectral properties of graph adjacency, Laplacian, incidence, and resistance matrices and how they relate to the analysis, network reduction, and dynamics of certain classes of electrical networks. We study these relations for models of increasing complexity ranging from static resistive direct current (dc) circuits, over dynamic resistor..inductor..capacitor (RLC) circuits, to nonlinear alternating current (ac) power flow. We conclude this paper by presenting a set of fundamental open questions at the intersection of algebraic graph theory and electrical networks.",977-1005,10.1109/JPROC.2018.2821924,http://beck.org/categories/tags/blog/index/
8b7a2b2f4665be8b177abcf9a4d3634ef20eaefc,"Algebraic Graph Theory: Morphisms, Monoids and Matrices","This is a highly self-contained book about algebraic graph theory which iswritten with a view to keep the lively and unconventional atmosphere of a spoken text to communicate the enthusiasm the author feels about this subject. The focus is on homomorphisms and endomorphisms, matrices and eigenvalues. Graph models are extremely useful for almost all applications and applicators as they play an important role as structuring tools. They allow to model net structures -like roads, computers, telephones -instances of abstract data structures -likelists, stacks, trees -and functional or object oriented programming.",65-132,10.1515/9783110255096,https://www.rodriguez.com/
81b8dd00d832cc46bc72f11298b0839483c48d52,Graph theory approaches to functional network organization in brain disorders: A critique for a brave new small-world,"Over the past two decades, resting-state functional connectivity (RSFC) methods have provided new insights into the network organization of the human brain. Studies of brain disorders such as Alzheimer’s disease or depression have adapted tools from graph theory to characterize differences between healthy and patient populations. Here, we conducted a review of clinical network neuroscience, summarizing methodological details from 106 RSFC studies. Although this approach is prevalent and promising, our review identified four challenges. First, the composition of networks varied remarkably in terms of region parcellation and edge definition, which are fundamental to graph analyses. Second, many studies equated the number of connections across graphs, but this is conceptually problematic in clinical populations and may induce spurious group differences. Third, few graph metrics were reported in common, precluding meta-analyses. Fourth, some studies tested hypotheses at one level of the graph without a clear neurobiological rationale or considering how findings at one level (e.g., global topology) are contextualized by another (e.g., modular structure). Based on these themes, we conducted network simulations to demonstrate the impact of specific methodological decisions on case-control comparisons. Finally, we offer suggestions for promoting convergence across clinical studies in order to facilitate progress in this important field.",Jan-26,10.1162/NETN_a_00054,http://jensen.com/about.htm
ee4fd9cd27836870dd18eb2d81efac596a758fb1,Thermal Modeling in Metal Additive Manufacturing Using Graph Theory,"The goal of this work is to predict the effect of part geometry and process parameters on the instantaneous spatiotemporal distribution of temperature, also called the thermal field or temperature history, in metal parts as they are being built layer-by-layer using additive manufacturing (AM) processes. In pursuit of this goal, the objective of this work is to develop and verify a graph theory-based approach for predicting the temperature distribution in metal AM parts. This objective is consequential to overcome the current poor process consistency and part quality in AM. One of the main reasons for poor part quality in metal AM processes is ascribed to the nature of temperature distribution in the part. For instance, steep thermal gradients created in the part during printing leads to defects, such as warping and thermal stress-induced cracking. Existing nonproprietary approaches to predict the temperature distribution in AM parts predominantly use mesh-based finite element analyses that are computationally tortuous—the simulation of a few layers typically requires several hours, if not days. Hence, to alleviate these challenges in metal AM processes, there is a need for efficient computational models to predict the temperature distribution, and thereby guide part design and selection of process parameters instead of expensive empirical testing. Compared with finite element analyses techniques, the proposed mesh-free graph theory-based approach facilitates prediction of the temperature distribution within a few minutes on a desktop computer. To explore these assertions, we conducted the following two studies: (1) comparing the heat diffusion trends predicted using the graph theory approach with finite element analysis, and analytical heat transfer calculations based on Green’s functions for an elementary cuboid geometry which is subjected to an impulse heat input in a certain part of its volume and (2) simulating the laser powder bed fusion metal AM of three-part geometries with (a) Goldak’s moving heat source finite element method, (b) the proposed graph theory approach, and (c) further comparing the thermal trends predicted from the last two approaches with a commercial solution. From the first study, we report that the thermal trends approximated by the graph theory approach are found to be accurate within 5% of the Green’s functions-based analytical solution (in terms of the symmetric mean absolute percentage error). Results from the second study show that the thermal trends predicted for the AM parts using graph theory approach agree with finite element analyses, and the computational time for predicting the temperature distribution was significantly reduced with graph theory. For instance, for one of the AM part geometries studied, the temperature trends were predicted in less than 18 min within 10% error using the graph theory approach compared with over 180 min with finite element analyses. Although this paper is restricted to theoretical development and verification of the graph theory approach, our forthcoming research will focus on experimental validation through in-process thermal measurements.",77-104,10.1115/1.4043648,http://shields.com/main/search/tags/author.asp
968ea337e15004a2ed3e1442d7f632a1214e2268,Novel reliable routing method for engineering of internet of vehicles based on graph theory,"
Purpose
The communication link in the engineering of Internet of Vehicle (IOV) is more frequent than the communication link in the Mobile ad hoc Network (MANET). Therefore, the highly dynamic network routing reliability problem is a research hotspot to be solved.


Design/methodology/approach
The graph theory is used to model the MANET communication diagram on the highway and propose a new reliable routing method for internet of vehicles based on graph theory.


Findings
The expanded graph theory can help capture the evolution characteristics of the network topology and predetermine the reliable route to promote quality of service (QoS) in the routing process. The program can find the most reliable route from source to the destination from the MANET graph theory.


Originality/value
The good performance of the proposed method is verified and compared with the related algorithms of the literature.
",82-110,10.1108/EC-07-2018-0299,http://johnson.net/privacy.jsp
56a13467a3cfb7a9ec00b7f3ed5e953324225233,Graph Theory with Applications,,1-226,10.1057/jors.1977.45,http://hall.com/app/app/list/main.php
aec03b62900277709b10793a168058c5d05fd34f,A GENERAL POSITION PROBLEM IN GRAPH THEORY,"The paper introduces a graph theory variation of the general position problem: given a graph $G$ , determine a largest set $S$ of vertices of $G$ such that no three vertices of $S$ lie on a common geodesic. Such a set is a max-gp-set of $G$ and its size is the gp-number $\text{gp}(G)$ of $G$ . Upper bounds on $\text{gp}(G)$ in terms of different isometric covers are given and used to determine the gp-number of several classes of graphs. Connections between general position sets and packings are investigated and used to give lower bounds on the gp-number. It is also proved that the general position problem is NP-complete.",177 - 187,10.1017/S0004972718000473,http://parker.biz/
ecaa9d581d26cb23a759ca1310b0a5d8ce27b7b2,Topics in graph theory,"A graph is a system G = (V, E) consisting of a set V of vertices and a set E (disjoint from V ) of edges, together with an incidence function End : E → M2(V ), where M2(V ) is set of all 2-element sub-multisets of V . We usually write V = V (G), E = E(G), and End = EndG. For each edge e ∈ E with End(e) = {u, v}, we called u, v the end-vertices of e, and say that the edge e is incident with the vertices u, v, or the vertices u, v are incident with the edge e, or the vertices u, v are adjacent by the edge e. Sometimes it is more convenient to just write the incidence relation as e = uv. If u = v, the edge e is called a loop; if u 6= v, the edge is called a link. Two edges are said to be parallel if their end vertices are the same. Parallel edges are also referred to multiple edges. A simple graph is a graph without loops and multiple edges. When we emphasize that a graph may have loops and multiple edges, we refer the graph as a multigraph. A graph is said to be (i) finite if it has finite number of vertices and edges; (ii) null if it has no vertices, and consequently has no edges; (iii) trivial if it has only one vertex with possible loops; (iv) empty if its has no edges; and (v) nontrivial if it is not trivial. A complete graph is a simple graph that every pair of vertices are adjacent. A complete graph with n vertices is denoted by Kn. A graph G is said to be bipartite if its vertex set V (G) can be partitioned into two disjoint nonempty parts X,Y such that every edge has one end-vertex in X and the other in Y ; such a partition {X,Y } is called a bipartition of G, and such a bipartite graph is denoted by G[X,Y ]. A bipartite graph G[X,Y ] is called a complete bipartite graph if each vertex in X is joined to every vertex in Y ; we abbreviate G[X,Y ] to Km,n if |X| = m and |Y | = n. Let G be a graph. Two vertices of G are called neighbors each other if they are adjacent. For each vertex v ∈ V (G), the set of neighbors of v in G is denoted by Nv(G), the number of edges incident with v (loops counted twice) is called the degree of v in G, denoted deg (v) or deg G(v). A vertex of degree 0 is called an isolated vertex; a vertex of degree 1 is called a leaf. A graph is said to be regular if its every vertex has the same degree. A graph is said to be k-regular if its every vertex has degree k. We always have",24-118,10.1090/text/041/13,http://www.rivera-cook.com/list/tags/posts/index/
f5a04ec1d43b3d5385ad450dd28dcac11e300f64,Graph Theory,,35-115,10.1007/978-3-030-61943-5_3,http://www.salazar-carlson.com/terms.htm
bdba4cf0aa5c831301fd75e0d100f1fb14158bc6,Graph Theory,,16-103,10.4171/owr/2019/1,https://petersen-salazar.com/login.html
756bd8b609f9754fee6ae9e9056f5face4386726,BRAPH: A graph theory software for the analysis of brain connectivity,"The brain is a large-scale complex network whose workings rely on the interaction between its various regions. In the past few years, the organization of the human brain network has been studied extensively using concepts from graph theory, where the brain is represented as a set of nodes connected by edges. This representation of the brain as a connectome can be used to assess important measures that reflect its topological architecture. We have developed a freeware MatLab-based software (BRAPH – BRain Analysis using graPH theory) for connectivity analysis of brain networks derived from structural magnetic resonance imaging (MRI), functional MRI (fMRI), positron emission tomography (PET) and electroencephalogram (EEG) data. BRAPH allows building connectivity matrices, calculating global and local network measures, performing non-parametric permutations for group comparisons, assessing the modules in the network, and comparing the results to random networks. By contrast to other toolboxes, it allows performing longitudinal comparisons of the same patients across different points in time. Furthermore, even though a user-friendly interface is provided, the architecture of the program is modular (object-oriented) so that it can be easily expanded and customized. To demonstrate the abilities of BRAPH, we performed structural and functional graph theory analyses in two separate studies. In the first study, using MRI data, we assessed the differences in global and nodal network topology in healthy controls, patients with amnestic mild cognitive impairment, and patients with Alzheimer’s disease. In the second study, using resting-state fMRI data, we compared healthy controls and Parkinson’s patients with mild cognitive impairment.",86-138,10.1371/journal.pone.0178798,https://weber.com/category/posts/explore/category/
fc9051185b4879606ed006d00055940aee4da5e1,Modern Graph Theory,,83-145,10.1093/acprof:oso/9780199656592.003.0015,https://kennedy-reid.com/
2f1214615605b298733c008ffbd8a79051992473,Fuzzy Graph Theory,,46-137,10.1007/978-3-319-71407-3,https://www.green-ray.com/app/posts/faq.html
e1274867d404fd1dbf73a654c217ea5c0c32852c,Graph Theory and Brain Connectivity in Alzheimer’s Disease,This article presents a review of recent advances in neuroscience research in the specific area of brain connectivity as a potential biomarker of Alzheimer’s disease with a focus on the application of graph theory. The review will begin with a brief overview of connectivity and graph theory. Then resent advances in connectivity as a biomarker for Alzheimer’s disease will be presented and analyzed.,616 - 626,10.1177/1073858417702621,http://ruiz.com/posts/post.jsp
7d08932d11d5cf03ac2725250e83ec68f5a0f878,Graph Theory 1736 1936,"Thank you very much for downloading graph theory 1736 1936. Maybe you have knowledge that, people have search hundreds times for their favorite readings like this graph theory 1736 1936, but end up in malicious downloads. Rather than enjoying a good book with a cup of coffee in the afternoon, instead they cope with some malicious bugs inside their laptop. graph theory 1736 1936 is available in our digital library an online access to it is set as public so you can get it instantly. Our books collection spans in multiple countries, allowing you to get the most less latency time to download any of our books like this one. Kindly say, the graph theory 1736 1936 is universally compatible with any devices to read.",68-148,9721287-5-4,http://www.avila-rowe.net/explore/list/about.htm
0e63d3bea6a258017c43bb6c91f923839d260c8f,Graph Theory,,1-582,10.1007/978-1-84628-970-5,http://wallace.com/explore/list/category.asp
b9d26450520aa98e6ccb1b2e11a48fec29f273b9,Cytoscape.js: a graph theory library for visualisation and analysis,"Summary: Cytoscape.js is an open-source JavaScript-based graph library. Its most common use case is as a visualization software component, so it can be used to render interactive graphs in a web browser. It also can be used in a headless manner, useful for graph operations on a server, such as Node.js. Availability and implementation: Cytoscape.js is implemented in JavaScript. Documentation, downloads and source code are available at http://js.cytoscape.org. Contact: gary.bader@utoronto.ca",309 - 311,10.1093/bioinformatics/btv557,https://www.bowen.com/app/posts/list/search.htm
f8dab78b4ddd64a83bbc62cb153161a033dc06d1,Study of biological networks using graph theory,,1212 - 1219,10.1016/j.sjbs.2017.11.022,https://jones.com/about/
12748e904f5025ca1757ce49d72cad3878e1be8f,Graph Theory-Based Pinning Synchronization of Stochastic Complex Dynamical Networks,"This paper is concerned with the adaptive pinning synchronization problem of stochastic complex dynamical networks (CDNs). Based on algebraic graph theory and Lyapunov theory, pinning controller design conditions are derived, and the rigorous convergence analysis of synchronization errors in the probability sense is also conducted. Compared with the existing results, the topology structures of stochastic CDN are allowed to be unknown due to the use of graph theory. In particular, it is shown that the selection of nodes for pinning depends on the unknown lower bounds of coupling strengths. Finally, an example on a Chua’s circuit network is given to validate the effectiveness of the theoretical results.",427-437,10.1109/TNNLS.2016.2515080,http://www.brock-miller.net/search/tags/app/login/
6192ad6db166477e001d853ff9ca217a5bf8f7a3,"Network science and the human brain: Using graph theory to understand the brain and one of its hubs, the amygdala, in health and disease","Over the past 15 years, the emerging field of network science has revealed the key features of brain networks, which include small‐world topology, the presence of highly connected hubs, and hierarchical modularity. The value of network studies of the brain is underscored by the range of network alterations that have been identified in neurological and psychiatric disorders, including epilepsy, depression, Alzheimer's disease, schizophrenia, and many others. Here we briefly summarize the concepts of graph theory that are used to quantify network properties and describe common experimental approaches for analysis of brain networks of structural and functional connectivity. These range from tract tracing to functional magnetic resonance imaging, diffusion tensor imaging, electroencephalography, and magnetoencephalography. We then summarize the major findings from the application of graph theory to nervous systems ranging from Caenorhabditis elegans to more complex primate brains, including man. Focusing, then, on studies involving the amygdala, a brain region that has attracted intense interest as a center for emotional processing, fear, and motivation, we discuss the features of the amygdala in brain networks for fear conditioning and emotional perception. Finally, to highlight the utility of graph theory for studying dysfunction of the amygdala in mental illness, we review data with regard to changes in the hub properties of the amygdala in brain networks of patients with depression. We suggest that network studies of the human brain may serve to focus attention on regions and connections that act as principal drivers and controllers of brain function in health and disease.†Published 2016",81-114,10.1002/jnr.23705,https://www.stevens.com/app/blog/login/
74eb4d6abf1d0236be338c1bd5ee59a498b961b1,Band connectivity for topological quantum chemistry: Band structures as a graph theory problem,"The conventional theory of solids is well suited to describing band structures locally near isolated points in momentum space, but struggles to capture the full, global picture necessary for understanding topological phenomena. In part of a recent paper [B. Bradlyn et al., Nature 547, 298 (2017)], we have introduced the way to overcome this difficulty by formulating the problem of sewing together many disconnected local ""k-dot-p"" band structures across the Brillouin zone in terms of graph theory. In the current manuscript we give the details of our full theoretical construction. We show that crystal symmetries strongly constrain the allowed connectivities of energy bands, and we employ graph-theoretic techniques such as graph connectivity to enumerate all the solutions to these constraints. The tools of graph theory allow us to identify disconnected groups of bands in these solutions, and so identify topologically distinct insulating phases.",35138,10.1103/PhysRevB.97.035138,https://www.johnson.com/homepage/
06ba4f1bda6f3f413e5b98e23012b845878184d6,Graph Theory,,45-115,10.1007/978-3-030-21321-3_12,http://curtis-carr.com/login/
abb780f85a1a27919e461efdcc2a77cf1ab46c8b,Applying Graph Theory in Ecological Research,"Graph theory can be applied to ecological questions in many ways, and more insights can be gained by expanding the range of graph theoretical concepts applied to a specific system. But how do you know which methods might be used? And what do you do with the graph once it has been obtained? This book provides a broad introduction to the application of graph theory in different ecological systems, providing practical guidance for researchers in ecology and related fields. Readers are guided through the creation of an appropriate graph for the system being studied, including the application of spatial, spatio-temporal, and more abstract structural process graphs. Simple figures accompany the explanations to add clarity, and a broad range of ecological phenomena from many ecological systems are covered. This is the ideal book for graduate students and researchers looking to apply graph theoretical methods in their work.",49-117,10.1017/9781316105450,http://sharp-hodges.com/
f6c5a2a6cccae1db5f0a753946a2e89d7d22aa5d,"Handbook of graph theory, combinatorial optimization, and algorithms","Basic Concepts and Algorithms Basic Concepts in Graph Theory and Algorithms Subramanian Arumugam and Krishnaiyan ""KT"" Thulasiraman Basic Graph Algorithms Krishnaiyan ""KT"" Thulasiraman Depth-First Search and Applications Krishnaiyan ""KT"" Thulasiraman Flows in Networks Maximum Flow Problem F. Zeynep Sargut, Ravindra K. Ahuja, James B. Orlin, and Thomas L. Magnanti Minimum Cost Flow Problem Balachandran Vaidyanathan, Ravindra K. Ahuja, James B. Orlin, and Thomas L. Magnanti Multi-Commodity Flows Balachandran Vaidyanathan, Ravindra K. Ahuja, James B. Orlin, and Thomas L. Magnanti Algebraic Graph Theory Graphs and Vector Spaces Krishnaiyan ""KT"" Thulasiraman and M.N.S. Swamy Incidence, Cut, and Circuit Matrices of a Graph Krishnaiyan ""KT"" Thulasiraman and M.N.S. Swamy Adjacency Matrix and Signal Flow Graphs Krishnaiyan ""KT"" Thulasiraman and M.N.S. Swamy Adjacency Spectrum and the Laplacian Spectrum of a Graph R. Balakrishnan Resistance Networks, Random Walks, and Network Theorems Krishnaiyan ""KT"" Thulasiraman and Mamta Yadav Structural Graph Theory Connectivity Subramanian Arumugam and Karam Ebadi Connectivity Algorithms Krishnaiyan ""KT"" Thulasiraman Graph Connectivity Augmentation Andras Frank and Tibor Jordan Matchings Michael D. Plummer Matching Algorithms Krishnaiyan ""KT"" Thulasiraman Stable Marriage Problem Shuichi Miyazaki Domination in Graphs Subramanian Arumugam and M. Sundarakannan Graph Colorings Subramanian Arumugam and K. Raja Chandrasekar Planar Graphs Planarity and Duality Krishnaiyan ""KT"" Thulasiraman and M.N.S. Swamy Edge Addition Planarity Testing Algorithm John M. Boyer Planarity Testing Based on PC-Trees Wen-Lian Hsu Graph Drawing Md. Saidur Rahman and Takao Nishizeki Interconnection Networks Introduction to Interconnection Networks S.A. Choudum, Lavanya Sivakumar, and V. Sunitha Cayley Graphs S. Lakshmivarahan, Lavanya Sivakumar, and S.K. Dhall Graph Embedding and Interconnection Networks S.A. Choudum, Lavanya Sivakumar, and V. Sunitha Special Graphs Program Graphs Krishnaiyan ""KT"" Thulasiraman Perfect Graphs Chinh T. Hoang and R. Sritharan Tree-Structured Graphs Andreas Brandstadt and Feodor F. Dragan Partitioning Graph and Hypergraph Partitioning Sachin B. Patkar and H. Narayanan Matroids Matroids H. Narayanan and Sachin B. Patkar Hybrid Analysis and Combinatorial Optimization H. Narayanan Probabilistic Methods, Random Graph Models, and Randomized Algorithms Probabilistic Arguments in Combinatorics C.R. Subramanian Random Models and Analyses for Chemical Graphs Daniel Pascua, Tina M. Kouri, and Dinesh P. Mehta Randomized Graph Algorithms: Techniques and Analysis Surender Baswana and Sandeep Sen Coping with NP-Completeness General Techniques for Combinatorial Approximation Sartaj Sahni epsilon-Approximation Schemes for the Constrained Shortest Path Problem Krishnaiyan ""KT"" Thulasiraman Constrained Shortest Path Problem: Lagrangian Relaxation-Based Algorithmic Approaches Ying Xiao and Krishnaiyan ""KT"" Thulasiraman Algorithms for Finding Disjoint Paths with QoS Constraints Alex Sprintson and Ariel Orda Set-Cover Approximation Neal E. Young Approximation Schemes for Fractional Multicommodity Flow Problems George Karakostas Approximation Algorithms for Connectivity Problems Ramakrishna Thurimella Rectilinear Steiner Minimum Trees Tao Huang and Evangeline F.Y. Young Fixed-Parameter Algorithms and Complexity Venkatesh Raman and Saket Saurabh",81-116,10.1201/B19163,https://roth.com/
37c0809e246e593d524fa8a918eaee661124c21f,An Introduction to Bipolar Single Valued Neutrosophic Graph Theory,"In this paper, we first define the concept of bipolar single neutrosophic graphs as the generalization of bipolar fuzzy graphs, N-graphs, intuitionistic fuzzy graph, single valued neutrosophic graphs and bipolar intuitionistic fuzzy graphs.",184 - 191,10.4028/www.scientific.net/AMM.841.184,http://johnson-ellison.com/home.jsp
8e8152d46c8ff1070805096c214df7f389c57b80,Wavelets on Graphs via Spectral Graph Theory,,84-131,10.1016/J.ACHA.2010.04.005,http://keller-hart.org/privacy/
36e7356a12554d8af87094a9ca245b49b4f4a5c4,Basic Graph Theory,,1-163,10.1007/978-3-319-49475-3,http://www.jones-holloway.com/
cfc104f686b2190f64db03c8933d8f2a7fff5a5d,Molecular Orbital Calculations Using Chemical Graph Theory,"molecular orbital calculations using chemical graph theory is available in our digital library an online access to it is set as public so you can get it instantly. Our book servers hosts in multiple countries, allowing you to get the most less latency time to download any of our books like this one. Kindly say, the molecular orbital calculations using chemical graph theory is universally compatible with any devices to read.",27-134,04-685623-3,http://durham.com/posts/login/
d66c93134b8345d76c6d4d25cab52abb160fee67,A Brief Introduction to Spectral Graph Theory,"Spectral graph theory starts by associating matrices to graphs, notably, the adjacency matrix and the laplacian matrix. The general theme is then, firstly, to compute or estimate the eigenvalues of such matrices, and secondly, to relate the eigenvalues to structural properties of graphs. As it turns out, the spectral perspective is a powerful tool. Some of its loveliest applications concern facts that are, in principle, purely graph-theoretic or combinatorial. To give just one example, spectral ideas are a key ingredient in the proof of the so-called Friendship Theorem: if, in a group of people, any two persons have exactly one common friend, then there is a person who is everybody’s friend. This text is an introduction to spectral graph theory, but it could also be seen as an invitation to algebraic graph theory. On the one hand, there is, of course, the linear algebra that underlies the spectral ideas in graph theory. On the other hand, most of our examples are graphs of algebraic origin. The two recurring sources are Cayley graphs of groups, and graphs built out of finite fields. In the study of such graphs, some further algebraic ingredients (e.g., characters) naturally come up. The table of contents gives, as it should, a good glimpse of where is this text going. Very broadly, the first half is devoted to graphs, finite fields, and how they come together. This part is meant as an appealing and meaningful motivation. It provides a context that frames and fuels much of the second, spectral, half. Most sections have one or two exercises. Their position within the text is a hint. The exercises are optional, in the sense that virtually nothing in the main body depends on them. But the exercises are often of the non-trivial variety, and they should enhance the text in an interesting way. The hope is that the reader will enjoy them. We assume a basic familiarity with linear algebra, finite fields, and groups, but not necessarily with graph theory. This, again, betrays our algebraic perspective. This text is based on a course I taught in Göttingen, in the Fall of 2015. I would like to thank Jerome Baum for his help with some of the drawings. The present version is preliminary, and comments are welcome (email: bogdan.nica@gmail.com).",22-119,10.4171/188,http://www.johnson.biz/category/
a94b761fc2c9c299f5dc4c10d44e6d2bdd9a8267,Quantum Zero-Error Source-Channel Coding and Non-Commutative Graph Theory,"Alice and Bob receive a bipartite state (possibly entangled) from some finite collection or from some subspace. Alice sends a message to Bob through a noisy quantum channel such that Bob may determine the initial state, with zero chance of error. This framework encompasses, for example, teleportation, dense coding, entanglement assisted quantum channel capacity, and one-way communication complexity of function evaluation. With classical sources and channels, this problem can be analyzed using graph homomorphisms. We show this quantum version can be analyzed using homomorphisms on non-commutative graphs (an operator space generalization of graphs). Previously the Lovász ϑ number has been generalized to non-commutative graphs; we show this to be a homomorphism monotone, thus providing bounds on quantum source-channel coding. We generalize the Schrijver and Szegedy numbers, and show these to be monotones as well. As an application, we construct a quantum channel whose entanglement assisted zero-error one-shot capacity can only be unlocked using a non-maximally entangled state. These homomorphisms allow definition of a chromatic number for non-commutative graphs. Many open questions are presented regarding the possibility of a more fully developed theory.",554-577,10.1109/TIT.2015.2496377,https://keith.com/search/search/main/
788eab78c4c6e998fc6812b7d7af7ce6505af2e4,Graph Theory And Sparse Matrix Computation,"Thank you for downloading graph theory and sparse matrix computation. As you may know, people have look numerous times for their chosen readings like this graph theory and sparse matrix computation, but end up in infectious downloads. Rather than reading a good book with a cup of coffee in the afternoon, instead they juggled with some infectious virus inside their laptop. graph theory and sparse matrix computation is available in our book collection an online access to it is set as public so you can get it instantly. Our books collection hosts in multiple countries, allowing you to get the most less latency time to download any of our books like this one. Merely said, the graph theory and sparse matrix computation is universally compatible with any devices to read.",68-127,668-27244-3,https://pugh-holland.com/
d53aa6487575762c1a14addf273ea271fef24d29,Algorithmic graph theory and perfect graphs,,34-126,10.1016/c2013-0-10739-8,http://www.manning-henson.biz/main/
9893595f6950ca1340055da05fd2fae2b7b4dfd3,Three conjectures in extremal spectral graph theory,,137-161,10.1016/j.jctb.2017.04.006,http://www.avila.com/index.php
e9b90c75c5c99353a43cbf5e083334a59e50cca8,Rational exponents in extremal graph theory,"Given a family of graphs $\mathcal{H}$, the extremal number $\textrm{ex}(n, \mathcal{H})$ is the largest $m$ for which there exists a graph with $n$ vertices and $m$ edges containing no graph from the family $\mathcal{H}$ as a subgraph. We show that for every rational number $r$ between $1$ and $2$, there is a family of graphs $\mathcal{H}_r$ such that $\textrm{ex}(n, \mathcal{H}_r) = \Theta(n^r)$. This solves a longstanding problem in the area of extremal graph theory.",75-132,10.4171/JEMS/798,http://walker-ortega.com/app/tags/main/
d2ae0082986d965469ece8c0ebcf4885c84b9ccc,Presurgery resting‐state local graph‐theory measures predict neurocognitive outcomes after brain surgery in temporal lobe epilepsy,This study determined the ability of resting‐state functional connectivity (rsFC) graph‐theory measures to predict neurocognitive status postsurgery in patients with temporal lobe epilepsy (TLE) who underwent anterior temporal lobectomy (ATL).,44-148,10.1111/epi.12936,http://www.conner-smith.com/
bba08b50d33448a3b65b4baf7c0df3c1eca8b10d,Evolutionary graph theory revisited: when is an evolutionary process equivalent to the Moran process?,"Evolution in finite populations is often modelled using the classical Moran process. Over the last 10 years, this methodology has been extended to structured populations using evolutionary graph theory. An important question in any such population is whether a rare mutant has a higher or lower chance of fixating (the fixation probability) than the Moran probability, i.e. that from the original Moran model, which represents an unstructured population. As evolutionary graph theory has developed, different ways of considering the interactions between individuals through a graph and an associated matrix of weights have been considered, as have a number of important dynamics. In this paper, we revisit the original paper on evolutionary graph theory in light of these extensions to consider these developments in an integrated way. In particular, we find general criteria for when an evolutionary graph with general weights satisfies the Moran probability for the set of six common evolutionary dynamics.",100-128,10.1098/rspa.2015.0334,https://www.hall.com/wp-content/posts/tags/about/
e3acbca01b107b43b04f73499fbce2eeadc5970a,Neutrosophic Graphs: A New Dimension to Graph Theory,"In this book authors for the first time have made a through study of neutrosophic graphs. This study reveals that these neutrosophic graphs give a new dimension to graph theory. The important feature of this book is it contains over 200 neutrosophic graphs to provide better understanding of this concepts. Further these graphs happen to behave in a unique way inmost cases, for even the edge colouring problem is different from the classical one. Several directions and dimensions in graph theory are obtained from this study.",79-117,10.5281/ZENODO.31820,http://www.reeves.org/about.htm
9699f9186a38107ff5dfaae9d44dd1796cbe7ac7,Water Network Sectorization Based on Graph Theory and Energy Performance Indices,"AbstractThis paper proposes a new methodology for the optimal design of water network sectorization, which is an essential technique for improving the management and security of multiple-source water supply systems. In particular, the network sectorization problem under consideration concerns the definition of isolated district meter areas, each of which is supplied by its own source (or sources) and is completely disconnected from the rest of the water system through boundary valves or permanent pipe sectioning. The proposed methodology uses graph theory principles and a heuristic procedure based on minimizing the amount of dissipated power in the water network. The procedure has been tested on two existing water distribution networks (WDNs) (in Parete, Italy and San Luis Rio Colorado, Mexico) using different performance indices. The simulation results, which confirmed the effectiveness of the proposed methodology, surpass empirical trial-and-error approaches and offer water utilities a tool for the desi...",620-629,10.1061/(ASCE)WR.1943-5452.0000364,http://www.thompson.com/tag/tag/list/home/
b255694c69768b03bf854a72f3b0520befb0c509,The Fascinating World of Graph Theory,"The fascinating world of graph theory goes back several centuries and revolves around the study of graphsmathematical structures showing relations between objects. With applications in biology, computer science, transportation science, and other areas, graph theory encompasses some of the most beautiful formulas in mathematicsand some of its most famous problems. For example, what is the shortest route for a traveling salesman seeking to visit a number of cities in one trip? What is the least number of colors needed to fill in any map so that neighboring regions are always colored differently? Requiring readers to have a math background only up to high school algebra, this book explores the questions and puzzles that have been studied, and often solved, through graph theory. In doing so, the book looks at graph theorys development and the vibrant individuals responsible for the fields growth. Introducing graph theorys fundamental concepts, the authors explore a diverse plethora of classic problems such as the Lights Out Puzzle, the Minimum Spanning Tree Problem, the Knigsberg Bridge Problem, the Chinese Postman Problem, a Knights Tour, and the Road Coloring Problem. They present every type of graph imaginable, such as bipartite graphs, Eulerian graphs, the Petersen graph, and trees. Each chapter contains math exercises and problems for readers to savor. An eye-opening journey into the world of graphs, this book offers exciting problem-solving possibilities for mathematics and beyond.",16-141,10.5860/choice.190184,http://hamilton.com/categories/categories/posts/category.php
5b2d23a567b117ffe32dad3907a473e328127ff2,Modern Graph Theory,,"I-XIII, 1-394",10.1007/978-1-4612-0619-4,https://robinson.org/posts/register/
89ce4cf020e64bb4c7820b550ed9895525d31872,Functional connectivity and graph theory in preclinical Alzheimer's disease,,757-768,10.1016/j.neurobiolaging.2013.10.081,https://bailey.com/
0c2cab97a9ea7cc7f31801d21a19aec82fc9a28c,An Introduction To The Theory Of Graph Spectra,"an introduction to the theory of graph spectra is available in our book collection an online access to it is set as public so you can download it instantly. Our digital library hosts in multiple countries, allowing you to get the most less latency time to download any of our books like this one. Merely said, the an introduction to the theory of graph spectra is universally compatible with any devices to read.",84-123,10.5860/choice.48-0921,http://www.warner.org/register/
b19fd39320634d3421191e0614b32f809d8b0866,A Survey on some Applications of Graph Theory in Cryptography,"Abstract Graph theory is rapidly moving into the main stream of research because of its applications in diverse fields such as biochemistry (genomics), coding theory, communication networks and their security etc. In particular researchers are exploring the concepts of graph theory that can be used in different areas of Cryptography. In this paper a review of the works carried out in the field of Cryptography which use the concepts of Graph Theory, is given. Some of the Cryptographic Algorithms based on general graph theory concepts, Extremal Graph Theory and Expander Graphs are analyzed.",209 - 217,10.1080/09720529.2013.878819,https://www.owen-miller.biz/blog/faq.htm
73be8ad32db73d4f4f50c5dd96d71bdfb02ea9bb,Algorithmic graph theory and perfect graphs,,207-208,10.1007/BF00390110,https://randall.com/categories/about.html
ca73d88c497fb354a3a1f59fd2a3ca2bfbb7358c,Extremal Graph Theory,3 Third Lecture 11 3.1 Applications of the Zarankiewicz Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 3.2 The Turán Problem for Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 3.3 The Girth Problem and Moore’s Bound . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 3.4 Application of Moore’s Bound to Graph Spanners . . . . . . . . . . . . . . . . . . . . . . . 14,75-102,10.1201/9781439863879-10,http://www.campbell-howell.net/main/category/main/
3278d34e0eb3d58d36cc2300a4396664a6c950e5,Chemical Graph Theory,This chapter on chemical graph theory forms part of the natural science and processes section of the handbook,1538-1558,10.1201/B16132-66,https://becker-steele.com/index/
9455977ce159fdc496cda7128ec35e03615baf43,Graph theory in the geosciences,,147-160,10.1016/J.EARSCIREV.2015.02.002,https://lopez-wilkerson.org/app/about/
3bf89f38b238dd2d4ca4870e6b1fb28dbd136c84,"Handbook of Graph Theory, Second Edition","In the ten years since the publication of the best-selling first edition, more than 1,000 graph theory papers have been published each year. Reflecting these advances, Handbook of Graph Theory, Second Edition provides comprehensive coverage of the main topics in pure and applied graph theory. This second editionover 400 pages longer than its predecessorincorporates 14 new sections. Each chapter includes lists of essential definitions and facts, accompanied by examples, tables, remarks, and, in some cases, conjectures and open problems. A bibliography at the end of each chapter provides an extensive guide to the research literature and pointers to monographs. In addition, a glossary is included in each chapter as well as at the end of each section. This edition also contains notes regarding terminology and notation. With 34 new contributors, this handbook is the most comprehensive single-source guide to graph theory. It emphasizes quick accessibility to topics for non-experts and enables easy cross-referencing among chapters.",100-146,10.1201/B16132,http://www.copeland-mcfarland.info/categories/blog/login/
9318f9c688debeee0d671faae0f2cc384a673499,Automatic cone photoreceptor segmentation using graph theory and dynamic programming,"Geometrical analysis of the photoreceptor mosaic can reveal subclinical ocular pathologies. In this paper, we describe a fully automatic algorithm to identify and segment photoreceptors in adaptive optics ophthalmoscope images of the photoreceptor mosaic. This method is an extension of our previously described closed contour segmentation framework based on graph theory and dynamic programming (GTDP). We validated the performance of the proposed algorithm by comparing it to the state-of-the-art technique on a large data set consisting of over 200,000 cones and posted the results online. We found that the GTDP method achieved a higher detection rate, decreasing the cone miss rate by over a factor of five.",924 - 937,10.1364/BOE.4.000924,https://www.hernandez.com/index/
b131ac160af2c3ef91aff47f6578067183ca4c4b,"Descriptive Complexity, Canonisation, and Definable Graph Structure Theory","Descriptive complexity theory establishes a connection between the computational complexity of algorithmic problems (the computational resources required to solve the problems) and their descriptive complexity (the language resources required to describe the problems). This ground-breaking book approaches descriptive complexity from the angle of modern structural graph theory, specifically graph minor theory. It develops a ‘definable structure theory’ concerned with the logical definability of graph-theoretic concepts such as tree decompositions and embeddings. The first part starts with an introduction to the background, from logic, complexity, and graph theory, and develops the theory up to first applications in descriptive complexity theory and graph isomorphism testing. It may serve as the basis for a graduate-level course. The second part is more advanced and mainly devoted to the proof of a single, previously unpublished theorem: properties of graphs with excluded minors are decidable in polynomial time if, and only if, they are definable in fixed-point logic with counting.",28-139,10.1017/9781139028868,https://sutton.org/register.php
7923f4dd95866891d0bfeeae521f5803b8b98b67,A Simple Algorithm for the Graph Minor Decomposition - Logic meets Structural Graph Theory,"A key result of Robertson and Seymour's graph minor theory is a structure theorem stating that all graphs excluding some fixed graph as a minor have a tree decomposition into pieces that are almost embeddable in a fixed surface. Most algorithmic applications of graph minor theory rely on an algorithmic version of this result. However, the known algorithms for computing such graph minor decompositions heavily rely on the very long and complicated proofs of the existence of such decompositions, essentially they retrace these proofs and show that all steps are algorithmic. 
 
In this paper, we give a simple quadratic time algorithm for computing graph minor decompositions. The best previously known algorithm due to Kawarabayashi and Wollan runs in cubic time and is far more complicated. 
 
Our algorithm combines techniques from logic and structural graph theory, or more precisely, a variant of Courcelle's Theorem stating that monadic second-order logic formulas can be evaluated in linear time on graphs of bounded tree width and Robertson and Seymour's so called Weak Structure Theorem.",414-431,10.1137/1.9781611973105.30,http://dunn.com/
c6309b2aa8706f69b70830ef18147c79be36ba50,Complex brain networks: graph theoretical analysis of structural and functional systems,,186-198,10.1038/nrn2575,https://www.wilson-young.com/home/
a23407b19100acba66fcfc2803d251a3c829e9e3,Applying Graph theory to the Internet of Things,"In the Internet of Things (IoT), we all are ``things''. Graph theory, a branch of discrete mathematics, has been proven to be useful and powerful in understanding complex networks in history. By means of graph theory, we define new concepts and terminology, and explore the definition of IoT, and then show that IoT is the union of a topological network, a data-functional network and a domi-functional network.",2354-2361,10.1109/HPCC.and.EUC.2013.339,http://gardner-hogan.org/post/
bfacd964ba59bcc6b87b251fa3b30df90736c315,Fuzzy Graph Theory: A Survey,"A fuzzy graph (f-graph) is a pair G : ( σ, �) where σ is a fuzzy subset of a set S andis a fuzzy relation on σ. A fuzzy graph H : ( τ, υ) is called a partial fuzzy subgraph of G : ( σ, �) if τ (u) ≤ σ(u) for every u and υ (u, v) ≤ �(u, v) for every u and v . In particular we call a partial fuzzy subgraph H : ( τ, υ) a fuzzy subgraph of G : ( σ, � ) if τ (u) = σ(u) for every u in τ * and υ (u, v) = �(u, v) for every arc (u, v) in υ*. A connected f-graph G : ( σ, �) is a fuzzy tree(f-tree) if it has a fuzzy spannin g subgraph F : (σ, υ), which is a tree, where for all arcs (x, y) not i n F there exists a path from x to y in F whose strength is more than �(x, y). A path P of length n is a sequence of disti nct nodes u0, u 1, ..., u n such that �(u i1 , u i) > 0, i = 1, 2, ..., n and the degree of membershi p of a weakest arc is defined as its strength. If u 0 = u n and n ≥ 3, then P is called a cycle and a cycle P is called a fuzzy cycle(f-cycle) if it cont ains more than one weakest arc . The strength of connectedness between two nodes x and y is defined as the maximum of the strengths of all paths between x and y and is denot ed by CONN G(x, y). An x y path P is called a strongest x y",66-112,660-61269-8,http://flowers-richmond.com/privacy.htm
9a5a7a1b13f51af9369ef4ddeb75bb8b5ff87e70,Graph Theory: An Introductory Course,"From the reviews: ""Bla Bollob's introductory course on graph theory deserves to be considered as a watershed in the development of this theory as a serious academic subject...The book has chapters on electrical networks, flows, connectivity and matchings, extremal problems, colouring, Ramsey theory, random graphs, and graphs and groups. Each chapter starts at a measured and gentle pace. Classical results are proved and new insight is provided, with the examples at the end of each chapter fully supplementing the text...Even so this allows an introduction not only to some of the deeper results but, more vitally, provides outlines of, and firm insights into, their proofs. Thus in an elementary text book, we gain an overall understanding of well-known standard results, and yet at the same time constant hints of, and guidelines into, the higher levels of the subject. It is this aspect of the book which should guarantee it a permanent place in the literature.""",217,10.2307/3615150,http://bailey.com/author/
ab9accbaf61438bf518de0af0946f4bf12b7e158,"Network meta‐analysis, electrical networks and graph theory","Network meta‐analysis is an active field of research in clinical biostatistics. It aims to combine information from all randomized comparisons among a set of treatments for a given medical condition. We show how graph‐theoretical methods can be applied to network meta‐analysis. A meta‐analytic graph consists of vertices (treatments) and edges (randomized comparisons). We illustrate the correspondence between meta‐analytic networks and electrical networks, where variance corresponds to resistance, treatment effects to voltage, and weighted treatment effects to current flows. Based thereon, we then show that graph‐theoretical methods that have been routinely applied to electrical networks also work well in network meta‐analysis. In more detail, the resulting consistent treatment effects induced in the edges can be estimated via the Moore–Penrose pseudoinverse of the Laplacian matrix. Moreover, the variances of the treatment effects are estimated in analogy to electrical effective resistances. It is shown that this method, being computationally simple, leads to the usual fixed effect model estimate when applied to pairwise meta‐analysis and is consistent with published results when applied to network meta‐analysis examples from the literature. Moreover, problems of heterogeneity and inconsistency, random effects modeling and including multi‐armed trials are addressed. Copyright © 2012 John Wiley & Sons, Ltd.",87-119,10.1002/jrsm.1058,http://terrell-russell.com/
fbe0aeb6d1aaca06c84c985ce63b7c061569dd99,Comparing Brain Networks of Different Size and Connectivity Density Using Graph Theory,"Graph theory is a valuable framework to study the organization of functional and anatomical connections in the brain. Its use for comparing network topologies, however, is not without difficulties. Graph measures may be influenced by the number of nodes (N) and the average degree (k) of the network. The explicit form of that influence depends on the type of network topology, which is usually unknown for experimental data. Direct comparisons of graph measures between empirical networks with different N and/or k can therefore yield spurious results. We list benefits and pitfalls of various approaches that intend to overcome these difficulties. We discuss the initial graph definition of unweighted graphs via fixed thresholds, average degrees or edge densities, and the use of weighted graphs. For instance, choosing a threshold to fix N and k does eliminate size and density effects but may lead to modifications of the network by enforcing (ignoring) non-significant (significant) connections. Opposed to fixing N and k, graph measures are often normalized via random surrogates but, in fact, this may even increase the sensitivity to differences in N and k for the commonly used clustering coefficient and small-world index. To avoid such a bias we tried to estimate the N,k-dependence for empirical networks, which can serve to correct for size effects, if successful. We also add a number of methods used in social sciences that build on statistics of local network structures including exponential random graph models and motif counting. We show that none of the here-investigated methods allows for a reliable and fully unbiased comparison, but some perform better than others.",54-142,10.1371/journal.pone.0013701,https://lopez-johnson.com/
29af614088e9a2b387c2dc81539cf8420483c28a,A First Course in Graph Theory,"Because of its inherent simplicity, graph theory has a wide range of applications in engineering, and in physical sciences. It has of course uses in social sciences, in linguistics and in numerous other areas. In fact, a graph can be used to represent almost any physical situation involving discrete objects and the relationship among them. Now with the solutions to engineering and other problems becoming so complex leading to larger graphs, it is virtually difficult to analyze without the use of computers. This book is recommended in IIT Kharagpur, West Bengal for B.Tech Computer Science, NIT Arunachal Pradesh, NIT Nagaland, NIT Agartala, NIT Silchar, Gauhati University, Dibrugarh University, North Eastern Regional Institute of Management, Assam Engineering College, West Bengal Univerity of Technology (WBUT) for B.Tech, M.Tech Computer Science, University of Burdwan, West Bengal for B.Tech. Computer Science, Jadavpur University, West Bengal for M.Sc. Computer Science, Kalyani College of Engineering, West Bengal for B.Tech. Computer Science. Key Features: This book provides a rigorous yet informal treatment of graph theory with an emphasis on computational aspects of graph theory and graph-theoretic algorithms. Numerous applications to actual engineering problems are incorpo-rated with software design and optimization topics.",90-131,12-957158-5,http://fry.info/search/
5b18ce706c138393a127658d0a062d24f5f145bb,"Exponential Random Graph Models for Social Networks: Theory, Methods, and Applications","result of the social structure of American society.’’ This is true of the particular form or version of organized crime which he describes— it did not emanate from a transplanted Sicilian Mafia. That explanation, however, does not account for the many faces of organized crime in Australia, China, Russia, Japan, and many other places. Nor does it account for the growing phenomenon of transnational organized crime. Apart from any points of disagreement, this is a serious work of scholarship on a subject that has too often gotten short shrift in that respect. Organized Crime in Chicago is a good addition to the organized crime literature.",552 - 553,10.1177/0094306114539455gg,http://www.sims.com/
5656c718164f6ab8950f13d78ccf5d3b9942df7f,Test-Retest Reliability of Graph Theory Measures of Structural Brain Connectivity,,"
          305-12
        ",10.1007/978-3-642-33454-2_38,http://www.johnson-holland.org/home/
e4715a13f6364b1c81e64f247651c3d9e80b6808,Link Prediction Based on Graph Neural Networks,"Link prediction is a key problem for network-structured data. Link prediction heuristics use some score functions, such as common neighbors and Katz index, to measure the likelihood of links. They have obtained wide practical uses due to their simplicity, interpretability, and for some of them, scalability. However, every heuristic has a strong assumption on when two nodes are likely to link, which limits their effectiveness on networks where these assumptions fail. In this regard, a more reasonable way should be learning a suitable heuristic from a given network instead of using predefined ones. By extracting a local subgraph around each target link, we aim to learn a function mapping the subgraph patterns to link existence, thus automatically learning a `heuristic' that suits the current network. In this paper, we study this heuristic learning paradigm for link prediction. First, we develop a novel $\gamma$-decaying heuristic theory. The theory unifies a wide range of heuristics in a single framework, and proves that all these heuristics can be well approximated from local subgraphs. Our results show that local subgraphs reserve rich information related to link existence. Second, based on the $\gamma$-decaying theory, we propose a new algorithm to learn heuristics from local subgraphs using a graph neural network (GNN). Its experimental results show unprecedented performance, working consistently well on a wide range of problems.",54-136,544-05776-2,https://www.jennings-stanley.com/categories/wp-content/categories/terms/
cfa6e9e2331fca45f5c948a3477164fd14c956c9,"Graph Theory, 4th Edition",,"I-XVIII, 1-436",422-32068-X,https://www.walker.com/blog/category.php
446cd7da78f629cfdcef88b91ffbd24022a0b67b,Graph Neural Networks Exponentially Lose Expressive Power for Node Classification,"Graph Neural Networks (graph NNs) are a promising deep learning approach for analyzing graph-structured data. However, it is known that they do not improve (or sometimes worsen) their predictive performance as we pile up many layers and add non-lineality. To tackle this problem, we investigate the expressive power of graph NNs via their asymptotic behaviors as the layer size tends to infinity. Our strategy is to generalize the forward propagation of a Graph Convolutional Network (GCN), which is a popular graph NN variant, as a specific dynamical system. In the case of a GCN, we show that when its weights satisfy the conditions determined by the spectra of the (augmented) normalized Laplacian, its output exponentially approaches the set of signals that carry information of the connected components and node degrees only for distinguishing nodes. Our theory enables us to relate the expressive power of GCNs with the topological information of the underlying graphs inherent in the graph spectra. To demonstrate this, we characterize the asymptotic behavior of GCNs on the Erdős -- Renyi graph. We show that when the Erdős -- Renyi graph is sufficiently dense and large, a broad range of GCNs on it suffers from the ""information loss"" in the limit of infinite layers with high probability. Based on the theory, we provide a principled guideline for weight normalization of graph NNs. We experimentally confirm that the proposed weight scaling enhances the predictive performance of GCNs in real data. Code is available at this https URL.",21-150,04-526227-0,https://www.smith-weaver.com/categories/main/faq/
fc472f5017c6200e79b7540be07bc6b4798b4ef8,A review of evolutionary graph theory with applications to game theory,,"
          66-80
        ",10.1016/j.biosystems.2011.09.006,http://knight-arnold.com/index/
d089f5e5d0548df6066ff281f3918ed67ae742a0,Recent developments in graph Ramsey theory,"Given a graph $H$, the Ramsey number $r(H)$ is the smallest natural number $N$ such that any two-colouring of the edges of $K_N$ contains a monochromatic copy of $H$. The existence of these numbers has been known since 1930 but their quantitative behaviour is still not well understood. Even so, there has been a great deal of recent progress on the study of Ramsey numbers and their variants, spurred on by the many advances across extremal combinatorics. In this survey, we will describe some of this progress.",49-118,10.1017/CBO9781316106853.003,https://green.com/app/search/home/
a51a630f1088baff85282993ebf14fb910ccf126,Using graph theory to analyze biological networks,,10-Oct,10.1186/1756-0381-4-10,https://www.neal-gilbert.com/search/tags/about.asp
f4080a231a943b848d4d8e6c597c647caf4390c1,Spectral Graph Theory,,53-109,10.1201/b11644-19,http://garcia-martinez.org/
4d89d0da3bb1c8bc2ef36c7641abd41149def20b,Graph theory and molecular orbitals. Total φ-electron energy of alternant hydrocarbons,,535-538,10.1016/0009-2614(72)85099-1,http://www.potter.info/category/category/faq/
8df5d1e909de14932e42b347adf35070f60dc9ba,"An $L^p$ theory of sparse graph convergence I: Limits, sparse random graph models, and power law distributions","We introduce and develop a theory of limits for sequences of sparse graphs based on $L^p$ graphons, which generalizes both the existing $L^\infty$ theory of dense graph limits and its extension by Bollob\'as and Riordan to sparse graphs without dense spots. In doing so, we replace the no dense spots hypothesis with weaker assumptions, which allow us to analyze graphs with power law degree distributions. This gives the first broadly applicable limit theory for sparse graphs with unbounded average degrees. In this paper, we lay the foundations of the $L^p$ theory of graphons, characterize convergence, and develop corresponding random graph models, while we prove the equivalence of several alternative metrics in a companion paper.",18-146,10.1090/tran/7543,https://www.brooks.com/
74b6089b46e3e12f74da7d5d981d787aa7fb8459,Active semi-supervised learning using sampling theory for graph signals,"We consider the problem of offline, pool-based active semi-supervised learning on graphs. This problem is important when the labeled data is scarce and expensive whereas unlabeled data is easily available. The data points are represented by the vertices of an undirected graph with the similarity between them captured by the edge weights. Given a target number of nodes to label, the goal is to choose those nodes that are most informative and then predict the unknown labels. We propose a novel framework for this problem based on our recent results on sampling theory for graph signals. A graph signal is a real-valued function defined on each node of the graph. A notion of frequency for such signals can be defined using the spectrum of the graph Laplacian matrix. The sampling theory for graph signals aims to extend the traditional Nyquist-Shannon sampling theory by allowing us to identify the class of graph signals that can be reconstructed from their values on a subset of vertices. This approach allows us to define a criterion for active learning based on sampling set selection which aims at maximizing the frequency of the signals that can be reconstructed from their samples on the set. Experiments show the effectiveness of our method.",47-115,10.1145/2623330.2623760,https://www.brewer.com/category/
36e3cf186ebc510800fb6e217cc8edae93fbf4f9,"An $L^{p}$ theory of sparse graph convergence II: LD convergence, quotients and right convergence","We extend the LpLp theory of sparse graph limits, which was introduced in a companion paper, by analyzing different notions of convergence. Under suitable restrictions on node weights, we prove the equivalence of metric convergence, quotient convergence, microcanonical ground state energy convergence, microcanonical free energy convergence and large deviation convergence. Our theorems extend the broad applicability of dense graph convergence to all sparse graphs with unbounded average degree, while the proofs require new techniques based on uniform upper regularity. Examples to which our theory applies include stochastic block models, power law graphs and sparse versions of WW-random graphs.",337-396,10.1214/17-AOP1187,http://hernandez.com/main/main/index.php
d133cb102ad0f81e3fd17a7db090b28afc124c4a,Crystal Graph Convolutional Neural Networks for an Accurate and Interpretable Prediction of Material Properties.,"The use of machine learning methods for accelerating the design of crystalline materials usually requires manually constructed feature vectors or complex transformation of atom coordinates to input the crystal structure, which either constrains the model to certain crystal types or makes it difficult to provide chemical insights. Here, we develop a crystal graph convolutional neural networks framework to directly learn material properties from the connection of atoms in the crystal, providing a universal and interpretable representation of crystalline materials. Our method provides a highly accurate prediction of density functional theory calculated properties for eight different properties of crystals with various structure types and compositions after being trained with 10^{4} data points. Further, our framework is interpretable because one can extract the contributions from local chemical environments to global properties. Using an example of perovskites, we show how this information can be utilized to discover empirical rules for materials design.","
          145301
        ",10.1103/PhysRevLett.120.145301,http://www.copeland-hayes.com/main.html
39afec82e60d44f003cbd15e85d892b6fc5016e9,"Algorithms, Graph Theory, and Linear Equations in Laplacian Matrices","The Laplacian matrices of graphs are fundamental. In addition to facilitating the application of linear algebra to graph theory, they arise in many practical problems. In this talk we survey recent progress on the design of provably fast algorithms for solving linear equations in the Laplacian matrices of graphs. These algorithms motivate and rely upon fascinating primitives in graph theory, including low-stretch spanning trees, graph sparsifiers, ultra-sparsifiers, and local graph clustering. These are all connected by a definition of what it means for one graph to approximate another. While this definition is dictated by Numerical Linear Algebra, it proves useful and natural from a graph theoretic perspective. Mathematics Subject Classification (2010). Primary 68Q25; Secondary 65F08.",2698-2722,10.1142/9789814324359_0164,https://martinez.com/tags/blog/category/login/
f2a34394994089ee965dd9fa7a4bbc2446432770,Applications of Graph Theory and Network Science to Transit Network Design,"Abstract While the network nature of public transportation systems is well known, the study of their design from a topological/geometric perspective remains relatively limited. From the work of Euler in the 1750s to the discovery of scale-free networks in the late 1990s, the goal of this paper is to review the topical literature that applied concepts of graph theory and network science. After briefly introducing the origins of graph theory, we review early indicators developed to study transport networks, which notably includes the works of Garrison and Marble, and Kansky. Afterwards, we examine network indicators and characteristics developed to study transit systems specifically, in particular by reviewing the works of Vuchic and Musso. Subsequently, we introduce the concepts of small-worlds and scale-free networks from the emerging field network science, and review early applications to transit networks. Finally, we identify three challenges that will need to be addressed in the future. As transit systems are likely to grow in the world, the study of their network feature could be of substantial help to planners so as to better design the transit systems of tomorrow, but much work lies ahead.",495 - 519,10.1080/01441647.2010.543709,http://www.thomas-smith.com/
2ced07a295025f46ae51efa0a61679815c0866c5,Some new results in extremal graph theory,"In recent years several classical results in extremal graph theory have been improved in a uniform way and their proofs have been simplified and streamlined. These results include a new Erd\H{o}s-Stone-Bollob\'as theorem, several stability theorems, several saturation results and bounds for the number of graphs with large forbidden subgraphs. Another recent trend is the expansion of spectral extremal graph theory, in which extremal properties of graphs are studied by means of eigenvalues of various matrices. One particular achievement in this area is the casting of the central results above in spectral terms, often with additional enhancement. In addition, new, specific spectral results were found that have no conventional analogs. All of the above material is scattered throughout various journals, and since it may be of some interest, the purpose of this survey is to present the best of these results in a uniform, structured setting, together with some discussions of the underpinning ideas.",48-104,10.1017/cbo9781139004114.005,http://morrison.com/
8bd4a8eb93ede5c55c09bed842be671c8cb3413f,Robust automatic segmentation of corneal layer boundaries in SDOCT images using graph theory and dynamic programming,"Segmentation of anatomical structures in corneal images is crucial for the diagnosis and study of anterior segment diseases. However, manual segmentation is a time-consuming and subjective process. This paper presents an automatic approach for segmenting corneal layer boundaries in Spectral Domain Optical Coherence Tomography images using graph theory and dynamic programming. Our approach is robust to the low-SNR and different artifact types that can appear in clinical corneal images. We show that our method segments three corneal layer boundaries in normal adult eyes more accurately compared to an expert grader than a second grader—even in the presence of significant imaging outliers.",1524 - 1538,10.1364/BOE.2.001524,https://morgan-gordon.com/author/
cc2c33aea50ef4e2aec9413f83ccd7d35dace3c6,Applications of Graph Theory in Computer Science,"Graphs are among the most ubiquitous models of both natural and human-made structures. They can be used to model many types of relations and process dynamics in computer science, physical, biological and social systems. Many problems of practical interest can be represented by graphs. In general graphs theory has a wide range of applications in diverse fields. This paper explores different elements involved in graph theory including graph representations using computer systems and graph-theoretic data structures such as list structure and matrix structure. The emphasis of this paper is on graph applications in computer science. To demonstrate the importance of graph theory in computer science, this article addresses most common applications for graph theory in computer science. These applications are presented especially to project the idea of graph theory and to demonstrate its importance in computer science.",142-145,10.1109/CICSyN.2011.40,http://www.ray-arnold.com/categories/blog/blog/homepage.htm
1a6b1bac33da9cdff5fc311e9b9e4bd065bd7a9f,Systemic Risk in Energy Derivative Markets: A Graph-Theory Analysis,"This article uses graph theory to provide novel evidence regarding market integration, a favorable condition for systemic risk to appear in. Relying on daily futures returns covering a 12-year period, we examine cross- and inter-market linkages, both within the commodity complex and between commodities and other financial assets. In such a high dimensional analysis, graph theory enables us to understand the dynamic behavior of our price system. We show that energy markets - as a whole - stand at the heart of this system. We also establish that crude oil is itself at the center of the energy complex. Further, we provide evidence that commodity markets have become more integrated over time.",27-135,10.2139/ssrn.2025641,http://www.barnes.com/login.html
cc2101c1368c528a7def9bf9ae711d3725933ac8,Local Higher-Order Graph Clustering,"Local graph clustering methods aim to find a cluster of nodes by exploring a small region of the graph. These methods are attractive because they enable targeted clustering around a given seed node and are faster than traditional global graph clustering methods because their runtime does not depend on the size of the input graph. However, current local graph partitioning methods are not designed to account for the higher-order structures crucial to the network, nor can they effectively handle directed networks. Here we introduce a new class of local graph clustering methods that address these issues by incorporating higher-order network information captured by small subgraphs, also called network motifs. We develop the Motif-based Approximate Personalized PageRank (MAPPR) algorithm that finds clusters containing a seed node with minimal \emph{motif conductance}, a generalization of the conductance metric for network motifs. We generalize existing theory to prove the fast running time (independent of the size of the graph) and obtain theoretical guarantees on the cluster quality (in terms of motif conductance). We also develop a theory of node neighborhoods for finding sets that have small motif conductance, and apply these results to the case of finding good seed nodes to use as input to the MAPPR algorithm. Experimental validation on community detection tasks in both synthetic and real-world networks, shows that our new framework MAPPR outperforms the current edge-based personalized PageRank methodology.",68-132,10.1145/3097983.3098069,https://www.terry-payne.biz/search/
a44b5f0314a12aca034d52cf047445cdf7c884bf,"Graph Theory, Combinatorics and Algorithms: Interdisciplinary Applications","Graph Theory, Combinatorics and Algorithms: Interdisciplinary Applications focuses on discrete mathematics and combinatorial algorithms interacting with real world problems in computer science, operations research, applied mathematics and engineering.The book containseleven chapters written by experts in their respective fields, and covers a wide spectrum of high-interest problems across these discipline domains. Among the contributing authors are Richard Karp of UC Berkeley and Robert Tarjan of Princeton; both are at the pinnacle of research scholarship in Graph Theory and Combinatorics. The chapters from the contributing authors focus on ""real world"" applications, all of which will be of considerable interest across the areas of Operations Research, Computer Science, Applied Mathematics, and Engineering. These problems include Internet congestion control, high-speed communication networks, multi-object auctions, resource allocation, software testing, data structures, etc. In sum, this is a book focused on major, contemporary problems, written by the top research scholars in the field, using cutting-edge mathematical and computational techniques.",36-118,355-62852-7,http://www.wilson.com/register/
69381b5efd97e7c55f51c2730caccab3d632d4d2,Graph Embedding and Extensions: A General Framework for Dimensionality Reduction,"A large family of algorithms - supervised or unsupervised; stemming from statistics or geometry theory - has been designed to provide different solutions to the problem of dimensionality reduction. Despite the different motivations of these algorithms, we present in this paper a general formulation known as graph embedding to unify them within a common framework. In graph embedding, each algorithm can be considered as the direct graph embedding or its linear/kernel/tensor extension of a specific intrinsic graph that describes certain desired statistical or geometric properties of a data set, with constraints from scale normalization or a penalty graph that characterizes a statistical or geometric property that should be avoided. Furthermore, the graph embedding framework can be used as a general platform for developing new dimensionality reduction algorithms. By utilizing this framework as a tool, we propose a new supervised dimensionality reduction algorithm called marginal Fisher analysis in which the intrinsic graph characterizes the intraclass compactness and connects each data point with its neighboring points of the same class, while the penalty graph connects the marginal points and characterizes the interclass separability. We show that MFA effectively overcomes the limitations of the traditional linear discriminant analysis algorithm due to data distribution assumptions and available projection directions. Real face recognition experiments show the superiority of our proposed MFA in comparison to LDA, also for corresponding kernel and tensor extensions",40-51,10.1109/TPAMI.2007.12,https://singh.net/login.html
287650fad5a478581f0040ad064e6856c7c13af2,Topological Graph Theory,Introduction Voltage Graphs and Covering Spaces Surfaces and Graph Imbeddings Imbedded Voltage Graphs and Current Graphs Map Colorings The Genus of A Group References.,610-786,10.1201/9780203490204.ch7,http://www.smith.org/main/category/faq/
1f400c9732c13eae95be34a181c2b8042a774a39,Handbook of graph theory,"Introduction to Graphs Fundamentals of Graph Theory, Jonathan L. Gross and Jay Yellen Families of Graphs and Digraphs, Lowell W. Beineke History of Graph Theory, Robin J. Wilson Graph Representation Computer Representation of Graphs, Alfred V. Aho Graph Isomorphism, Brendan D. McKay The Reconstruction Problem, Josef Lauri Recursively Constructed Graphs, Richard B. Borie, R. Gary Parker, and Craig A. Tovey Structural Graph Theory, Maria Chudnovsky Directed Graphs Basic Digraph Models and Properties, Jay Yellen Directed Acyclic Graphs, Stephen B. Maurer Tournaments, K.B. Reid Connectivity and Traversability Connectivity Properties and Structure, Camino Balbuena, Josep Fabrega, and Miguel Angel Fiol Eulerian Graphs, Herbert Fleischner Chinese Postman Problems, R. Gary Parker and Richard B. Borie DeBruijn Graphs and Sequences, A.K. Dewdney Hamiltonian Graphs, Ronald J. Gould Traveling Salesman Problems, Gregory Gutin Further Topics in Connectivity, Josep Fabrega and Miguel Angel Fiol Colorings and Related Topics Graph Coloring, Zsolt Tuza Further Topics in Graph Coloring, Zsolt Tuza Independence and Cliques, Gregory Gutin Factors and Factorization, Michael Plummer Applications to Timetabling, Edmund Burke, Dominique de Werra, and Jeffrey Kingston Graceful Labelings, Joseph A. Gallian Algebraic Graph Theory Automorphisms, Mark E. Watkins Cayley Graphs, Brian Alspach Enumeration, Paul K. Stockmeyer Graphs and Vector Spaces, Krishnaiyan ""KT"" Thulasiraman Spectral Graph Theory, Michael Doob Matroidal Methods in Graph Theory, James Oxley Topological Graph Theory Graphs on Surfaces, Tomaz Pisanski and Primoz Potocnik Minimum Genus and Maximum Genus, Jianer Chen Genus Distributions, Jonathan L. Gross Voltage Graphs, Jonathan L. Gross The Genus of a Group, Thomas W. Tucker Maps, Roman Nedela and Martin Skoviera Representativity, Dan Archdeacon Triangulations, Seiya Negami Graphs and Finite Geometries, Arthur T. White Crossing Numbers, R. Bruce Richter and Gelasio Salazar Analytic Graph Theory Extremal Graph Theory, Bela Bollobas and Vladimir Nikiforov Random Graphs, Nicholas Wormald Ramsey Graph Theory, Ralph J. Faudree The Probabilistic Method, Alan Frieze and Po-Shen Loh Graph Limits, Bojan Mohar Graphical Measurement Distance in Graphs, Gary Chartrand and Ping Zhang Domination in Graphs, Teresa W. Haynes and Michael A. Henning Tolerance Graphs, Martin Charles Golumbic Bandwidth, Robert C. Brigham Pursuit-Evasion Problems, Richard B. Borie, Sven Koenig, and Craig A. Tovey Graphs in Computer Science Searching, Harold N. Gabow Dynamic Graph Algorithms, Camil Demetrescu, Irene Finocchi, and Giuseppe F. Italiano Drawings of Graphs, Emilio Di Giacomo, Giuseppe Liotta, and Roberto Tamassia Algorithms on Recursively Constructed Graphs, Richard B. Borie, R. Gary Parker, and Craig A. Tovey Fuzzy Graphs, John N. Mordeson and D.S. Malik Expander Graphs, Mike Krebs and Anthony Shaheen Visibility Graphs, Alice M. Dean and Joan P. Hutchinson Networks and Flows Maximum Flows, Clifford Stein Minimum Cost Flows, Lisa Fleischer Matchings and Assignments, Jay Sethuraman and Douglas R. Shier Communication Networks Complex Networks, Anthony Bonato and Fan Chung Broadcasting and Gossiping, Hovhannes A. Harutyunyan, Arthur L. Liestman, Joseph G. Peters, and Dana Richards Communication Network Design Models, Prakash Mirchandani and David Simchi-Levi Network Science for Graph Theorists, David C. Arney and Steven B. Horton Natural Science and Processes Chemical Graph Theory, Ernesto Estrada and Danail Bonchev Ties between Graph Theory and Biology, Jacek Blazewicz, Marta Kasprzak, and Nikos Vlassis Index A Glossary appears at the end of each chapter.",1-1167,10.1201/9780203490204,https://ellis.com/main/privacy/
ef2704b8e44692c7f6430451434af64df913d575,Introduction to Graph and Hypergraph Theory,Preface Basic Definitions and Concepts Trees and Bipartite Graphs Chordal Graphs Planar Graphs Graph Coloring Traversals and Flows Basic Hypergraph Concepts Hypertrees and Chordal Hypergraphs Some Other Remarkable Hypergraph Classes Hypergraph Coloring Modeling with Hypergraphs Appendix Index.,93-145,9564977-8-0,http://www.rich.com/wp-content/list/post.htm
a59b5da1de93e35bbb26d335ca2247ea710a9315,A graph‐theory algorithm for rapid protein side‐chain prediction,"Fast and accurate side‐chain conformation prediction is important for homology modeling, ab initio protein structure prediction, and protein design applications. Many methods have been presented, although only a few computer programs are publicly available. The SCWRL program is one such method and is widely used because of its speed, accuracy, and ease of use. A new algorithm for SCWRL is presented that uses results from graph theory to solve the combinatorial problem encountered in the side‐chain prediction problem. In this method, side chains are represented as vertices in an undirected graph. Any two residues that have rotamers with nonzero interaction energies are considered to have an edge in the graph. The resulting graph can be partitioned into connected subgraphs with no edges between them. These subgraphs can in turn be broken into biconnected components, which are graphs that cannot be disconnected by removal of a single vertex. The combinatorial problem is reduced to finding the minimum energy of these small biconnected components and combining the results to identify the global minimum energy conformation. This algorithm is able to complete predictions on a set of 180 proteins with 34,342 side chains in <7 min of computer time. The total χ1 and χ1 + 2 dihedral angle accuracies are 82.6% and 73.7% using a simple energy function based on the backbone‐dependent rotamer library and a linear repulsive steric energy. The new algorithm will allow for use of SCWRL in more demanding applications such as sequence design and ab initio structure prediction, as well addition of a more complex energy function and conformational flexibility, leading to increased accuracy.",68-123,10.1110/ps.03154503,https://smith-davis.org/
7dbdb4209626fd92d2436a058663206216036e68,Elements of Information Theory,"Preface to the Second Edition. Preface to the First Edition. Acknowledgments for the Second Edition. Acknowledgments for the First Edition. 1. Introduction and Preview. 1.1 Preview of the Book. 2. Entropy, Relative Entropy, and Mutual Information. 2.1 Entropy. 2.2 Joint Entropy and Conditional Entropy. 2.3 Relative Entropy and Mutual Information. 2.4 Relationship Between Entropy and Mutual Information. 2.5 Chain Rules for Entropy, Relative Entropy, and Mutual Information. 2.6 Jensen's Inequality and Its Consequences. 2.7 Log Sum Inequality and Its Applications. 2.8 Data-Processing Inequality. 2.9 Sufficient Statistics. 2.10 Fano's Inequality. Summary. Problems. Historical Notes. 3. Asymptotic Equipartition Property. 3.1 Asymptotic Equipartition Property Theorem. 3.2 Consequences of the AEP: Data Compression. 3.3 High-Probability Sets and the Typical Set. Summary. Problems. Historical Notes. 4. Entropy Rates of a Stochastic Process. 4.1 Markov Chains. 4.2 Entropy Rate. 4.3 Example: Entropy Rate of a Random Walk on a Weighted Graph. 4.4 Second Law of Thermodynamics. 4.5 Functions of Markov Chains. Summary. Problems. Historical Notes. 5. Data Compression. 5.1 Examples of Codes. 5.2 Kraft Inequality. 5.3 Optimal Codes. 5.4 Bounds on the Optimal Code Length. 5.5 Kraft Inequality for Uniquely Decodable Codes. 5.6 Huffman Codes. 5.7 Some Comments on Huffman Codes. 5.8 Optimality of Huffman Codes. 5.9 Shannon-Fano-Elias Coding. 5.10 Competitive Optimality of the Shannon Code. 5.11 Generation of Discrete Distributions from Fair Coins. Summary. Problems. Historical Notes. 6. Gambling and Data Compression. 6.1 The Horse Race. 6.2 Gambling and Side Information. 6.3 Dependent Horse Races and Entropy Rate. 6.4 The Entropy of English. 6.5 Data Compression and Gambling. 6.6 Gambling Estimate of the Entropy of English. Summary. Problems. Historical Notes. 7. Channel Capacity. 7.1 Examples of Channel Capacity. 7.2 Symmetric Channels. 7.3 Properties of Channel Capacity. 7.4 Preview of the Channel Coding Theorem. 7.5 Definitions. 7.6 Jointly Typical Sequences. 7.7 Channel Coding Theorem. 7.8 Zero-Error Codes. 7.9 Fano's Inequality and the Converse to the Coding Theorem. 7.10 Equality in the Converse to the Channel Coding Theorem. 7.11 Hamming Codes. 7.12 Feedback Capacity. 7.13 Source-Channel Separation Theorem. Summary. Problems. Historical Notes. 8. Differential Entropy. 8.1 Definitions. 8.2 AEP for Continuous Random Variables. 8.3 Relation of Differential Entropy to Discrete Entropy. 8.4 Joint and Conditional Differential Entropy. 8.5 Relative Entropy and Mutual Information. 8.6 Properties of Differential Entropy, Relative Entropy, and Mutual Information. Summary. Problems. Historical Notes. 9. Gaussian Channel. 9.1 Gaussian Channel: Definitions. 9.2 Converse to the Coding Theorem for Gaussian Channels. 9.3 Bandlimited Channels. 9.4 Parallel Gaussian Channels. 9.5 Channels with Colored Gaussian Noise. 9.6 Gaussian Channels with Feedback. Summary. Problems. Historical Notes. 10. Rate Distortion Theory. 10.1 Quantization. 10.2 Definitions. 10.3 Calculation of the Rate Distortion Function. 10.4 Converse to the Rate Distortion Theorem. 10.5 Achievability of the Rate Distortion Function. 10.6 Strongly Typical Sequences and Rate Distortion. 10.7 Characterization of the Rate Distortion Function. 10.8 Computation of Channel Capacity and the Rate Distortion Function. Summary. Problems. Historical Notes. 11. Information Theory and Statistics. 11.1 Method of Types. 11.2 Law of Large Numbers. 11.3 Universal Source Coding. 11.4 Large Deviation Theory. 11.5 Examples of Sanov's Theorem. 11.6 Conditional Limit Theorem. 11.7 Hypothesis Testing. 11.8 Chernoff-Stein Lemma. 11.9 Chernoff Information. 11.10 Fisher Information and the Cram-er-Rao Inequality. Summary. Problems. Historical Notes. 12. Maximum Entropy. 12.1 Maximum Entropy Distributions. 12.2 Examples. 12.3 Anomalous Maximum Entropy Problem. 12.4 Spectrum Estimation. 12.5 Entropy Rates of a Gaussian Process. 12.6 Burg's Maximum Entropy Theorem. Summary. Problems. Historical Notes. 13. Universal Source Coding. 13.1 Universal Codes and Channel Capacity. 13.2 Universal Coding for Binary Sequences. 13.3 Arithmetic Coding. 13.4 Lempel-Ziv Coding. 13.5 Optimality of Lempel-Ziv Algorithms. Compression. Summary. Problems. Historical Notes. 14. Kolmogorov Complexity. 14.1 Models of Computation. 14.2 Kolmogorov Complexity: Definitions and Examples. 14.3 Kolmogorov Complexity and Entropy. 14.4 Kolmogorov Complexity of Integers. 14.5 Algorithmically Random and Incompressible Sequences. 14.6 Universal Probability. 14.7 Kolmogorov complexity. 14.9 Universal Gambling. 14.10 Occam's Razor. 14.11 Kolmogorov Complexity and Universal Probability. 14.12 Kolmogorov Sufficient Statistic. 14.13 Minimum Description Length Principle. Summary. Problems. Historical Notes. 15. Network Information Theory. 15.1 Gaussian Multiple-User Channels. 15.2 Jointly Typical Sequences. 15.3 Multiple-Access Channel. 15.4 Encoding of Correlated Sources. 15.5 Duality Between Slepian-Wolf Encoding and Multiple-Access Channels. 15.6 Broadcast Channel. 15.7 Relay Channel. 15.8 Source Coding with Side Information. 15.9 Rate Distortion with Side Information. 15.10 General Multiterminal Networks. Summary. Problems. Historical Notes. 16. Information Theory and Portfolio Theory. 16.1 The Stock Market: Some Definitions. 16.2 Kuhn-Tucker Characterization of the Log-Optimal Portfolio. 16.3 Asymptotic Optimality of the Log-Optimal Portfolio. 16.4 Side Information and the Growth Rate. 16.5 Investment in Stationary Markets. 16.6 Competitive Optimality of the Log-Optimal Portfolio. 16.7 Universal Portfolios. 16.8 Shannon-McMillan-Breiman Theorem (General AEP). Summary. Problems. Historical Notes. 17. Inequalities in Information Theory. 17.1 Basic Inequalities of Information Theory. 17.2 Differential Entropy. 17.3 Bounds on Entropy and Relative Entropy. 17.4 Inequalities for Types. 17.5 Combinatorial Bounds on Entropy. 17.6 Entropy Rates of Subsets. 17.7 Entropy and Fisher Information. 17.8 Entropy Power Inequality and Brunn-Minkowski Inequality. 17.9 Inequalities for Determinants. 17.10 Inequalities for Ratios of Determinants. Summary. Problems. Historical Notes. Bibliography. List of Symbols. Index.",26-139,10.1002/0471200611,https://www.montes.info/
997a3ec13bbab8f2f208005740d7f89ad9831dfe,Graph Theory,"This is a replacement paper. Some of the conclusions from statistical graph theory are reiterated in the sixth section, but not fully resolved therein. Most of the redundancies in the later sections have been deleted. An important number-theoretical technique in the opening section relates to one of the main techniques in the last (sixth) section.",53-138,10.1887/0750306521/b803c10,https://walters.com/blog/search/main/
3f2e5b88c4b631c931f6db805f4ae3c0b531a8e8,APPLICATIONS OF GRAPH THEORY IN COMPUTER SCIENCE AN OVERVIEW,"The field of mathematics plays vital role in various fields. One of the important areas in mathematics is graph theory which is used in structural models. This structural arrangements of various objects or technologies lead to new inventions and modifications in the existing environment for enhancement in those fields. The field graph theory started its journey from the problem of Koinsberg bridge in 1735. This paper gives an overview of the applications of graph theory in heterogeneous fields to some extent but mainly focuses on the computer science applications that uses graph theoretical concepts. Various papers based on graph theory have been studied related to scheduling concepts, computer science applications and an overview has been presented here.",26-149,85127-208-8,https://tucker.info/author/
46c0b71ceea130bead6c9120cf6d32b2835db831,The Cambridge Structural Database,"This paper is the definitive article describing the creation, maintenance, information content and availability of the Cambridge Structural Database (CSD), the world’s repository of small molecule crystal structures.",171 - 179,10.1107/S2052520616003954,https://www.nguyen-weaver.org/search/search/
948fd800ecdd3c99488dde36b41480ca1b8acce3,The PRIDE database and related tools and resources in 2019: improving support for quantification data,"Abstract The PRoteomics IDEntifications (PRIDE) database (https://www.ebi.ac.uk/pride/) is the world’s largest data repository of mass spectrometry-based proteomics data, and is one of the founding members of the global ProteomeXchange (PX) consortium. In this manuscript, we summarize the developments in PRIDE resources and related tools since the previous update manuscript was published in Nucleic Acids Research in 2016. In the last 3 years, public data sharing through PRIDE (as part of PX) has definitely become the norm in the field. In parallel, data re-use of public proteomics data has increased enormously, with multiple applications. We first describe the new architecture of PRIDE Archive, the archival component of PRIDE. PRIDE Archive and the related data submission framework have been further developed to support the increase in submitted data volumes and additional data types. A new scalable and fault tolerant storage backend, Application Programming Interface and web interface have been implemented, as a part of an ongoing process. Additionally, we emphasize the improved support for quantitative proteomics data through the mzTab format. At last, we outline key statistics on the current data contents and volume of downloads, and how PRIDE data are starting to be disseminated to added-value resources including Ensembl, UniProt and Expression Atlas.",D442 - D450,10.1093/nar/gky1106,http://www.williamson.net/home.html
95cd83603a0d2b6918a8e34a5637a8f382da96f5,"MIMIC-III, a freely accessible critical care database",,61-117,10.1038/sdata.2016.35,http://walter.info/author/
da692ee969d9c33986196372c3f7cb87fa6b6f8f,Database resources of the National Center for Biotechnology Information,"Abstract The National Center for Biotechnology Information (NCBI) provides a large suite of online resources for biological information and data, including the GenBank® nucleic acid sequence database and the PubMed database of citations and abstracts for published life science journals. The Entrez system provides search and retrieval operations for most of these data from 39 distinct databases. The E-utilities serve as the programming interface for the Entrez system. Augmenting many of the Web applications are custom implementations of the BLAST program optimized to search specialized data sets. New resources released in the past year include PubMed Data Management, RefSeq Functional Elements, genome data download, variation services API, Magic-BLAST, QuickBLASTp, and Identical Protein Groups. Resources that were updated in the past year include the genome data viewer, a human genome resources page, Gene, virus variation, OSIRIS, and PubChem. All of these resources can be accessed through the NCBI home page at www.ncbi.nlm.nih.gov.",D8 - D13,10.1093/nar/gkx1095,https://robles.com/faq/
98128fd412ebfa90201a276f2c59020ccc696a75,DrugBank 5.0: a major update to the DrugBank database for 2018,"Abstract DrugBank (www.drugbank.ca) is a web-enabled database containing comprehensive molecular information about drugs, their mechanisms, their interactions and their targets. First described in 2006, DrugBank has continued to evolve over the past 12 years in response to marked improvements to web standards and changing needs for drug research and development. This year’s update, DrugBank 5.0, represents the most significant upgrade to the database in more than 10 years. In many cases, existing data content has grown by 100% or more over the last update. For instance, the total number of investigational drugs in the database has grown by almost 300%, the number of drug-drug interactions has grown by nearly 600% and the number of SNP-associated drug effects has grown more than 3000%. Significant improvements have been made to the quantity, quality and consistency of drug indications, drug binding data as well as drug-drug and drug-food interactions. A great deal of brand new data have also been added to DrugBank 5.0. This includes information on the influence of hundreds of drugs on metabolite levels (pharmacometabolomics), gene expression levels (pharmacotranscriptomics) and protein expression levels (pharmacoprotoemics). New data have also been added on the status of hundreds of new drug clinical trials and existing drug repurposing trials. Many other important improvements in the content, interface and performance of the DrugBank website have been made and these should greatly enhance its ease of use, utility and potential applications in many areas of pharmacological research, pharmaceutical science and drug education.",D1074 - D1082,10.1093/nar/gkx1037,http://www.parsons.com/list/categories/terms.htm
6e1e6afb314f9c5a24d744252a30aa5efc313571,"The STRING database in 2017: quality-controlled protein–protein association networks, made broadly accessible","A system-wide understanding of cellular function requires knowledge of all functional interactions between the expressed proteins. The STRING database aims to collect and integrate this information, by consolidating known and predicted protein–protein association data for a large number of organisms. The associations in STRING include direct (physical) interactions, as well as indirect (functional) interactions, as long as both are specific and biologically meaningful. Apart from collecting and reassessing available experimental data on protein–protein interactions, and importing known pathways and protein complexes from curated databases, interaction predictions are derived from the following sources: (i) systematic co-expression analysis, (ii) detection of shared selective signals across genomes, (iii) automated text-mining of the scientific literature and (iv) computational transfer of interaction knowledge between organisms based on gene orthology. In the latest version 10.5 of STRING, the biggest changes are concerned with data dissemination: the web frontend has been completely redesigned to reduce dependency on outdated browser technologies, and the database can now also be queried from inside the popular Cytoscape software framework. Further improvements include automated background analysis of user inputs for functional enrichments, and streamlined download options. The STRING resource is available online, at http://string-db.org/.",D362 - D368,10.1093/nar/gkw937,http://alexander.com/register/
51da1eab2d350b5aa0eeebf83fba7caae3a3bc29,Introducing EzBioCloud: a taxonomically united database of 16S rRNA gene sequences and whole-genome assemblies,"The recent advent of DNA sequencing technologies facilitates the use of genome sequencing data that provide means for more informative and precise classification and identification of members of the Bacteria and Archaea. Because the current species definition is based on the comparison of genome sequences between type and other strains in a given species, building a genome database with correct taxonomic information is of paramount need to enhance our efforts in exploring prokaryotic diversity and discovering novel species as well as for routine identifications. Here we introduce an integrated database, called EzBioCloud, that holds the taxonomic hierarchy of the Bacteria and Archaea, which is represented by quality-controlled 16S rRNA gene and genome sequences. Whole-genome assemblies in the NCBI Assembly Database were screened for low quality and subjected to a composite identification bioinformatics pipeline that employs gene-based searches followed by the calculation of average nucleotide identity. As a result, the database is made of 61 700 species/phylotypes, including 13 132 with validly published names, and 62 362 whole-genome assemblies that were identified taxonomically at the genus, species and subspecies levels. Genomic properties, such as genome size and DNA G+C content, and the occurrence in human microbiome data were calculated for each genus or higher taxa. This united database of taxonomy, 16S rRNA gene and genome sequences, with accompanying bioinformatics tools, should accelerate genome-based classification and identification of members of the Bacteria and Archaea. The database and related search tools are available at www.ezbiocloud.net/.",1613 - 1617,10.1099/ijsem.0.001755,https://yates-ryan.biz/login.php
0f5c63182b5d40850c741888a89e6c055a3593af,The Pfam protein families database: towards a more sustainable future,"In the last two years the Pfam database (http://pfam.xfam.org) has undergone a substantial reorganisation to reduce the effort involved in making a release, thereby permitting more frequent releases. Arguably the most significant of these changes is that Pfam is now primarily based on the UniProtKB reference proteomes, with the counts of matched sequences and species reported on the website restricted to this smaller set. Building families on reference proteomes sequences brings greater stability, which decreases the amount of manual curation required to maintain them. It also reduces the number of sequences displayed on the website, whilst still providing access to many important model organisms. Matches to the full UniProtKB database are, however, still available and Pfam annotations for individual UniProtKB sequences can still be retrieved. Some Pfam entries (1.6%) which have no matches to reference proteomes remain; we are working with UniProt to see if sequences from them can be incorporated into reference proteomes. Pfam-B, the automatically-generated supplement to Pfam, has been removed. The current release (Pfam 29.0) includes 16 295 entries and 559 clans. The facility to view the relationship between families within a clan has been improved by the introduction of a new tool.",D279 - D285,10.1093/nar/gkv1344,http://www.henry.com/
788b43b7c62b497cf69b31544c6f81c6f4856d42,Pfam: the protein families database,"Pfam, available via servers in the UK (http://pfam.sanger.ac.uk/) and the USA (http://pfam.janelia.org/), is a widely used database of protein families, containing 14 831 manually curated entries in the current release, version 27.0. Since the last update article 2 years ago, we have generated 1182 new families and maintained sequence coverage of the UniProt Knowledgebase (UniProtKB) at nearly 80%, despite a 50% increase in the size of the underlying sequence database. Since our 2012 article describing Pfam, we have also undertaken a comprehensive review of the features that are provided by Pfam over and above the basic family data. For each feature, we determined the relevance, computational burden, usage statistics and the functionality of the feature in a website context. As a consequence of this review, we have removed some features, enhanced others and developed new ones to meet the changing demands of computational biology. Here, we describe the changes to Pfam content. Notably, we now provide family alignments based on four different representative proteome sequence data sets and a new interactive DNA search interface. We also discuss the mapping between Pfam and known 3D structures.",D222 - D230,10.1093/nar/gkt1223,http://www.castro.net/faq/
b204970b0503a923359bff532726666f5e0e971b,The SILVA ribosomal RNA gene database project: improved data processing and web-based tools,"SILVA (from Latin silva, forest, http://www.arb-silva.de) is a comprehensive web resource for up to date, quality-controlled databases of aligned ribosomal RNA (rRNA) gene sequences from the Bacteria, Archaea and Eukaryota domains and supplementary online services. The referred database release 111 (July 2012) contains 3 194 778 small subunit and 288 717 large subunit rRNA gene sequences. Since the initial description of the project, substantial new features have been introduced, including advanced quality control procedures, an improved rRNA gene aligner, online tools for probe and primer evaluation and optimized browsing, searching and downloading on the website. Furthermore, the extensively curated SILVA taxonomy and the new non-redundant SILVA datasets provide an ideal reference for high-throughput classification of data from next-generation sequencing approaches.",D590 - D596,10.1093/nar/gks1219,https://smith.info/category/
93d5369a0be3134c6018373d5290923f3d718815,The Molecular Signatures Database Hallmark Gene Set Collection,,417-425,10.1016/J.CELS.2015.12.004,https://evans.com/home/
d2c733e34d48784a37d717fe43d9e93277a8c53e,ImageNet: A large-scale hierarchical image database,"The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.",248-255,10.1109/CVPR.2009.5206848,https://www.ruiz.net/explore/index.htm
02613d6e3ecf67ed9ae8ce67a35a92f3986bc4cf,Gapped BLAST and PSI-BLAST: a new generation of protein database search programs.,"The BLAST programs are widely used tools for searching protein and DNA databases for sequence similarities. For protein comparisons, a variety of definitional, algorithmic and statistical refinements described here permits the execution time of the BLAST programs to be decreased substantially while enhancing their sensitivity to weak similarities. A new criterion for triggering the extension of word hits, combined with a new heuristic for generating gapped alignments, yields a gapped BLAST program that runs at approximately three times the speed of the original. In addition, a method is introduced for automatically combining statistically significant alignments produced by BLAST into a position-specific score matrix, and searching the database using this matrix. The resulting Position-Specific Iterated BLAST (PSI-BLAST) program runs at approximately the same speed per iteration as gapped BLAST, but in many cases is much more sensitive to weak but biologically relevant sequence similarities. PSI-BLAST is used to uncover several new and interesting members of the BRCT superfamily.","
          3389-402
        ",10.1093/NAR/25.17.3389,https://gomez-fletcher.com/
0e5bccdedb82fbafece8ca71d64b16ff05ec9145,The carbohydrate-active enzymes database (CAZy) in 2013,"The Carbohydrate-Active Enzymes database (CAZy; http://www.cazy.org) provides online and continuously updated access to a sequence-based family classification linking the sequence to the specificity and 3D structure of the enzymes that assemble, modify and breakdown oligo- and polysaccharides. Functional and 3D structural information is added and curated on a regular basis based on the available literature. In addition to the use of the database by enzymologists seeking curated information on CAZymes, the dissemination of a stable nomenclature for these enzymes is probably a major contribution of CAZy. The past few years have seen the expansion of the CAZy classification scheme to new families, the development of subfamilies in several families and the power of CAZy for the analysis of genomes and metagenomes. This article outlines the changes that have occurred in CAZy during the past 5 years and presents our novel effort to display the resolution and the carbohydrate ligands in crystallographic complexes of CAZymes.",D490 - D495,10.1093/nar/gkt1178,https://www.newman.com/tags/author.htm
f986968735459e789890f24b6b277b0920a9725d,Places: A 10 Million Image Database for Scene Recognition,"The rise of multi-million-item dataset initiatives has enabled data-hungry machine learning algorithms to reach near-human semantic classification performance at tasks such as visual object and scene recognition. Here we describe the Places Database, a repository of 10 million scene photographs, labeled with scene semantic categories, comprising a large and diverse list of the types of environments encountered in the world. Using the state-of-the-art Convolutional Neural Networks (CNNs), we provide scene classification CNNs (Places-CNNs) as baselines, that significantly outperform the previous approaches. Visualization of the CNNs trained on Places shows that object detectors emerge as an intermediate representation of scene classification. With its high-coverage and high-diversity of exemplars, the Places Database along with the Places-CNNs offer a novel resource to guide future progress on scene recognition problems.",1452-1464,10.1109/TPAMI.2017.2723009,http://banks.com/search/tag/register.asp
7f19972754ac0c15329666b3a6efbf569b27d8d5,The Pfam protein families database in 2019,"Abstract The last few years have witnessed significant changes in Pfam (https://pfam.xfam.org). The number of families has grown substantially to a total of 17,929 in release 32.0. New additions have been coupled with efforts to improve existing families, including refinement of domain boundaries, their classification into Pfam clans, as well as their functional annotation. We recently began to collaborate with the RepeatsDB resource to improve the definition of tandem repeat families within Pfam. We carried out a significant comparison to the structural classification database, namely the Evolutionary Classification of Protein Domains (ECOD) that led to the creation of 825 new families based on their set of uncharacterized families (EUFs). Furthermore, we also connected Pfam entries to the Sequence Ontology (SO) through mapping of the Pfam type definitions to SO terms. Since Pfam has many community contributors, we recently enabled the linking between authorship of all Pfam entries with the corresponding authors’ ORCID identifiers. This effectively permits authors to claim credit for their Pfam curation and link them to their ORCID record.",D427 - D432,10.1093/nar/gky995,http://www.moore.com/register.html
57dfc18815bba1c3737dbc2e5497fd1fc595edb5,Introducing EzTaxon-e: a prokaryotic 16S rRNA gene sequence database with phylotypes that represent uncultured species.,"Despite recent advances in commercially optimized identification systems, bacterial identification remains a challenging task in many routine microbiological laboratories, especially in situations where taxonomically novel isolates are involved. The 16S rRNA gene has been used extensively for this task when coupled with a well-curated database, such as EzTaxon, containing sequences of type strains of prokaryotic species with validly published names. Although the EzTaxon database has been widely used for routine identification of prokaryotic isolates, sequences from uncultured prokaryotes have not been considered. Here, the next generation database, named EzTaxon-e, is formally introduced. This new database covers not only species within the formal nomenclatural system but also phylotypes that may represent species in nature. In addition to an identification function based on Basic Local Alignment Search Tool (blast) searches and pairwise global sequence alignments, a new objective method of assessing the degree of completeness in sequencing is proposed. All sequences that are held in the EzTaxon-e database have been subjected to phylogenetic analysis and this has resulted in a complete hierarchical classification system. It is concluded that the EzTaxon-e database provides a useful taxonomic backbone for the identification of cultured and uncultured prokaryotes and offers a valuable means of communication among microbiologists who routinely encounter taxonomically novel isolates. The database and its analytical functions can be found at http://eztaxon-e.ezbiocloud.net/.","
          716-21
        ",10.1099/ijs.0.038075-0,http://www.henry.org/posts/posts/faq.html
9f7626c7af925b7b69f1ba86ceb916d21bc03dbe,Pfam: The protein families database in 2021,"Abstract The Pfam database is a widely used resource for classifying protein sequences into families and domains. Since Pfam was last described in this journal, over 350 new families have been added in Pfam 33.1 and numerous improvements have been made to existing entries. To facilitate research on COVID-19, we have revised the Pfam entries that cover the SARS-CoV-2 proteome, and built new entries for regions that were not covered by Pfam. We have reintroduced Pfam-B which provides an automatically generated supplement to Pfam and contains 136 730 novel clusters of sequences that are not yet matched by a Pfam family. The new Pfam-B is based on a clustering by the MMseqs2 software. We have compared all of the regions in the RepeatsDB to those in Pfam and have started to use the results to build and refine Pfam repeat families. Pfam is freely available for browsing and download at http://pfam.xfam.org/.",D412 - D419,10.1093/nar/gkaa913,https://www.simmons-cruz.com/tags/search/
16b0744424f02e01fe2f01b3ea03e2862f1359fc,"Reference sequence (RefSeq) database at NCBI: current status, taxonomic expansion, and functional annotation","The RefSeq project at the National Center for Biotechnology Information (NCBI) maintains and curates a publicly available database of annotated genomic, transcript, and protein sequence records (http://www.ncbi.nlm.nih.gov/refseq/). The RefSeq project leverages the data submitted to the International Nucleotide Sequence Database Collaboration (INSDC) against a combination of computation, manual curation, and collaboration to produce a standard set of stable, non-redundant reference sequences. The RefSeq project augments these reference sequences with current knowledge including publications, functional features and informative nomenclature. The database currently represents sequences from more than 55 000 organisms (>4800 viruses, >40 000 prokaryotes and >10 000 eukaryotes; RefSeq release 71), ranging from a single record to complete genomes. This paper summarizes the current status of the viral, prokaryotic, and eukaryotic branches of the RefSeq project, reports on improvements to data access and details efforts to further expand the taxonomic representation of the collection. We also highlight diverse functional curation initiatives that support multiple uses of RefSeq data including taxonomic validation, genome annotation, comparative genomics, and clinical testing. We summarize our approach to utilizing available RNA-Seq and other data types in our manual curation process for vertebrate, plant, and other species, and describe a new direction for prokaryotic genomes and protein name management.",D733 - D745,10.1093/nar/gkv1189,http://caldwell.com/main/homepage/
d87ceda3042f781c341ac17109d1e94a717f5f60,WordNet : an electronic lexical database,"Part 1 The lexical database: nouns in WordNet, George A. Miller modifiers in WordNet, Katherine J. Miller a semantic network of English verbs, Christiane Fellbaum design and implementation of the WordNet lexical database and searching software, Randee I. Tengi. Part 2: automated discovery of WordNet relations, Marti A. Hearst representing verb alterations in WordNet, Karen T. Kohl et al the formalization of WordNet by methods of relational concept analysis, Uta E. Priss. Part 3 Applications of WordNet: building semantic concordances, Shari Landes et al performance and confidence in a semantic annotation task, Christiane Fellbaum et al WordNet and class-based probabilities, Philip Resnik combining local context and WordNet similarity for word sense identification, Claudia Leacock and Martin Chodorow using WordNet for text retrieval, Ellen M. Voorhees lexical chains as representations of context for the detection and correction of malapropisms, Graeme Hirst and David St-Onge temporal indexing through lexical chaining, Reem Al-Halimi and Rick Kazman COLOR-X - using knowledge from WordNet for conceptual modelling, J.F.M. Burg and R.P. van de Riet knowledge processing on an extended WordNet, Sanda M. Harabagiu and Dan I Moldovan appendix - obtaining and using WordNet.",706,10.2307/417141,https://www.spencer-blevins.com/blog/tag/category/
68c03788224000794d5491ab459be0b2a2c38677,WordNet: A Lexical Database for English,"Because meaningful sentences are composed of meaningful words, any system that hopes to process natural languages as people do must have information about words and their meanings. This information is traditionally provided through dictionaries, and machine-readable dictionaries are now widely available. But dictionary entries evolved for the convenience of human readers, not for machines. WordNet1 provides a more effective combination of traditional lexicographic information and modern computing. WordNet is an online lexical database designed for use under program control. English nouns, verbs, adjectives, and adverbs are organized into sets of synonyms, each representing a lexicalized concept. Semantic relations link the synonym sets [4].",39-41,10.1145/219717.219748,https://hodges-owens.com/
80777d42513103bede188b2eebbdce7fb6f91390,HMDB 4.0: the human metabolome database for 2018,"Abstract The Human Metabolome Database or HMDB (www.hmdb.ca) is a web-enabled metabolomic database containing comprehensive information about human metabolites along with their biological roles, physiological concentrations, disease associations, chemical reactions, metabolic pathways, and reference spectra. First described in 2007, the HMDB is now considered the standard metabolomic resource for human metabolic studies. Over the past decade the HMDB has continued to grow and evolve in response to emerging needs for metabolomics researchers and continuing changes in web standards. This year's update, HMDB 4.0, represents the most significant upgrade to the database in its history. For instance, the number of fully annotated metabolites has increased by nearly threefold, the number of experimental spectra has grown by almost fourfold and the number of illustrated metabolic pathways has grown by a factor of almost 60. Significant improvements have also been made to the HMDB’s chemical taxonomy, chemical ontology, spectral viewing, and spectral/text searching tools. A great deal of brand new data has also been added to HMDB 4.0. This includes large quantities of predicted MS/MS and GC–MS reference spectral data as well as predicted (physiologically feasible) metabolite structures to facilitate novel metabolite identification. Additional information on metabolite-SNP interactions and the influence of drugs on metabolite levels (pharmacometabolomics) has also been added. Many other important improvements in the content, the interface, and the performance of the HMDB website have been made and these should greatly enhance its ease of use and its potential applications in nutrition, biochemistry, clinical chemistry, clinical genetics, medicine, and metabolomics science.",D608 - D617,10.1093/nar/gkx1089,https://www.baker.com/main/tags/post.jsp
bbe6e5fcc96e685db714d6aa11ffe6f49567c585,A global database of COVID-19 vaccinations,,947 - 953,10.1038/s41562-021-01122-8,http://harris-sanders.com/main/search/explore/main/
3aca912f21d54b3931fa1fdfac0c199c557374a4,GTDB-Tk: a toolkit to classify genomes with the Genome Taxonomy Database,Abstract Summary The Genome Taxonomy Database Toolkit (GTDB-Tk) provides objective taxonomic assignments for bacterial and archaeal genomes based on the GTDB. GTDB-Tk is computationally efficient and able to classify thousands of draft genomes in parallel. Here we demonstrate the accuracy of the GTDB-Tk taxonomic assignments by evaluating its performance on a phylogenetically diverse set of 10 156 bacterial and archaeal metagenome-assembled genomes. Availability and implementation GTDB-Tk is implemented in Python and licenced under the GNU General Public Licence v3.0. Source code and documentation are available at: https://github.com/ecogenomics/gtdbtk. Supplementary information Supplementary data are available at Bioinformatics online.,1925 - 1927,10.1093/bioinformatics/btz848,https://www.diaz-lewis.biz/author.htm
9dd54cd7ce4ebf2e52b762817c2688b56bb9e652,2016 update of the PRIDE database and its related tools,"The PRoteomics IDEntifications (PRIDE) database is one of the world-leading data repositories of mass spectrometry (MS)-based proteomics data. Since the beginning of 2014, PRIDE Archive (http://www.ebi.ac.uk/pride/archive/) is the new PRIDE archival system, replacing the original PRIDE database. Here we summarize the developments in PRIDE resources and related tools since the previous update manuscript in the Database Issue in 2013. PRIDE Archive constitutes a complete redevelopment of the original PRIDE, comprising a new storage backend, data submission system and web interface, among other components. PRIDE Archive supports the most-widely used PSI (Proteomics Standards Initiative) data standard formats (mzML and mzIdentML) and implements the data requirements and guidelines of the ProteomeXchange Consortium. The wide adoption of ProteomeXchange within the community has triggered an unprecedented increase in the number of submitted data sets (around 150 data sets per month). We outline some statistics on the current PRIDE Archive data contents. We also report on the status of the PRIDE related stand-alone tools: PRIDE Inspector, PRIDE Converter 2 and the ProteomeXchange submission tool. Finally, we will give a brief update on the resources under development 'PRIDE Cluster' and 'PRIDE Proteomes', which provide a complementary view and quality-scored information of the peptide and protein identification data available in PRIDE Archive.",11033 - 11033,10.1093/NAR/GKV1145,http://www.forbes-stewart.com/post/
c6b3ca4f939e36a9679a70e14ce8b1bbbc5618f3,Labeled Faces in the Wild: A Database forStudying Face Recognition in Unconstrained Environments,"Most face databases have been created under controlled conditions to facilitate the study of specific parameters on the face recognition problem. These parameters include such variables as position, pose, lighting, background, camera quality, and gender. While there are many applications for face recognition technology in which one can control the parameters of image acquisition, there are also many applications in which the practitioner has little or no control over such parameters. This database, Labeled Faces in the Wild, is provided as an aid in studying the latter, unconstrained, recognition problem. The database contains labeled face photographs spanning the range of conditions typically encountered in everyday life. The database exhibits “natural” variability in factors such as pose, lighting, race, accessories, occlusions, and background. In addition to describing the details of the database, we provide specific experimental paradigms for which the database is suitable. This is done in an effort to make research performed with the database as consistent and comparable as possible. We provide baseline results, including results of a state of the art face recognition system combined with a face alignment system. To facilitate experimentation on the database, we provide several parallel databases, including an aligned version.",64-126,609-80517-4,http://ortega-white.com/privacy.htm
e6ce8255f48e3736f0a5fa0d85fb43c700d4f743,The Cambridge Structural Database: a quarter of a million crystal structures and rising.,"The Cambridge Structural Database (CSD) now contains data for more than a quarter of a million small-molecule crystal structures. The information content of the CSD, together with methods for data acquisition, processing and validation, are summarized, with particular emphasis on the chemical information added by CSD editors. Nearly 80% of new structural data arrives electronically, mostly in CIF format, and the CCDC acts as the official crystal structure data depository for 51 major journals. The CCDC now maintains both a CIF archive (more than 73,000 CIFs dating from 1996), as well as the distributed binary CSD archive; the availability of data in both archives is discussed. A statistical survey of the CSD is also presented and projections concerning future accession rates indicate that the CSD will contain at least 500,000 crystal structures by the year 2010.","
          380-8
        ",10.1107/S0108768102003890,https://www.simmons.org/home.html
41abf43dc718e271299457bce65bccfe3feeb9d6,"Greengenes, a Chimera-Checked 16S rRNA Gene Database and Workbench Compatible with ARB","ABSTRACT A 16S rRNA gene database (http://greengenes.lbl.gov ) addresses limitations of public repositories by providing chimera screening, standard alignment, and taxonomic classification using multiple published taxonomies. It was found that there is incongruent taxonomic nomenclature among curators even at the phylum level. Putative chimeras were identified in 3% of environmental sequences and in 0.2% of records derived from isolates. Environmental sequences were classified into 100 phylum-level lineages in the Archaea and Bacteria.",5069 - 5072,10.1128/AEM.03006-05,https://www.marks.info/
7b1d8dfb9e6260685d9fbb8c41bfc0a35710fe41,CARD 2020: antibiotic resistome surveillance with the comprehensive antibiotic resistance database,"Abstract The Comprehensive Antibiotic Resistance Database (CARD; https://card.mcmaster.ca) is a curated resource providing reference DNA and protein sequences, detection models and bioinformatics tools on the molecular basis of bacterial antimicrobial resistance (AMR). CARD focuses on providing high-quality reference data and molecular sequences within a controlled vocabulary, the Antibiotic Resistance Ontology (ARO), designed by the CARD biocuration team to integrate with software development efforts for resistome analysis and prediction, such as CARD’s Resistance Gene Identifier (RGI) software. Since 2017, CARD has expanded through extensive curation of reference sequences, revision of the ontological structure, curation of over 500 new AMR detection models, development of a new classification paradigm and expansion of analytical tools. Most notably, a new Resistomes & Variants module provides analysis and statistical summary of in silico predicted resistance variants from 82 pathogens and over 100 000 genomes. By adding these resistance variants to CARD, we are able to summarize predicted resistance using the information included in CARD, identify trends in AMR mobility and determine previously undescribed and novel resistance variants. Here, we describe updates and recent expansions to CARD and its biocuration process, including new resources for community biocuration of AMR molecular reference data.",D517 - D525,10.1093/nar/gkz935,https://www.gilbert.com/homepage/
1976c9eeccc7115d18a04f1e7fb5145db6b96002,Freebase: a collaboratively created graph database for structuring human knowledge,"Freebase is a practical, scalable tuple database used to structure general human knowledge. The data in Freebase is collaboratively created, structured, and maintained. Freebase currently contains more than 125,000,000 tuples, more than 4000 types, and more than 7000 properties. Public read/write access to Freebase is allowed through an HTTP-based graph-query API using the Metaweb Query Language (MQL) as a data query and manipulation language. MQL provides an easy-to-use object-oriented interface to the tuple data in Freebase and is designed to facilitate the creation of collaborative, Web-based data-oriented applications.",1247-1250,10.1145/1376616.1376746,https://www.kim.com/author.php
76eb8e5688ee2951e5f04fb14956abf93a890149,The Carbohydrate-Active EnZymes database (CAZy): an expert resource for Glycogenomics,"The Carbohydrate-Active Enzyme (CAZy) database is a knowledge-based resource specialized in the enzymes that build and breakdown complex carbohydrates and glycoconjugates. As of September 2008, the database describes the present knowledge on 113 glycoside hydrolase, 91 glycosyltransferase, 19 polysaccharide lyase, 15 carbohydrate esterase and 52 carbohydrate-binding module families. These families are created based on experimentally characterized proteins and are populated by sequences from public databases with significant similarity. Protein biochemical information is continuously curated based on the available literature and structural information. Over 6400 proteins have assigned EC numbers and 700 proteins have a PDB structure. The classification (i) reflects the structural features of these enzymes better than their sole substrate specificity, (ii) helps to reveal the evolutionary relationships between these enzymes and (iii) provides a convenient framework to understand mechanistic properties. This resource has been available for over 10 years to the scientific community, contributing to information dissemination and providing a transversal nomenclature to glycobiologists. More recently, this resource has been used to improve the quality of functional predictions of a number genome projects by providing expert annotation. The CAZy resource resides at URL: http://www.cazy.org/.",D233 - D238,10.1093/nar/gkn663,https://www.wheeler.com/register/
cdad2f8ca559f425ab7fa402535354a86b0a370a,CDD/SPARCLE: the conserved domain database in 2020,"As NLM's Conserved Domain Database (CDD) enters its 20th year of operations as a publicly available resource, CDD curation staff continues to develop hierarchical classifications of widely distributed protein domain families, and to record conserved sites associated with molecular function, so that they can be mapped onto user queries in support of hypothesis-driven biomolecular research. CDD offers both an archive of pre-computed domain annotations as well as live search services for both single protein or nucleotide queries and larger sets of protein query sequences. CDD staff has continued to characterize protein families via conserved domain architectures and has built up a significant corpus of curated domain architectures in support of naming bacterial proteins in RefSeq. These architecture definitions are available via SPARCLE, the Subfamily Protein Architecture Labeling Engine. CDD can be accessed at https://www.ncbi.nlm.nih.gov/Structure/cdd/cdd.shtml.",41-148,10.1093/nar/gkz991,https://www.dunn-taylor.com/categories/categories/terms.asp
c07eca9862e0144aa4c0c29c7978caa6eff60e2f,The Ribosomal Database Project: improved alignments and new tools for rRNA analysis,"The Ribosomal Database Project (RDP) provides researchers with quality-controlled bacterial and archaeal small subunit rRNA alignments and analysis tools. An improved alignment strategy uses the Infernal secondary structure aware aligner to provide a more consistent higher quality alignment and faster processing of user sequences. Substantial new analysis features include a new Pyrosequencing Pipeline that provides tools to support analysis of ultra high-throughput rRNA sequencing data. This pipeline offers a collection of tools that automate the data processing and simplify the computationally intensive analysis of large sequencing libraries. In addition, a new Taxomatic visualization tool allows rapid visualization of taxonomic inconsistencies and suggests corrections, and a new class Assignment Generator provides instructors with a lesson plan and individualized teaching materials. Details about RDP data and analytical functions can be found at http://rdp.cme.msu.edu/.",D141 - D145,10.1093/nar/gkn879,https://rodriguez.com/homepage/
716000409a3a2e2c75801b3d58b9b17b68eeaef7,An improved method of constructing a database of monthly climate observations and associated high‐resolution grids,"A database of monthly climate observations from meteorological stations is constructed. The database includes six climate elements and extends over the global land surface. The database is checked for inhomogeneities in the station records using an automated method that refines previous methods by using incomplete and partially overlapping records and by detecting inhomogeneities with opposite signs in different seasons. The method includes the development of reference series using neighbouring stations. Information from different sources about a single station may be combined, even without an overlapping period, using a reference series. Thus, a longer station record may be obtained and fragmentation of records reduced. The reference series also enables 1961–90 normals to be calculated for a larger proportion of stations.",68-115,10.1002/JOC.1181,http://rivera-walker.com/
80e394ee3e1834091596e8b55c9ad9bf11456e09,"DAVID: Database for Annotation, Visualization, and Integrated Discovery",,R60 - R60,10.1186/gb-2003-4-9-r60,https://garrett.com/
6a074a3fa856e86b2e6bc60e83d66cc488090ae9,The ecoinvent database version 3 (part I): overview and methodology,,1218-1230,10.1007/s11367-016-1087-8,http://moore-andrews.com/
8c1a1e761b715b23668b4f850e2bcc958fa21ad2,Empirical statistical model to estimate the accuracy of peptide identifications made by MS/MS and database search.,"We present a statistical model to estimate the accuracy of peptide assignments to tandem mass (MS/MS) spectra made by database search applications such as SEQUEST. Employing the expectation maximization algorithm, the analysis learns to distinguish correct from incorrect database search results, computing probabilities that peptide assignments to spectra are correct based upon database search scores and the number of tryptic termini of peptides. Using SEQUEST search results for spectra generated from a sample of known protein components, we demonstrate that the computed probabilities are accurate and have high power to discriminate between correctly and incorrectly assigned peptides. This analysis makes it possible to filter large volumes of MS/MS database search results with predictable false identification error rates and can serve as a common standard by which the results of different research groups are compared.","
          5383-92
        ",10.1021/AC025747H,http://carrillo.net/about/
d7c78b7071ea150346320e5b43a03824263e0fa9,Ribosomal Database Project: data and tools for high throughput rRNA analysis,"Ribosomal Database Project (RDP; http://rdp.cme.msu.edu/) provides the research community with aligned and annotated rRNA gene sequence data, along with tools to allow researchers to analyze their own rRNA gene sequences in the RDP framework. RDP data and tools are utilized in fields as diverse as human health, microbial ecology, environmental microbiology, nucleic acid chemistry, taxonomy and phylogenetics. In addition to aligned and annotated collections of bacterial and archaeal small subunit rRNA genes, RDP now includes a collection of fungal large subunit rRNA genes. RDP tools, including Classifier and Aligner, have been updated to work with this new fungal collection. The use of high-throughput sequencing to characterize environmental microbial populations has exploded in the past several years, and as sequence technologies have improved, the sizes of environmental datasets have increased. With release 11, RDP is providing an expanded set of tools to facilitate analysis of high-throughput data, including both single-stranded and paired-end reads. In addition, most tools are now available as open source packages for download and local use by researchers with high-volume needs or who would like to develop custom analysis pipelines.",D633 - D642,10.1093/nar/gkt1244,https://www.robertson.com/list/main/tags/terms/
dc52d1ede1b90bf9d296bc5b34c9310b7eaa99a2,The mnist database of handwritten digits,Disclosed is an improved articulated bar flail having shearing edges for efficiently shredding materials. An improved shredder cylinder is disclosed with a plurality of these flails circumferentially spaced and pivotally attached to the periphery of a rotatable shaft. Also disclosed is an improved shredder apparatus which has a pair of these shredder cylinders mounted to rotate about spaced parallel axes which cooperates with a conveyer apparatus which has a pair of inclined converging conveyer belts with one of the belts mounted to move with respect to the other belt to allow the transport of articles of various sizes therethrough.,39-109,7297-3110-6,http://jensen-bishop.com/home/
317325439a0ce543d7629848a35adea04b6e7d12,The InterPro protein families and domains database: 20 years on,"Abstract The InterPro database (https://www.ebi.ac.uk/interpro/) provides an integrative classification of protein sequences into families, and identifies functionally important domains and conserved sites. InterProScan is the underlying software that allows protein and nucleic acid sequences to be searched against InterPro's signatures. Signatures are predictive models which describe protein families, domains or sites, and are provided by multiple databases. InterPro combines signatures representing equivalent families, domains or sites, and provides additional information such as descriptions, literature references and Gene Ontology (GO) terms, to produce a comprehensive resource for protein classification. Founded in 1999, InterPro has become one of the most widely used resources for protein family annotation. Here, we report the status of InterPro (version 81.0) in its 20th year of operation, and its associated software, including updates to database content, the release of a new website and REST API, and performance improvements in InterProScan.",D344 - D354,10.1093/nar/gkaa977,https://www.sweeney.info/homepage.htm
ceee6447b291f8052a28c9eb00ca360d6f39f9b1,"The STRING database in 2011: functional interaction networks of proteins, globally integrated and scored","An essential prerequisite for any systems-level understanding of cellular functions is to correctly uncover and annotate all functional interactions among proteins in the cell. Toward this goal, remarkable progress has been made in recent years, both in terms of experimental measurements and computational prediction techniques. However, public efforts to collect and present protein interaction information have struggled to keep up with the pace of interaction discovery, partly because protein–protein interaction information can be error-prone and require considerable effort to annotate. Here, we present an update on the online database resource Search Tool for the Retrieval of Interacting Genes (STRING); it provides uniquely comprehensive coverage and ease of access to both experimental as well as predicted interaction information. Interactions in STRING are provided with a confidence score, and accessory information such as protein domains and 3D structures is made available, all within a stable and consistent identifier space. New features in STRING include an interactive network viewer that can cluster networks on demand, updated on-screen previews of structural information including homology models, extensive data updates and strongly improved connectivity and integration with third-party resources. Version 9.0 of STRING covers more than 1100 completely sequenced organisms; the resource can be reached at http://string-db.org.",D561 - D568,10.1093/nar/gkq973,http://www.harvey.biz/author.php
9667f8264745b626c6173b1310e2ff0298b09cfc,Learning Deep Features for Scene Recognition using Places Database,"Scene recognition is one of the hallmark tasks of computer vision, allowing definition of a context for object recognition. Whereas the tremendous recent progress in object recognition tasks is due to the availability of large datasets like ImageNet and the rise of Convolutional Neural Networks (CNNs) for learning high-level features, performance at scene recognition has not attained the same level of success. This may be because current deep features trained from ImageNet are not competitive enough for such tasks. Here, we introduce a new scene-centric database called Places with over 7 million labeled pictures of scenes. We propose new methods to compare the density and diversity of image datasets and show that Places is as dense as other scene datasets and has more diversity. Using CNN, we learn deep features for scene recognition tasks, and establish new state-of-the-art results on several scene-centric datasets. A visualization of the CNN layers' responses allows us to show differences in the internal representations of object-centric and scene-centric networks.",487-495,8343-2126-6,http://fuller.com/home/
5ef2cf7b7aa6f7e44488d5db5409ef7f76b9ef9a,SCOP: a structural classification of proteins database for the investigation of sequences and structures.,,"
          536-40
        ",10.1016/S0022-2836(05)80134-2,http://porter-stewart.biz/
633f318876c41fed36b3905b8af5fdc27f734615,The Molecular Signatures Database (MSigDB) hallmark gene set collection.,"The Molecular Signatures Database (MSigDB) is one of the most widely used and comprehensive databases of gene sets for performing gene set enrichment analysis. Since its creation, MSigDB has grown beyond its roots in metabolic disease and cancer to include >10,000 gene sets. These better represent a wider range of biological processes and diseases, but the utility of the database is reduced by increased redundancy across, and heterogeneity within, gene sets. To address this challenge, here we use a combination of automated approaches and expert curation to develop a collection of ""hallmark"" gene sets as part of MSigDB. Each hallmark in this collection consists of a ""refined"" gene set, derived from multiple ""founder"" sets, that conveys a specific biological state or process and displays coherent expression. The hallmarks effectively summarize most of the relevant information of the original founder sets and, by reducing both variation and redundancy, provide more refined and concise inputs for gene set enrichment analysis.","
          417-425
        ",7218-3715-7,https://west.com/home.htm
298d799da82395a64a3bda38ef9d2a4646828ccb,A fast quantum mechanical algorithm for database search,"were proposed in the early 1980’s [Benioff80] and shown to be at least as powerful as classical computers an important but not surprising result, since classical computers, at the deepest level, ultimately follow the laws of quantum mechanics. The description of quantum mechanical computers was formalized in the late 80’s and early 90’s [Deutsch85][BB92] [BV93] [Yao93] and they were shown to be more powerful than classical computers on various specialized problems. In early 1994, [Shor94] demonstrated that a quantum mechanical computer could efficiently solve a well-known problem for which there was no known efficient algorithm using classical computers. This is the problem of integer factorization, i.e. testing whether or not a given integer, N, is prime, in a time which is a finite power of o (logN) . ----------------------------------------------",212-219,10.1145/237814.237866,http://dunn.info/
11f647b95a7c9a94c346cd8dc53987105cb0f7c1,dbSNP: the NCBI database of genetic variation,"In response to a need for a general catalog of genome variation to address the large-scale sampling designs required by association studies, gene mapping and evolutionary biology, the National Center for Biotechnology Information (NCBI) has established the dbSNP database [S.T.Sherry, M.Ward and K. Sirotkin (1999) Genome Res., 9, 677-679]. Submissions to dbSNP will be integrated with other sources of information at NCBI such as GenBank, PubMed, LocusLink and the Human Genome Project data. The complete contents of dbSNP are available to the public at website: http://www.ncbi.nlm.nih.gov/SNP. The complete contents of dbSNP can also be downloaded in multiple formats via anonymous FTP at ftp://ncbi.nlm.nih.gov/snp/.","
          308-11
        ",10.1093/NAR/29.1.308,http://www.pratt.com/blog/search/posts/register.php
b7599c8ba88e7c93edbce57df513152e8f5693e7,The COG database: an updated version includes eukaryotes,,41 - 41,10.1186/1471-2105-4-41,https://bauer.com/main.php
f80a6ab4b0cfae0d00747f0f41f3e643f22f33ee,Molecular signatures database (MSigDB) 3.0,"MOTIVATION
Well-annotated gene sets representing the universe of the biological processes are critical for meaningful and insightful interpretation of large-scale genomic data. The Molecular Signatures Database (MSigDB) is one of the most widely used repositories of such sets.


RESULTS
We report the availability of a new version of the database, MSigDB 3.0, with over 6700 gene sets, a complete revision of the collection of canonical pathways and experimental signatures from publications, enhanced annotations and upgrades to the web site.


AVAILABILITY AND IMPLEMENTATION
MSigDB is freely available for non-commercial use at http://www.broadinstitute.org/msigdb.","
          1739-40
        ",10.1093/bioinformatics/btr260,http://www.lucas.net/wp-content/tags/category.html
4bd970a37c59c97804ff93cbb2c108e081de3a37,Introduction to WordNet: An On-line Lexical Database,"Standard alphabetical procedures for organizing lexical information put together words that are spelled alike and scatter words with similar or related meanings haphazardly through the list. Unfortunately, there is no obvious alternative, no other simple way for lexicographers to keep track of what has been done or for readers to find the word they are looking for. But a frequent objection to this solution is that finding things on an alphabetical list can be tedious and time-consuming. Many people who would like to refer to a dictionary decide not to bother with it because finding the information would interrupt their work and break their train of thought.",235-244,10.1093/IJL/3.4.235,https://williams.net/posts/posts/blog/terms/
d6a5a1e8f56260608d2f7651a2f6aac6a041b57a,The Pfam protein families database,"Pfam is a widely used database of protein families, currently containing more than 13 000 manually curated protein families as of release 26.0. Pfam is available via servers in the UK (http://pfam.sanger.ac.uk/), the USA (http://pfam.janelia.org/) and Sweden (http://pfam.sbc.su.se/). Here, we report on changes that have occurred since our 2010 NAR paper (release 24.0). Over the last 2 years, we have generated 1840 new families and increased coverage of the UniProt Knowledgebase (UniProtKB) to nearly 80%. Notably, we have taken the step of opening up the annotation of our families to the Wikipedia community, by linking Pfam families to relevant Wikipedia pages and encouraging the Pfam and Wikipedia communities to improve and expand those pages. We continue to improve the Pfam website and add new visualizations, such as the ‘sunburst’ representation of taxonomic distribution of families. In this work we additionally address two topics that will be of particular interest to the Pfam community. First, we explain the definition and use of family-specific, manually curated gathering thresholds. Second, we discuss some of the features of domains of unknown function (also known as DUFs), which constitute a rapidly growing class of families within Pfam.",D290 - D301,10.1093/nar/gkr1065,http://www.steele-garcia.biz/explore/posts/search/author.php
bb967168ead7a14adcb0121dcf24a930d1a383b3,The HITRAN 2008 molecular spectroscopic database,,139-204,10.1016/J.JQSRT.2017.06.038,http://potter.com/category/tags/terms.htm
90bc0ca3feebe0215079cf575b90017170a0089f,CDD: NCBI's conserved domain database,"NCBI's CDD, the Conserved Domain Database, enters its 15th year as a public resource for the annotation of proteins with the location of conserved domain footprints. Going forward, we strive to improve the coverage and consistency of domain annotation provided by CDD. We maintain a live search system as well as an archive of pre-computed domain annotation for sequences tracked in NCBI's Entrez protein database, which can be retrieved for single sequences or in bulk. We also maintain import procedures so that CDD contains domain models and domain definitions provided by several collections available in the public domain, as well as those produced by an in-house curation effort. The curation effort aims at increasing coverage and providing finer-grained classifications of common protein domains, for which a wealth of functional and structural data has become available. CDD curation generates alignment models of representative sequence fragments, which are in agreement with domain boundaries as observed in protein 3D structure, and which model the structurally conserved cores of domain families as well as annotate conserved features. CDD can be accessed at http://www.ncbi.nlm.nih.gov/Structure/cdd/cdd.shtml.",D222 - D226,10.1093/nar/gku1221,http://parker.net/faq.asp
8b3b8848a311c501e704c45c6d50430ab7068956,HMDB: A large video database for human motion recognition,"With nearly one billion online videos viewed everyday, an emerging new frontier in computer vision research is recognition and search in video. While much effort has been devoted to the collection and annotation of large scalable static image datasets containing thousands of image categories, human action datasets lag far behind. Current action recognition databases contain on the order of ten different action categories collected under fairly controlled conditions. State-of-the-art performance on these datasets is now near ceiling and thus there is a need for the design and creation of new benchmarks. To address this issue we collected the largest action video database to-date with 51 action categories, which in total contain around 7,000 manually annotated clips extracted from a variety of sources ranging from digitized movies to YouTube. We use this database to evaluate the performance of two representative computer vision systems for action recognition and explore the robustness of these methods under various conditions such as camera motion, viewpoint, video quality and occlusion.",2556-2563,10.1109/ICCV.2011.6126543,http://www.rocha-caldwell.com/search/search.html
d364903a626ad70e6ce057209d9b7e004dafd4be,"PlantCARE, a database of plant cis-acting regulatory elements and a portal to tools for in silico analysis of promoter sequences","PlantCARE is a database of plant cis-acting regulatory elements, enhancers and repressors. Regulatory elements are represented by positional matrices, consensus sequences and individual sites on particular promoter sequences. Links to the EMBL, TRANSFAC and MEDLINE databases are provided when available. Data about the transcription sites are extracted mainly from the literature, supplemented with an increasing number of in silico predicted data. Apart from a general description for specific transcription factor sites, levels of confidence for the experimental evidence, functional information and the position on the promoter are given as well. New features have been implemented to search for plant cis-acting regulatory elements in a query sequence. Furthermore, links are now provided to a new clustering and motif search method to investigate clusters of co-expressed genes. New regulatory elements can be sent automatically and will be added to the database after curation. The PlantCARE relational database is available via the World Wide Web at http://sphinx.rug.ac.be:8080/PlantCARE/.","
          325-7
        ",10.1093/NAR/30.1.325,http://www.hawkins-martin.biz/login.html
82524ddee00fa0895dfca43995a7ec8bdb16f0d5,Fundamentals of Database Systems,"From the Publisher: 
Fundamentals of Database Systems combines clear explanations of theory and design, broad coverage of models and real systems, and excellent examples with up-to-date introductions to modern database technologies. This edition is completely revised and updated, and reflects the latest trends in technological and application development. Professors Elmasri and Navathe focus on the relational model and include coverage of recent object-oriented developments. They also address advanced modeling and system enhancements in the areas of active databases, temporal and spatial databases, and multimedia information systems. This edition also surveys the latest application areas of data warehousing, data mining, web databases, digital libraries, GIS, and genome databases. New to the Third Edition 
Reorganized material on data modeling to clearly separate entity relationship modeling, extended entity relationship modeling, and object-oriented modeling Expanded coverage of the object-oriented and object/relational approach to data management, including ODMG and SQL3 Uses examples from real database systems including OracleTM and Microsoft AccessAE Includes discussion of decision support applications of data warehousing and data mining, as well as emerging technologies of web databases, multimedia, and mobile databases Covers advanced modeling in the areas of active, temporal, and spatial databases Provides coverage of issues of physical database tuning Discusses current database application areas of GIS, genome, and digital libraries",60-135,338-04616-1,http://www.garcia.biz/main/main/register.html
62f5ffb09a4c9543509c38f005b9c6eb308c6974,The COG database: a tool for genome-scale analysis of protein functions and evolution,"Rational classification of proteins encoded in sequenced genomes is critical for making the genome sequences maximally useful for functional and evolutionary studies. The database of Clusters of Orthologous Groups of proteins (COGs) is an attempt on a phylogenetic classification of the proteins encoded in 21 complete genomes of bacteria, archaea and eukaryotes (http://www. ncbi.nlm. nih.gov/COG). The COGs were constructed by applying the criterion of consistency of genome-specific best hits to the results of an exhaustive comparison of all protein sequences from these genomes. The database comprises 2091 COGs that include 56-83% of the gene products from each of the complete bacterial and archaeal genomes and approximately 35% of those from the yeast Saccharomyces cerevisiae genome. The COG database is accompanied by the COGNITOR program that is used to fit new proteins into the COGs and can be applied to functional and phylogenetic annotation of newly sequenced genomes.","
          33-6
        ",10.1093/nar/28.1.33,http://lyons.com/
073df13736ed5407374af2f397bd65560f156dd2,"Repbase Update, a database of repetitive elements in eukaryotic genomes",,57-110,10.1186/s13100-015-0041-9,https://www.sullivan.com/category.htm
3b073a5e7de5513705a7e2a7b1c88d3acbeed82c,TCMSP: a database of systems pharmacology for drug discovery from herbal medicines,,13 - 13,10.1186/1758-2946-6-13,https://www.mcintyre.com/
b307d55ba07058d6183991d2d2a81b340d558186,"NCBI reference sequences (RefSeq): a curated non-redundant sequence database of genomes, transcripts and proteins","The National Center for Biotechnology Information (NCBI) Reference Sequence (RefSeq) database (http://www.ncbi.nlm.nih.gov/RefSeq/) provides a non-redundant collection of sequences representing genomic data, transcripts and proteins. Although the goal is to provide a comprehensive dataset representing the complete sequence information for any given species, the database pragmatically includes sequence data that are currently publicly available in the archival databases. The database incorporates data from over 2400 organisms and includes over one million proteins representing significant taxonomic diversity spanning prokaryotes, eukaryotes and viruses. Nucleotide and protein sequences are explicitly linked, and the sequences are linked to other resources including the NCBI Map Viewer and Gene. Sequences are annotated to include coding regions, conserved domains, variation, references, names, database cross-references, and other features using a combined approach of collaboration and other input from the scientific community, automated annotation, propagation from GenBank and curation by NCBI staff.","
          D501-4
        ",10.1093/NAR/GKL842,https://www.davis-cooley.com/list/wp-content/terms/
092c275005ae49dc1303214f6d02d134457c7053,LabelMe: A Database and Web-Based Tool for Image Annotation,,157-173,10.1007/s11263-007-0090-8,https://www.wells.com/
0e466ea033b982519f351022304dccb64a46b93c,IPD-IMGT/HLA Database,"Abstract The IPD-IMGT/HLA Database, http://www.ebi.ac.uk/ipd/imgt/hla/, currently contains over 25 000 allele sequence for 45 genes, which are located within the Major Histocompatibility Complex (MHC) of the human genome. This region is the most polymorphic region of the human genome, and the levels of polymorphism seen exceed most other genes. Some of the genes have several thousand variants and are now termed hyperpolymorphic, rather than just simply polymorphic. The IPD-IMGT/HLA Database has provided a stable, highly accessible, user-friendly repository for this information, providing the scientific and medical community access to the many variant sequences of this gene system, that are critical for the successful outcome of transplantation. The number of currently known variants, and dramatic increase in the number of new variants being identified has necessitated a dedicated resource with custom tools for curation and publication. The challenge for the database is to continue to provide a highly curated database of sequence variants, while supporting the increased number of submissions and complexity of sequences. In order to do this, traditional methods of accessing and presenting data will be challenged, and new methods will need to be utilized to keep pace with new discoveries.",D948 - D955,10.1093/nar/gkz950,https://www.harrison.com/category/
cb56121bc38e0f4b44bcb5296a12038626152e96,CARD 2017: expansion and model-centric curation of the comprehensive antibiotic resistance database,"The Comprehensive Antibiotic Resistance Database (CARD; http://arpcard.mcmaster.ca) is a manually curated resource containing high quality reference data on the molecular basis of antimicrobial resistance (AMR), with an emphasis on the genes, proteins and mutations involved in AMR. CARD is ontologically structured, model centric, and spans the breadth of AMR drug classes and resistance mechanisms, including intrinsic, mutation-driven and acquired resistance. It is built upon the Antibiotic Resistance Ontology (ARO), a custom built, interconnected and hierarchical controlled vocabulary allowing advanced data sharing and organization. Its design allows the development of novel genome analysis tools, such as the Resistance Gene Identifier (RGI) for resistome prediction from raw genome sequence. Recent improvements include extensive curation of additional reference sequences and mutations, development of a unique Model Ontology and accompanying AMR detection models to power sequence analysis, new visualization tools, and expansion of the RGI for detection of emergent AMR threats. CARD curation is updated monthly based on an interplay of manual literature curation, computational text mining, and genome analysis.",D566 - D573,10.1093/nar/gkw1004,https://www.rose.com/wp-content/category.htm
6d96f946aaabc734af7fe3fc4454cf8547fcd5ed,The AR face database,,50-148,09-341074-8,https://www.guerrero.com/search/
d71af418eeb9f5a68062929bae12af74773ffcb2,The ChEMBL database in 2017,"ChEMBL is an open large-scale bioactivity database (https://www.ebi.ac.uk/chembl), previously described in the 2012 and 2014 Nucleic Acids Research Database Issues. Since then, alongside the continued extraction of data from the medicinal chemistry literature, new sources of bioactivity data have also been added to the database. These include: deposited data sets from neglected disease screening; crop protection data; drug metabolism and disposition data and bioactivity data from patents. A number of improvements and new features have also been incorporated. These include the annotation of assays and targets using ontologies, the inclusion of targets and indications for clinical candidates, addition of metabolic pathways for drugs and calculation of structural alerts. The ChEMBL data can be accessed via a web-interface, RDF distribution, data downloads and RESTful web-services.",D945 - D954,10.1093/nar/gkw1074,https://johnson.com/
908091b4a8757c3b2f7d9cfa2c4f616ee12c5157,SUN database: Large-scale scene recognition from abbey to zoo,"Scene categorization is a fundamental problem in computer vision. However, scene understanding research has been constrained by the limited scope of currently-used databases which do not capture the full variety of scene categories. Whereas standard databases for object categorization contain hundreds of different classes of objects, the largest available dataset of scene categories contains only 15 classes. In this paper we propose the extensive Scene UNderstanding (SUN) database that contains 899 categories and 130,519 images. We use 397 well-sampled categories to evaluate numerous state-of-the-art algorithms for scene recognition and establish new bounds of performance. We measure human scene classification performance on the SUN database and compare this with computational methods. Additionally, we study a finer-grained scene representation to detect scenes embedded inside of larger scenes.",3485-3492,10.1109/CVPR.2010.5539970,https://www.osborne.com/main/categories/categories/terms.asp
bc744742f1644c9cab6b9535ab0bd6f2eed320bb,CDD: a Conserved Domain Database for the functional annotation of proteins,"NCBI’s Conserved Domain Database (CDD) is a resource for the annotation of protein sequences with the location of conserved domain footprints, and functional sites inferred from these footprints. CDD includes manually curated domain models that make use of protein 3D structure to refine domain models and provide insights into sequence/structure/function relationships. Manually curated models are organized hierarchically if they describe domain families that are clearly related by common descent. As CDD also imports domain family models from a variety of external sources, it is a partially redundant collection. To simplify protein annotation, redundant models and models describing homologous families are clustered into superfamilies. By default, domain footprints are annotated with the corresponding superfamily designation, on top of which specific annotation may indicate high-confidence assignment of family membership. Pre-computed domain annotation is available for proteins in the Entrez/Protein dataset, and a novel interface, Batch CD-Search, allows the computation and download of annotation for large sets of protein queries. CDD can be accessed via http://www.ncbi.nlm.nih.gov/Structure/cdd/cdd.shtml.",D225 - D229,10.1093/nar/gkq1189,http://doyle.com/login.html
3d1ba71a1c3b7302e12ab3d07bf4a8451db5aad0,Plant cis-acting regulatory DNA elements (PLACE) database: 1999,"PLACE (http://www.dna.affrc.go.jp/htdocs/PLACE/) is a database of nucleotide sequence motifs found in plant cis-acting regulatory DNA elements. Motifs were extracted from previously published reports on genes in vascular plants. In addition to the motifs originally reported, their variations in other genes or in other plant species in later reports are also compiled. Documents for each motif in the PLACE database contains, in addition to a motif sequence, a brief definition and description of each motif, and relevant literature with PubMed ID numbers and GenBank accession numbers where available. Users can search their query sequences for cis-elements using the Signal Scan program at our web site. The results will be reported in one of the three forms. Clicking the PLACE accession numbers in the result report will open the pertinent motif document. Clicking the PubMed or GenBank accession number in the document will allow users to access to these databases, and to read the of the literature or the annotation in the DNA database. This report summarizes the present status of this database and available tools.","
          297-300
        ",10.1093/nar/27.1.297,https://www.smith.com/main/main/
61533dd9e41f20e2f5deaf22afb04c94b4071eac,"Repbase Update, a database of eukaryotic repetitive elements","Repbase Update is a comprehensive database of repetitive elements from diverse eukaryotic organisms. Currently, it contains over 3600 annotated sequences representing different families and subfamilies of repeats, many of which are unreported anywhere else. Each sequence is accompanied by a short description and references to the original contributors. Repbase Update includes Repbase Reports, an electronic journal publishing newly discovered transposable elements, and the Transposon Pub, a web-based browser of selected chromosomal maps of transposable elements. Sequences from Repbase Update are used to screen and annotate repetitive elements using programs such as Censor and RepeatMasker. Repbase Update is available on the worldwide web at http://www.girinst.org/Repbase_Update.html.",462 - 467,10.1159/000084979,https://hebert.info/home.htm
5a2892f91addeea2f4600d28b23e684be32f5b2c,DEAP: A Database for Emotion Analysis ;Using Physiological Signals,"We present a multimodal data set for the analysis of human affective states. The electroencephalogram (EEG) and peripheral physiological signals of 32 participants were recorded as each watched 40 one-minute long excerpts of music videos. Participants rated each video in terms of the levels of arousal, valence, like/dislike, dominance, and familiarity. For 22 of the 32 participants, frontal face video was also recorded. A novel method for stimuli selection is proposed using retrieval by affective tags from the last.fm website, video highlight detection, and an online assessment tool. An extensive analysis of the participants' ratings during the experiment is presented. Correlates between the EEG signal frequencies and the participants' ratings are investigated. Methods and results are presented for single-trial classification of arousal, valence, and like/dislike ratings using the modalities of EEG, peripheral physiological signals, and multimedia content analysis. Finally, decision fusion of the classification results from different modalities is performed. The data set is made publicly available and we encourage other researchers to use it for testing their own affective state estimation methods.",18-31,10.1109/T-AFFC.2011.15,https://brewer.com/
a8db50edfe26a6ae33a6787e2049de5bacd18666,ChEMBL: a large-scale bioactivity database for drug discovery,"ChEMBL is an Open Data database containing binding, functional and ADMET information for a large number of drug-like bioactive compounds. These data are manually abstracted from the primary published literature on a regular basis, then further curated and standardized to maximize their quality and utility across a wide range of chemical biology and drug-discovery research problems. Currently, the database contains 5.4 million bioactivity measurements for more than 1 million compounds and 5200 protein targets. Access is available through a web-based interface, data downloads and web services at: https://www.ebi.ac.uk/chembldb.",D1100 - D1107,10.1093/nar/gkr777,https://www.williams-romero.com/blog/blog/about/
fc90bd571818dacf9f19d4198c6e00d948632d2b,Human Protein Reference Database—2009 update,"Human Protein Reference Database (HPRD—http://www.hprd.org/), initially described in 2003, is a database of curated proteomic information pertaining to human proteins. We have recently added a number of new features in HPRD. These include PhosphoMotif Finder, which allows users to find the presence of over 320 experimentally verified phosphorylation motifs in proteins of interest. Another new feature is a protein distributed annotation system—Human Proteinpedia (http://www.humanproteinpedia.org/)—through which laboratories can submit their data, which is mapped onto protein entries in HPRD. Over 75 laboratories involved in proteomics research have already participated in this effort by submitting data for over 15 000 human proteins. The submitted data includes mass spectrometry and protein microarray-derived data, among other data types. Finally, HPRD is also linked to a compendium of human signaling pathways developed by our group, NetPath (http://www.netpath.org/), which currently contains annotations for several cancer and immune signaling pathways. Since the last update, more than 5500 new protein sequences have been added, making HPRD a comprehensive resource for studying the human proteome.",D767 - D772,10.1093/nar/gkn892,https://www.evans.net/
f8928221d290a9cdd84d1de52e121373bc836caa,New tools in comparative political economy : the database of political institutions,"This article introduces a large new cross-country database, the database of political institutions. It covers 177 countries over 21 years, 1975-95. The article presents the intuition, construction, and definitions of the different variables. Among the novel variables introduced are several measures of checks and balances, tenure and stability, identification of party affiliation with government or opposition, and fragmentation of opposition and government parties in the legislature.",165-176,10.1093/WBER/15.1.165,https://www.norris-smith.com/terms/
1f53996347086be3bd3a32da0976ba2db7687988,miRDB: an online database for prediction of functional microRNA targets,"Abstract MicroRNAs (miRNAs) are small noncoding RNAs that act as master regulators in many biological processes. miRNAs function mainly by downregulating the expression of their gene targets. Thus, accurate prediction of miRNA targets is critical for characterization of miRNA functions. To this end, we have developed an online database, miRDB, for miRNA target prediction and functional annotations. Recently, we have performed major updates for miRDB. Specifically, by employing an improved algorithm for miRNA target prediction, we now present updated transcriptome-wide target prediction data in miRDB, including 3.5 million predicted targets regulated by 7000 miRNAs in five species. Further, we have implemented the new prediction algorithm into a web server, allowing custom target prediction with user-provided sequences. Another new database feature is the prediction of cell-specific miRNA targets. miRDB now hosts the expression profiles of over 1000 cell lines and presents target prediction data that are tailored for specific cell models. At last, a new web query interface has been added to miRDB for prediction of miRNA functions by integrative analysis of target prediction and Gene Ontology data. All data in miRDB are freely accessible at http://mirdb.org.",D127 - D131,10.1093/nar/gkz757,https://miller.info/faq/
072a0db716fb6f8332323f076b71554716a7271c,The impact of the MIT-BIH Arrhythmia Database,"The MIT-BIH Arrhythmia Database was the first generally available set of standard test material for evaluation of arrhythmia detectors, and it has been used for that purpose as well as for basic research into cardiac dynamics at about 500 sites worldwide since 1980. It has lived a far longer life than any of its creators ever expected. Together with the American Heart Association Database, it played an interesting role in stimulating manufacturers of arrhythmia analyzers to compete on the basis of objectively measurable performance, and much of the current appreciation of the value of common databases, both for basic research and for medical device development and evaluation, can be attributed to this experience. In this article, we briefly review the history of the database, describe its contents, discuss what we have learned about database design and construction, and take a look at some of the later projects that have been stimulated by both the successes and the limitations of the MIT-BIH Arrhythmia Database.",45-50,10.1109/51.932724,https://donaldson.com/terms/
e8104b335a4e499f7b79b913a92d23263c82f6b6,The HITRAN2012 molecular spectroscopic database,,71-108,10.1016/j.jqsrt.2013.07.002,http://www.jones.com/about.html
e274c1b6e17825feab52de205fd0bc4917d5be6c,Protein backbone angle restraints from searching a database for chemical shift and sequence homology,,289-302,10.1023/A:1008392405740,http://bishop-reynolds.com/homepage/
288b317e427c6bf4c94d455049bd1368ff2071eb,The Immune Epitope Database (IEDB): 2018 update,"Abstract The Immune Epitope Database (IEDB, iedb.org) captures experimental data confined in figures, text and tables of the scientific literature, making it freely available and easily searchable to the public. The scope of the IEDB extends across immune epitope data related to all species studied and includes antibody, T cell, and MHC binding contexts associated with infectious, allergic, autoimmune, and transplant related diseases. Having been publicly accessible for >10 years, the recent focus of the IEDB has been improved query and reporting functionality to meet the needs of our users to access and summarize data that continues to grow in quantity and complexity. Here we present an update on our current efforts and future goals.",D339 - D343,10.1093/nar/gky1006,http://www.le.com/post/
00bc156bac2b39fab1689fe047b23c9f216d7f29,Database resources of the National Center for Biotechnology Information: update,"In addition to maintaining the GenBank(R) nucleic acid sequence database, the National Center for Biotechnology Information (NCBI) provides data analysis and retrieval resources for the data in GenBank and other biological data made available through NCBI’s website. NCBI resources include Entrez, PubMed, PubMed Central, LocusLink, the NCBI Taxonomy Browser, BLAST, BLAST Link (BLink), Electronic PCR, OrfFinder, Spidey, RefSeq, UniGene, HomoloGene, ProtEST, dbMHC, dbSNP, Cancer Chromosome Aberration Project (CCAP), Entrez Genomes and related tools, the Map Viewer, Model Maker, Evidence Viewer, Clusters of Orthologous Groups (COGs) database, Retroviral Genotyping Tools, SARS Coronavirus Resource, SAGEmap, Gene Expression Omnibus (GEO), Online Mendelian Inheritance in Man (OMIM), the Molecular Modeling Database (MMDB), the Conserved Domain Database (CDD) and the Conserved Domain Architecture Retrieval Tool (CDART). Augmenting many of the web applications are custom implementations of the BLAST program optimized to search specialized data sets. All of the resources can be accessed through the NCBI home page at: http://www.ncbi.nlm.nih.gov.",D35 - D40,10.1093/nar/gkh073,https://www.stevens-stevenson.com/
8035e5002b7b0898ca7fa8263d09fe4454c6e4fd,The BioGRID interaction database: 2019 update,"Abstract The Biological General Repository for Interaction Datasets (BioGRID: https://thebiogrid.org) is an open access database dedicated to the curation and archival storage of protein, genetic and chemical interactions for all major model organism species and humans. As of September 2018 (build 3.4.164), BioGRID contains records for 1 598 688 biological interactions manually annotated from 55 809 publications for 71 species, as classified by an updated set of controlled vocabularies for experimental detection methods. BioGRID also houses records for >700 000 post-translational modification sites. BioGRID now captures chemical interaction data, including chemical–protein interactions for human drug targets drawn from the DrugBank database and manually curated bioactive compounds reported in the literature. A new dedicated aspect of BioGRID annotates genome-wide CRISPR/Cas9-based screens that report gene–phenotype and gene–gene relationships. An extension of the BioGRID resource called the Open Repository for CRISPR Screens (ORCS) database (https://orcs.thebiogrid.org) currently contains over 500 genome-wide screens carried out in human or mouse cell lines. All data in BioGRID is made freely available without restriction, is directly downloadable in standard formats and can be readily incorporated into existing applications via our web service platforms. BioGRID data are also freely distributed through partner model organism databases and meta-databases.",D529 - D541,10.1093/nar/gky1079,http://thomas-compton.com/about.html
e3f2391513693647e0ea87bfa86cd89e468f51d0,Comprehensive database for facial expression analysis,"Within the past decade, significant effort has occurred in developing methods of facial expression analysis. Because most investigators have used relatively limited data sets, the generalizability of these various methods remains unknown. We describe the problem space for facial expression analysis, which includes level of description, transitions among expressions, eliciting conditions, reliability and validity of training and test data, individual differences in subjects, head orientation and scene complexity image characteristics, and relation to non-verbal behavior. We then present the CMU-Pittsburgh AU-Coded Face Expression Image Database, which currently includes 2105 digitized image sequences from 182 adult subjects of varying ethnicity, performing multiple tokens of most primary FACS action units. This database is the most comprehensive testbed to date for comparative studies of facial expression analysis.",46-53,10.1109/AFGR.2000.840611,http://nicholson.com/
dd31f1439a0b80cb9447a112347836b3325e953e,The Standardized World Income Inequality Database,"Cross-national research on the causes and consequences of income inequality has been hindered by the limitations of existing inequality datasets: greater coverage across countries and over time is available from these sources only at the cost of significantly reduced comparability across observations. The goal of the Standardized World Income Inequality Database (SWIID) is to overcome these limitations. A custom missing-data algorithm was used to standardize the United Nations University's World Income Inequality Database and data from other sources; data collected by the Luxembourg Income Study served as the standard. The SWIID provides comparable Gini indices of gross and net income inequality for 173 countries for as many years as possible from 1960 to the present along with estimates of uncertainty in these statistics. By maximizing comparability for the largest possible sample of countries and years, the SWIID is better suited to broadly cross-national research on income inequality than previously available sources. 
 
In any papers or publications that use the SWIID, authors are asked to cite the article of record for the data set and give the version number as follows: 
 
Solt, Frederick. 2009. ""Standardizing the World Income Inequality Database."" Social Science Quarterly 90(2):231-242. SWIID Version 3.1, December 2011.",1267-1281,10.1111/SSQU.12295,https://www.dunn.com/homepage/
804836b8ad86ef8042e3dcbd45442a52f031ee03,A Database and Evaluation Methodology for Optical Flow,,Jan-31,10.1007/s11263-010-0390-2,https://www.sharp-randall.com/posts/main/app/terms/
eb960b5d56ed1368991eaa4f40cb7afee66edb1f,ONCOMINE: a cancer microarray database and integrated data-mining platform.,,"
          1-6
        ",10.1016/S1476-5586(04)80047-2,https://barron-taylor.com/terms/
1cf4a6954b419b5478c96119fc1e79aa90f87dea,The Cambridge Structural Database in retrospect and prospect.,"The Cambridge Crystallographic Data Centre (CCDC) was established in 1965 to record numerical, chemical and bibliographic data relating to published organic and metal-organic crystal structures. The Cambridge Structural Database (CSD) now stores data for nearly 700,000 structures and is a comprehensive and fully retrospective historical archive of small-molecule crystallography. Nearly 40,000 new structures are added each year. As X-ray crystallography celebrates its centenary as a subject, and the CCDC approaches its own 50th year, this article traces the origins of the CCDC as a publicly funded organization and its onward development into a self-financing charitable institution. Principally, however, we describe the growth of the CSD and its extensive associated software system, and summarize its impact and value as a basis for research in structural chemistry, materials science and the life sciences, including drug discovery and drug development. Finally, the article considers the CCDC's funding model in relation to open access and open data paradigms.","
          662-71
        ",10.1002/anie.201306438,http://young-montgomery.info/tag/tag/tags/about.htm
41e2692d9ac1434d1841d5a29e7ccb927b82b677,The Comparative Toxicogenomics Database: update 2019,"Abstract The Comparative Toxicogenomics Database (CTD; http://ctdbase.org/) is a premier public resource for literature-based, manually curated associations between chemicals, gene products, phenotypes, diseases, and environmental exposures. In this biennial update, we present our new chemical–phenotype module that codes chemical-induced effects on phenotypes, curated using controlled vocabularies for chemicals, phenotypes, taxa, and anatomical descriptors; this module provides unique opportunities to explore cellular and system-level phenotypes of the pre-disease state and allows users to construct predictive adverse outcome pathways (linking chemical–gene molecular initiating events with phenotypic key events, diseases, and population-level health outcomes). We also report a 46% increase in CTD manually curated content, which when integrated with other datasets yields more than 38 million toxicogenomic relationships. We describe new querying and display features for our enhanced chemical–exposure science module, providing greater scope of content and utility. As well, we discuss an updated MEDIC disease vocabulary with over 1700 new terms and accession identifiers. To accommodate these increases in data content and functionality, CTD has upgraded its computational infrastructure. These updates continue to improve CTD and help inform new testable hypotheses about the etiology and mechanisms underlying environmentally influenced diseases.",D948 - D954,10.1093/nar/gky868,https://www.sharp.biz/main/main/search/
8c8b7c1adb6f077bb3045928767b8bc6763e0c06,HMDB 3.0—The Human Metabolome Database in 2013,"The Human Metabolome Database (HMDB) (www.hmdb.ca) is a resource dedicated to providing scientists with the most current and comprehensive coverage of the human metabolome. Since its first release in 2007, the HMDB has been used to facilitate research for nearly 1000 published studies in metabolomics, clinical biochemistry and systems biology. The most recent release of HMDB (version 3.0) has been significantly expanded and enhanced over the 2009 release (version 2.0). In particular, the number of annotated metabolite entries has grown from 6500 to more than 40 000 (a 600% increase). This enormous expansion is a result of the inclusion of both ‘detected’ metabolites (those with measured concentrations or experimental confirmation of their existence) and ‘expected’ metabolites (those for which biochemical pathways are known or human intake/exposure is frequent but the compound has yet to be detected in the body). The latest release also has greatly increased the number of metabolites with biofluid or tissue concentration data, the number of compounds with reference spectra and the number of data fields per entry. In addition to this expansion in data quantity, new database visualization tools and new data content have been added or enhanced. These include better spectral viewing tools, more powerful chemical substructure searches, an improved chemical taxonomy and better, more interactive pathway maps. This article describes these enhancements to the HMDB, which was previously featured in the 2009 NAR Database Issue. (Note to referees, HMDB 3.0 will go live on 18 September 2012.).",D801 - D807,10.1093/nar/gks1065,https://russo-carroll.com/homepage/
a1b62db5c838df5630b1c238d4862dc69bdbc874,"The eICU Collaborative Research Database, a freely available multi-center database for critical care research",,23-148,10.1038/sdata.2018.178,https://smith.com/privacy/
68e3ccd3e8f24ad1a15caaddd99178edbf17f8b9,MODOMICS: a database of RNA modification pathways. 2017 update,"Abstract MODOMICS is a database of RNA modifications that provides comprehensive information concerning the chemical structures of modified ribonucleosides, their biosynthetic pathways, the location of modified residues in RNA sequences, and RNA-modifying enzymes. In the current database version, we included the following new features and data: extended mass spectrometry and liquid chromatography data for modified nucleosides; links between human tRNA sequences and MINTbase - a framework for the interactive exploration of mitochondrial and nuclear tRNA fragments; new, machine-friendly system of unified abbreviations for modified nucleoside names; sets of modified tRNA sequences for two bacterial species, updated collection of mammalian tRNA modifications, 19 newly identified modified ribonucleosides and 66 functionally characterized proteins involved in RNA modification. Data from MODOMICS have been linked to the RNAcentral database of RNA sequences. MODOMICS is available at http://modomics.genesilico.pl.",D303 - D307,10.1093/nar/gkx1030,http://miller.net/author/
aca92bbbe7ebc8b6b0f272aca209ac5a027222bd,The IPD and IMGT/HLA database: allele variant databases,"The Immuno Polymorphism Database (IPD) was developed to provide a centralized system for the study of polymorphism in genes of the immune system. Through the IPD project we have established a central platform for the curation and publication of locus-specific databases involved either directly or related to the function of the Major Histocompatibility Complex in a number of different species. We have collaborated with specialist groups or nomenclature committees that curate the individual sections before they are submitted to IPD for online publication. IPD consists of five core databases, with the IMGT/HLA Database as the primary database. Through the work of the various nomenclature committees, the HLA Informatics Group and in collaboration with the European Bioinformatics Institute we are able to provide public access to this data through the website http://www.ebi.ac.uk/ipd/. The IPD project continues to develop with new tools being added to address scientific developments, such as Next Generation Sequencing, and to address user feedback and requests. Regular updates to the website ensure that new and confirmatory sequences are dispersed to the immunogenetics community, and the wider research and clinical communities.",D423 - D431,10.1093/nar/gku1161,http://www.hess.com/explore/categories/wp-content/search/
f3c56ba45b39ec915477a19779cfc5090be09a73,The Pfam protein families database,"Pfam is a large collection of protein multiple sequence alignments and profile hidden Markov models. Pfam is available on the WWW in the UK at http://www.sanger.ac.uk/Software/Pfam/, in Sweden at http://www.cgr.ki.se/Pfam/ and in the US at http://pfam.wustl.edu/. The latest version (4.3) of Pfam contains 1815 families. These Pfam families match 63% of proteins in SWISS-PROT 37 and TrEMBL 9. For complete genomes Pfam currently matches up to half of the proteins. Genomic DNA can be directly searched against the Pfam library using the Wise2 package.","
          263-6
        ",10.1093/nar/gkh121,https://www.hicks-anderson.com/explore/wp-content/category/main.html
2c0aaeb420e1cd2d767a1797b2ded62e0d2ee426,GENEVESTIGATOR. Arabidopsis Microarray Database and Analysis Toolbox1[w],"High-throughput gene expression analysis has become a frequent and powerful research tool in biology. At present, however, few software applications have been developed for biologists to query large microarray gene expression databases using a Web-browser interface. We present GENEVESTIGATOR, a database and Web-browser data mining interface for Affymetrix GeneChip data. Users can query the database to retrieve the expression patterns of individual genes throughout chosen environmental conditions, growth stages, or organs. Reversely, mining tools allow users to identify genes specifically expressed during selected stresses, growth stages, or in particular organs. Using GENEVESTIGATOR, the gene expression profiles of more than 22,000 Arabidopsis genes can be obtained, including those of 10,600 currently uncharacterized genes. The objective of this software application is to direct gene functional discovery and design of new experiments by providing plant biologists with contextual information on the expression of genes. The database and analysis toolbox is available as a community resource at https://www.genevestigator.ethz.ch.",2621 - 2632,10.1104/pp.104.046367,https://www.olson.com/explore/homepage.php
dc8b25e35a3acb812beb499844734081722319b4,The FERET database and evaluation procedure for face-recognition algorithms,,295-306,10.1016/S0262-8856(97)00070-X,http://www.liu.com/
73a254f05fa694dc11a5efc5a033a8f1a4c84fd0,The Gene Expression Omnibus Database,,"
          93-110
        ",10.1007/978-1-4939-3578-9_5,https://www.lopez-montgomery.net/homepage/
12d8a9991ee7aecc65bc0991959c5b58a367b2ae,APD3: the antimicrobial peptide database as a tool for research and education,"The antimicrobial peptide database (APD, http://aps.unmc.edu/AP/) is an original database initially online in 2003. The APD2 (2009 version) has been regularly updated and further expanded into the APD3. This database currently focuses on natural antimicrobial peptides (AMPs) with defined sequence and activity. It includes a total of 2619 AMPs with 261 bacteriocins from bacteria, 4 AMPs from archaea, 7 from protists, 13 from fungi, 321 from plants and 1972 animal host defense peptides. The APD3 contains 2169 antibacterial, 172 antiviral, 105 anti-HIV, 959 antifungal, 80 antiparasitic and 185 anticancer peptides. Newly annotated are AMPs with antibiofilm, antimalarial, anti-protist, insecticidal, spermicidal, chemotactic, wound healing, antioxidant and protease inhibiting properties. We also describe other searchable annotations, including target pathogens, molecule-binding partners, post-translational modifications and animal models. Amino acid profiles or signatures of natural AMPs are important for peptide classification, prediction and design. Finally, we summarize various database applications in research and education.",D1087 - D1093,10.1093/nar/gkv1278,https://terry-sheppard.com/author/
5cf0d213f3253cd46673d955209f8463db73cc51,IEMOCAP: interactive emotional dyadic motion capture database,,335-359,10.1007/S10579-008-9076-6,https://www.mccoy.com/
ba90ae48b30594b57a5ca7bfd37cae150458ecfa,TRRUST v2: an expanded reference database of human and mouse transcriptional regulatory interactions,"Abstract Transcription factors (TFs) are major trans-acting factors in transcriptional regulation. Therefore, elucidating TF–target interactions is a key step toward understanding the regulatory circuitry underlying complex traits such as human diseases. We previously published a reference TF–target interaction database for humans—TRRUST (Transcriptional Regulatory Relationships Unraveled by Sentence-based Text mining)—which was constructed using sentence-based text mining, followed by manual curation. Here, we present TRRUST v2 (www.grnpedia.org/trrust) with a significant improvement from the previous version, including a significantly increased size of the database consisting of 8444 regulatory interactions for 800 TFs in humans. More importantly, TRRUST v2 also contains a database for TF–target interactions in mice, including 6552 TF–target interactions for 828 mouse TFs. TRRUST v2 is also substantially more comprehensive and less biased than other TF–target interaction databases. We also improved the web interface, which now enables prioritization of key TFs for a physiological condition depicted by a set of user-input transcriptional responsive genes. With the significant expansion in the database size and inclusion of the new web tool for TF prioritization, we believe that TRRUST v2 will be a versatile database for the study of the transcriptional regulation involved in human diseases.",D380 - D386,10.1093/nar/gkx1013,https://campos.org/explore/search/posts/home.php
26c075104d0ea1177cce4bd2d5c5d9eef93b8a3b,"The MEROPS database of proteolytic enzymes, their substrates and inhibitors in 2017 and a comparison with peptidases in the PANTHER database","Abstract The MEROPS database (http://www.ebi.ac.uk/merops/) is an integrated source of information about peptidases, their substrates and inhibitors. The hierarchical classification is: protein-species, family, clan, with an identifier at each level. The MEROPS website moved to the EMBL-EBI in 2017, requiring refactoring of the code-base and services provided. The interface to sequence searching has changed and the MEROPS protein sequence libraries can be searched at the EMBL-EBI with HMMER, FastA and BLASTP. Cross-references have been established between MEROPS and the PANTHER database at both the family and protein-species level, which will help to improve curation and coverage between the resources. Because of the increasing size of the MEROPS sequence collection, in future only sequences of characterized proteins, and from completely sequenced genomes of organisms of evolutionary, medical or commercial significance will be added. As an example, peptidase homologues in four proteomes from the Asgard superphylum of Archaea have been identified and compared to other archaean, bacterial and eukaryote proteomes. This has given insights into the origins and evolution of peptidase families, including an expansion in the number of proteasome components in Asgard archaeotes and as organisms increase in complexity. Novel structures for proteasome complexes in archaea are postulated.",D624 - D632,10.1093/nar/gkx1134,https://taylor.org/category.htm
2157f202c8c89d924dd4da4d1bcf92d16fcd8893,"Image database TID2013: Peculiarities, results and perspectives",,57-77,10.1016/J.IMAGE.2014.10.009,https://www.miller.org/about.htm
439a453090e28f0858ad5ba0765cc2eeffb23626,A New Database on Financial Development and Structure,"The authors introduce a new database of indicators of financial development and structure across countries and over time. This database is unique in that it unites a variety of indicators that measure the size, activity, and efficiency of financial intermediaries and markets. It improves on previous efforts by presenting data on the public share of commercial banks, by introducing indicators of the size and activity of non bank financial institutions, and by presenting measures of the size of bond and primary equity markets. The compiled data permit the construction of financial structure indicators to measure whether, for example, a country's banks are larger, more active, and more efficient than its stock markets. These indicators can then be used to investigate the empirical link between the legal, regulatory, and policy environment and indicators of financial structure. They can also be used to analyze the implications of financial structure for economic growth. The authors describe the sources and construction of, and the intuition behind, different indicators and present descriptive statistics.",70-139,10.1596/1813-9450-2146,https://valencia.biz/
3765df816dc5a061bc261e190acc8bdd9d47bec0,Presentation and validation of the Radboud Faces Database,"Many research fields concerned with the processing of information contained in human faces would benefit from face stimulus sets in which specific facial characteristics are systematically varied while other important picture characteristics are kept constant. Specifically, a face database in which displayed expressions, gaze direction, and head orientation are parametrically varied in a complete factorial design would be highly useful in many research domains. Furthermore, these stimuli should be standardised in several important, technical aspects. The present article presents the freely available Radboud Faces Database offering such a stimulus set, containing both Caucasian adult and children images. This face database is described both procedurally and in terms of content, and a validation study concerning its most important characteristics is presented. In the validation study, all frontal images were rated with respect to the shown facial expression, intensity of expression, clarity of expression, genuineness of expression, attractiveness, and valence. The results show very high recognition of the intended facial expressions.",1377 - 1388,10.1080/02699930903485076,http://vega.com/categories/home.html
